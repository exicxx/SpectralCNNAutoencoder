{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5917234b-e981-47c5-a2fb-a2448a8bf23f",
   "metadata": {},
   "source": [
    "# Goals\n",
    "- Access and filter DESI EDR galaxy spectra data from a database using SPARCL.\n",
    "- Process and normalis the spectra data to prepare it for model training.\n",
    "- Develop a CAE with skip connections to perform dimensionality reduction and reconstruction of the spectra.\n",
    "- Train the autoencoder model using a weighted mean squared error (MSE) loss function to emphasise critical spectral features.\n",
    "- Identify and visualise anomalies in the galaxy spectra based on high reconstruction errors.\n",
    "- Provide visual representations of detected anomalies and evaluate the model's performance through training loss metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7961d6d0-48c6-48bf-99f2-f5c303f5e9ad",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This project leverages the Dark Energy Spectroscopic Instrument (DESI) Early Data Release (EDR) dataset to train a convolutional autoencoder (CAE) for anomaly detection in galaxy spectra. The code retrieves galaxy spectra data from a database, processes and normaliss it, and then applies an autoencoder with skip connections to reconstruct the spectra. The reconstruction errors are used to identify anomalous spectra, which may indicate unusual features or observational issues in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2216cae-752c-4f4d-9b73-7799fbbd15a1",
   "metadata": {},
   "source": [
    "## Commands to run in terminal to upload changes to GitHub \n",
    "##### Note, to save and exit the commit comment section: Esc, \":wq\", Enter\n",
    "\n",
    "\n",
    "cd \"/Users/elicox/Desktop/Mac/Work/Yr4 Work/Project/CNN-auto\"\n",
    "\n",
    "git add SuperCAE.ipynb\n",
    "\n",
    "output_dir=\"SpectralCNNAutoencoder_output\"\n",
    "\n",
    "latest_mean_reconstruction_error=$(ls -t output_images/mean_reconstruction_error_*.png | head -n 1)\n",
    "\n",
    "latest_anomalous_spectra=$(ls -t output_images/anomalous_spectra_*.png | head -n 1)\n",
    "\n",
    "latest_sampling_info=$(ls -t anomalous_regions/sampling_info_*.json | head -n 1)\n",
    "\n",
    "if [ -f \"$latest_mean_reconstruction_error\" ]; then\n",
    "    mv \"$latest_mean_reconstruction_error\" \"$output_dir/\"\n",
    "    git add \"$output_dir/$(basename \"$latest_mean_reconstruction_error\")\"\n",
    "fi\n",
    "\n",
    "if [ -f \"$latest_anomalous_spectra\" ]; then\n",
    "    mv \"$latest_anomalous_spectra\" \"$output_dir/\"\n",
    "    git add \"$output_dir/$(basename \"$latest_anomalous_spectra\")\"\n",
    "fi\n",
    "\n",
    "if [ -f \"$latest_sampling_info\" ]; then\n",
    "    mv \"$latest_sampling_info\" \"$output_dir/\"\n",
    "    git add \"$output_dir/$(basename \"$latest_sampling_info\")\"\n",
    "fi\n",
    "\n",
    "git commit \n",
    "\n",
    "git push origin main\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f67de0-44c7-4be9-bf92-72a225ea02d3",
   "metadata": {},
   "source": [
    "# Change log\n",
    "<i> Place to log changes before they are recorded in a github update:\n",
    "\n",
    "As the deadline to the project became closer, the order within this report fell away therefore, this is the tidied version of the code used for the report.\n",
    "\n",
    "Beyond tidying, I have changed the normalisation from min max to robust scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5d3fa0-4dea-462a-b2ee-e67288319bbb",
   "metadata": {},
   "source": [
    "# Imports and Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0656252-1952-44a7-87c2-32c7340d734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "from sparcl.client import SparclClient\n",
    "from dl import queryClient as qc, authClient as ac\n",
    "from getpass import getpass\n",
    "import os\n",
    "import re\n",
    "import csv \n",
    "import torch.nn.functional as F\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "from torchviz import make_dot\n",
    "import time\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed \n",
    "import logging\n",
    "import json\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import matplotlib.patches as patches\n",
    "import requests\n",
    "from PIL import Image, ImageDraw\n",
    "from deap import base, creator, tools, algorithms\n",
    "import subprocess\n",
    "from skopt.space import Real\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize\n",
    "from skopt.plots import plot_convergence, plot_objective, plot_evaluations\n",
    "from skopt.plots import plot_evaluations\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.interpolate import griddata\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "#token = ac.login(input(\"Enter user name: (+ENTER) \"),getpass(\"Enter password: (+ENTER) \"))\n",
    "ac.whoAmI()\n",
    "\n",
    "# Configure file directories\n",
    "DATA_DIR = '/Users/elicox/Desktop/Mac/Work/Yr4 Work/Project/CNN-auto/'\n",
    "OUT_DIR = os.path.join(DATA_DIR, 'SpectralCNNAutoencoder_output')\n",
    "CSV_PATH = os.path.join(DATA_DIR, 'spectra_data_2.csv')\n",
    "JSON_DIR = os.path.join(DATA_DIR, 'anomalous_regions')\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "# os.environ[\"PATH\"] += os.pathsep + \"/opt/homebrew/bin\" # uncomment when CNN visualisation is needed\n",
    "\n",
    "# Initialise SPARCL client\n",
    "client = SparclClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2caccac-296c-4b97-9efb-de4492dab14e",
   "metadata": {},
   "source": [
    "# Database and Data Query Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1087ffdc-8b81-4049-8658-031d0beaea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_spectra_data():\n",
    "    \"\"\"\n",
    "    Queries the DESI Early Data Release (EDR) database to retrieve galaxy spectra data, \n",
    "    filtering for primary spectra with a minimum number of coadded spectra and good quality flags.\n",
    "    \n",
    "    This function selects observational fields from the DESI EDR database:\n",
    "        - `targetid`: Unique identifier for each galaxy.\n",
    "        - `z`: Spectroscopic redshift.\n",
    "        - `zwarn`: Redshift quality flag, where `zwarn = 0` indicates reliable data.\n",
    "        - `coadd_fiberstatus`: Status of the fiber used for observation, with `0` indicating no issues.\n",
    "        - `spectype`: Spectral type, filtered here to include only galaxies.\n",
    "        - `mean_fiber_ra`, `mean_fiber_dec`: Mean Right Ascension and Declination of the fiber position.\n",
    "        - `zcat_nspec`: Number of coadded spectra for the target.\n",
    "        - 'zp.wavelength': Wavelength data for the object.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with galaxy spectra data including flux error if available.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT zp.targetid, zp.z, zp.zwarn, zp.coadd_fiberstatus, zp.spectype, \n",
    "    zp.mean_fiber_ra, zp.mean_fiber_dec\n",
    "    FROM desi_edr.zpix AS zp\n",
    "    WHERE zp.zcat_primary = 't'\n",
    "      AND zp.zcat_nspec > 2\n",
    "      AND zp.z <= 0.5\n",
    "      AND zp.spectype = 'GALAXY'\n",
    "      AND zp.zwarn = '0'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Querying SPARCL database...\")\n",
    "        spec_data = qc.query(sql=query, fmt='table')\n",
    "        df = spec_data.to_pandas()\n",
    "        print(f\"Retrieved {len(df)} records from the database.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying data: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_or_query_data(csv_path):\n",
    "    \"\"\"\n",
    "    Loads data from CSV if available; otherwise, queries SPARCL and saves the results.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_path (str): Path to the CSV file to save or load data.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Loaded or queried data as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    if os.path.exists(csv_path):\n",
    "        print(f\"Loading data from {csv_path}...\")\n",
    "        try:\n",
    "            return pd.read_csv(csv_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading CSV file: {e}\")\n",
    "            print(\"Attempting to query the database instead...\")\n",
    "    data = query_spectra_data()\n",
    "    if data is not None:\n",
    "        try:\n",
    "            print(f\"Saving queried data to {csv_path}...\")\n",
    "            data.to_csv(csv_path, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data to CSV: {e}\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec4cf9d-20e7-4b4c-8668-99712b6f658e",
   "metadata": {},
   "source": [
    "# Data Retrieval and Parallel Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20111fb1-28d1-4abb-b48d-e87cd7b809ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_flux(targetid, inc, retries=5, delay=3):\n",
    "    \"\"\"\n",
    "    Retrieves and normalises flux data using RobustScaler, wavelength, and calculates error for a single target ID from the DESI database, with retry logic.\n",
    "    \"\"\"\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            res = client.retrieve_by_specid(specid_list=[targetid], include=inc, dataset_list=['DESI-EDR'])\n",
    "            for record in res.records:\n",
    "                if record['specprimary']:\n",
    "                    flux = record['flux']\n",
    "                    wavelength = record['wavelength']\n",
    "                    ivar = record['ivar']\n",
    "\n",
    "                    flux = flux.reshape(-1, 1)\n",
    "                    normalised_flux = scaler.fit_transform(flux).flatten()\n",
    "                    error = np.sqrt(1 / np.where(ivar == 0, 1e-10, ivar))\n",
    "\n",
    "                    return normalised_flux, wavelength, error  \n",
    "        except Exception:\n",
    "            if attempt < retries:\n",
    "                time.sleep(delay * (2 ** (attempt - 1)))  # Exponential backoff\n",
    "    return None, None, None\n",
    "\n",
    "\n",
    "def process_spectra_data(spec_data, batch_size=20, max_workers=10, output_csv_path=CSV_PATH):\n",
    "    \"\"\"\n",
    "    Retrieves and processes flux data in parallel for a DataFrame of galaxy spectra from the DESI database,\n",
    "    including flux, wavelength, and error data.\n",
    "\n",
    "    Parameters:\n",
    "    - spec_data (pd.DataFrame): DataFrame containing metadata for the galaxy spectra to be processed.\n",
    "    - batch_size (int): Number of spectra to retrieve in each batch.\n",
    "    - max_workers (int): Max number of parallel threads.\n",
    "    - output_csv_path (str): Path to save the updated DataFrame with flux and wavelength data.\n",
    "\n",
    "    Returns:\n",
    "    - tuple (list, list, list): Lists of flux, wavelength, and error values.\n",
    "    \"\"\"\n",
    "    all_fluxes, all_wavelengths, all_errors = [], [], []\n",
    "    total_records = len(spec_data)\n",
    "\n",
    "    inc = ['specid', 'redshift', 'flux', 'wavelength', 'ivar', 'spectype', \n",
    "           'specprimary', 'survey', 'program', 'targetid', 'coadd_fiberstatus']\n",
    "\n",
    "    for start_idx in tqdm(range(0, total_records, batch_size), desc=\"Processing spectra in batches\"):\n",
    "        batch = spec_data.iloc[start_idx:start_idx + batch_size]\n",
    "        batch_fluxes, batch_wavelengths, batch_errors = [], [], []\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {\n",
    "                executor.submit(retrieve_flux, int(row['targetid']), inc): row['targetid']\n",
    "                for _, row in batch.iterrows()\n",
    "            }\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    flux, wavelength, error = result\n",
    "                    batch_fluxes.append(flux)\n",
    "                    batch_wavelengths.append(wavelength)\n",
    "                    batch_errors.append(error)\n",
    "                else:\n",
    "                    print(f\"Warning: Missing flux for target ID {futures[future]}\")\n",
    "\n",
    "        all_fluxes.extend(batch_fluxes)\n",
    "        all_wavelengths.extend(batch_wavelengths)\n",
    "        all_errors.extend(batch_errors)\n",
    "\n",
    "    if not all_fluxes:\n",
    "        raise ValueError(\"Error: No flux data retrieved. Check data source or retrieval logic.\")\n",
    "\n",
    "    spec_data['flux'] = [list(f) for f in all_fluxes]\n",
    "    spec_data['wavelength'] = [list(w) for w in all_wavelengths]\n",
    "    spec_data['error'] = [list(e) for e in all_errors]\n",
    "\n",
    "    print(f\"Saving updated DataFrame with flux and wavelength data to {output_csv_path}...\")\n",
    "    spec_data.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    return all_fluxes, all_wavelengths, all_errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dfbffe-a9ea-4815-bcfb-b6c01fbfadd0",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d447a39-a337-4ecf-b252-f09e088d2470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate_generator(arrays, target_length):\n",
    "    \"\"\"\n",
    "    Generator that yields arrays padded with zeros or truncated to a fixed length.\n",
    "\n",
    "    Parameters:\n",
    "    - arrays (iterable): A collection of 1D arrays (e.g., lists or numpy arrays) of varying lengths.\n",
    "    - target_length (int): The desired length for each output array.\n",
    "\n",
    "    Yields:\n",
    "    - np.ndarray: A 1D array of length `target_length`, either padded with zeros or truncated as needed.\n",
    "    \"\"\"\n",
    "    for arr in arrays:\n",
    "        if arr is None:\n",
    "            yield np.zeros(target_length)\n",
    "        else:\n",
    "            yield np.pad(arr, (0, max(target_length - len(arr), 0)), mode='constant')[:target_length]\n",
    "\n",
    "def pad_or_truncate(arrays, target_length):\n",
    "    \"\"\"\n",
    "    Pads or truncates a list of 1D arrays to a uniform target length and stacks them into a 2D array.\n",
    "\n",
    "    Parameters:\n",
    "    - arrays (list of array-like): A list of arrays or lists to be padded or truncated.\n",
    "    - target_length (int): The length to which each array should be padded or truncated.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: A 2D array where each row has shape (target_length,).\n",
    "    \"\"\"\n",
    "    padded_list = list(pad_or_truncate_generator(arrays, target_length))\n",
    "    return np.vstack(padded_list)\n",
    "\n",
    "def filter_spectral_data(wavelengths, fluxes, errors, min_wavelength=4000):\n",
    "    \"\"\"\n",
    "    Filters spectral data to retain only wavelengths above a specified minimum value.\n",
    "\n",
    "    Parameters:\n",
    "    - wavelengths (list of lists or np.ndarray): 2D array or list of wavelength values.\n",
    "    - fluxes (list of lists or np.ndarray): 2D array or list of flux values.\n",
    "    - errors (list of lists or np.ndarray): 2D array or list of error values.\n",
    "    - min_wavelength (float): Minimum wavelength to retain in Ångstrom.\n",
    "\n",
    "    Returns:\n",
    "    - filtered_wavelengths (list of np.ndarray): List of filtered wavelength arrays.\n",
    "    - filtered_fluxes (list of np.ndarray): List of filtered flux arrays corresponding to wavelengths.\n",
    "    - filtered_errors (list of np.ndarray): List of filtered error arrays corresponding to wavelengths.\n",
    "    \"\"\"\n",
    "    filtered_wavelengths, filtered_fluxes, filtered_errors = [], [], []\n",
    "\n",
    "    for i in range(len(wavelengths)):\n",
    "        w_array = np.array(wavelengths[i])\n",
    "        f_array = np.array(fluxes[i])\n",
    "        e_array = np.array(errors[i])\n",
    "\n",
    "        mask = w_array >= min_wavelength\n",
    "        filtered_wavelengths.append(w_array[mask])\n",
    "        filtered_fluxes.append(f_array[mask])\n",
    "        filtered_errors.append(e_array[mask])\n",
    "\n",
    "    return filtered_wavelengths, filtered_fluxes, filtered_errors\n",
    "def create_padding_mask(fluxes, target_length):\n",
    "    \"\"\"\n",
    "    Generates a binary mask indicating the padded regions in each flux array.\n",
    "    \n",
    "    Parameters:\n",
    "    - fluxes (list of np.array): List of flux arrays for each spectrum.\n",
    "    - target_length (int): Length of the output mask arrays, matching the padded flux array length.\n",
    "    \n",
    "    Returns:\n",
    "    - np.array: Array of binary masks for each flux array, where 1 represents original data and 0 represents padded values.\n",
    "    \n",
    "    Notes:\n",
    "    - The mask is used during loss calculation to ignore the padded regions of the spectra.\n",
    "    \"\"\"\n",
    "    masks = []\n",
    "    for flux in fluxes:\n",
    "        mask = np.ones_like(flux)\n",
    "        if len(flux) < target_length:\n",
    "            mask = np.concatenate([mask, np.zeros(target_length - len(flux))])\n",
    "        masks.append(mask[:target_length])\n",
    "    return np.array(masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba78aad-71de-4177-9c49-c81600fcc0a6",
   "metadata": {},
   "source": [
    "#  Dataset and DataLoader Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5154fead-36de-4889-9c4d-731b4c765e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralDataset(Dataset):\n",
    "    def __init__(self, fluxes, wavelengths, errors, masks):\n",
    "        def to_tensor(x):\n",
    "            if isinstance(x, torch.Tensor):\n",
    "                return x.clone().detach()\n",
    "            return torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "        def ensure_3d(x):\n",
    "            x = to_tensor(x)\n",
    "            return x if x.ndim == 3 else x.unsqueeze(1)\n",
    "\n",
    "        self.fluxes = ensure_3d(fluxes)\n",
    "        self.wavelengths = ensure_3d(wavelengths)\n",
    "        self.errors = ensure_3d(errors)\n",
    "        self.masks = ensure_3d(masks)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fluxes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.fluxes[idx], self.wavelengths[idx], self.errors[idx], self.masks[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daa35f4-f6c3-4b04-b947-13a86e9743f2",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93943fcc-eeb9-4e9e-91de-4719527b76d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNAutoencoderWithSkip(nn.Module):\n",
    "    \"\"\"\n",
    "    A Convolutional Neural Network (CNN)-based autoencoder with skip connections for reconstructing spectral data.\n",
    "    \n",
    "    This model is designed to reduce the dimensionality of spectral data, capturing essential features \n",
    "    during encoding and reconstructing the data during decoding. Skip connections help retain \n",
    "    detailed information that may otherwise be lost in deeper layers, improving reconstruction \n",
    "    quality for tasks like anomaly detection or dimensionality reduction.\n",
    "    \n",
    "    Attributes:\n",
    "    - encoder1 (nn.Conv1d): First convolutional layer of the encoder, reducing input dimensions.\n",
    "    - encoder2 (nn.Conv1d): Second convolutional layer of the encoder, further reducing dimensions.\n",
    "    - encoder3 (nn.Conv1d): Final convolutional layer of the encoder, creating a compressed representation.\n",
    "    - decoder3 (nn.ConvTranspose1d): First transposed convolutional layer of the decoder.\n",
    "    - decoder2 (nn.ConvTranspose1d): Second transposed convolutional layer of the decoder.\n",
    "    - decoder1 (nn.ConvTranspose1d): Final transposed convolutional layer of the decoder, outputting the reconstructed data.\n",
    "    \n",
    "    Methods:\n",
    "    - forward(x): Defines the forward pass of the autoencoder. The input data `x` passes through the encoder \n",
    "      layers to compress it, and then through the decoder layers to reconstruct it. Skip connections are \n",
    "      used to combine encoder and decoder layers at corresponding depths.\n",
    "      \n",
    "    Returns:\n",
    "    - torch.Tensor: The reconstructed tensor, with values squashed between 0 and 1 using a sigmoid activation.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CNNAutoencoderWithSkip, self).__init__()\n",
    "        # Encoder layers with reduced channels\n",
    "        self.encoder1 = nn.Conv1d(1, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.encoder2 = nn.Conv1d(64, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.encoder3 = nn.Conv1d(32, 16, kernel_size=3, stride=2, padding=1)  \n",
    "\n",
    "        # Decoder layers with reduced channels\n",
    "        self.decoder3 = nn.ConvTranspose1d(16, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.decoder2 = nn.ConvTranspose1d(32, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.decoder1 = nn.ConvTranspose1d(64, 1, kernel_size=3, stride=2, padding=1, output_padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding with skip connections\n",
    "        x1 = F.relu(self.encoder1(x))  \n",
    "        x2 = F.relu(self.encoder2(x1))  \n",
    "        x3 = F.relu(self.encoder3(x2))  \n",
    "\n",
    "        # Decoding with skip connections\n",
    "        x = F.relu(self.decoder3(x3))\n",
    "        \n",
    "        # Adjust sizes for skip connection with x2\n",
    "        if x2.size(2) > x.size(2):\n",
    "            x2 = x2[:, :, :x.size(2)]\n",
    "        elif x2.size(2) < x.size(2):\n",
    "            x = x[:, :, :x2.size(2)]\n",
    "        x = F.relu(self.decoder2(x + x2))\n",
    "        \n",
    "        # Adjust sizes for skip connection with x1\n",
    "        if x1.size(2) > x.size(2):\n",
    "            x1 = x1[:, :, :x.size(2)]\n",
    "        elif x1.size(2) < x.size(2):\n",
    "            x = x[:, :, :x1.size(2)]\n",
    "        x = self.decoder1(x + x1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ce0ae-057f-4022-9063-9b0244e3639e",
   "metadata": {},
   "source": [
    "# Weighted Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4b43a37-6fbd-40da-ac32-31baab9d42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mse_loss(observed_flux, target_flux, mask, error_on_observed_flux):\n",
    "    \"\"\"\n",
    "    Calculates a modified Mean Squared Error (MSE) loss based on the \"significance difference\"\n",
    "    between observed and target flux, adjusted by the observational error.\n",
    "\n",
    "    Parameters:\n",
    "    - observed_flux (torch.Tensor): The model's predicted flux values (reconstructed spectra).\n",
    "    - target_flux (torch.Tensor): The ground truth flux values to be compared against observed_flux.\n",
    "    - mask (torch.Tensor): A binary mask that identifies which regions in the spectra should be included\n",
    "                           in the loss calculation, typically to ignore padded regions.\n",
    "    - error_on_observed_flux (torch.Tensor): Observational error associated with each flux point.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: The mean significance-based MSE loss, adjusted by the mask to ignore specific regions.\n",
    "\n",
    "    Process:\n",
    "    1. Compute the \"significance difference\" by normalising the difference between observed and target flux\n",
    "       using `error_on_observed_flux`, which represents the observational uncertainty.\n",
    "    2. Square the resulting \"significance difference\" to get a MSE-like loss, which is focused on relative\n",
    "       significance rather than absolute difference.\n",
    "    3. Apply the mask to this significance-based MSE loss to focus only on relevant parts of the spectrum.\n",
    "    4. Return the mean of the masked, significance-based MSE loss.\n",
    "    \"\"\"\n",
    "    significance_difference = (observed_flux - target_flux) / (error_on_observed_flux + 1e-5)\n",
    "    mse_loss = significance_difference ** 2\n",
    "    \n",
    "    return (mse_loss * mask).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92687470-04e6-4e7f-b9fb-f66ca2c1dfe7",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06eb68dd-26b0-4b61-bb31-6546a8ef4720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(model, data, mask, errors, epochs=50, batch_size=8, lr=0.001, \n",
    "                      grad_clip=1.0, l1_lambda=1e-5):\n",
    "    \"\"\"\n",
    "   Trains an autoencoder model using mini-batch gradient descent with gradient clipping and L1 regularization\n",
    "    and a custom weighted MSE loss function that incorporates errors.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): Autoencoder model.\n",
    "    - data (torch.Tensor): Input tensor (spectra).\n",
    "    - mask (torch.Tensor): Padding mask tensor.\n",
    "    - errors (torch.Tensor): Observational errors.\n",
    "    - epochs (int): Number of training epochs.\n",
    "    - batch_size (int): Batch size for training.\n",
    "    - lr (float): Learning rate.\n",
    "    - grad_clip (float): Gradient clipping value.\n",
    "    - l1_lambda (float): L1 regularization strength.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0\n",
    "        optimizer.zero_grad()  \n",
    "        \n",
    "        for i in range(0, len(data), batch_size):\n",
    "            batch_data = data[i:i+batch_size]\n",
    "            mask_batch = mask[i:i+batch_size]\n",
    "            error_batch = errors[i:i+batch_size]\n",
    "            \n",
    "            reconstructed = model(batch_data)\n",
    "            mse_loss = weighted_mse_loss(reconstructed, batch_data, mask_batch, error_batch)\n",
    "\n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters() if p.requires_grad)\n",
    "            total_loss = mse_loss + (l1_lambda * l1_norm)\n",
    "\n",
    "            total_loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            epoch_loss += total_loss.item()\n",
    "\n",
    "        epoch_duration = time.time() - start_time\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(data):.8f}, Time: {epoch_duration:.2f}s\")\n",
    "\n",
    "    print(f\"Final Loss after {epochs} epochs: {epoch_loss/len(data):.8f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917bdccf-a756-4c34-bf78-813f0822c514",
   "metadata": {},
   "source": [
    "# Anomaly Detection and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c84cce2-c84f-43d9-b7ec-3c3a62837389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalous_regions(original_fluxes, reconstructed_fluxes, window_size=50, \n",
    "                             abs_residual_threshold=0.1, rel_residual_threshold=0.1, \n",
    "                             range_mismatch_factor=1.5, overall_anomaly_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Identifies anomalous regions within spectra using residuals and mismatch factors,\n",
    "    with added logging to justify each detected anomaly and the triggering value.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    original_fluxes : np.ndarray\n",
    "        A 2D array of original flux values from the spectra, with shape (num_spectra, spectrum_length).\n",
    "        Each row represents a spectrum, and each column corresponds to a specific wavelength or flux point.\n",
    "\n",
    "    reconstructed_fluxes : np.ndarray\n",
    "        A 2D array of reconstructed flux values from the model, with the same shape as `original_fluxes`.\n",
    "        These are the model’s best attempt to recreate the original spectra, used to compute errors.\n",
    "\n",
    "    window_size : int, optional, default=50\n",
    "        The size of the sliding window (in data points) used to analyse local regions within each spectrum.\n",
    "\n",
    "    abs_residual_threshold : float, optional, default=0.1\n",
    "        The fixed threshold value for absolute residuals beyond which regions are flagged as anomalous.\n",
    "\n",
    "    rel_residual_threshold : float, optional, default=0.1\n",
    "        The fixed threshold value for relative residuals beyond which regions are flagged as anomalous.\n",
    "\n",
    "    range_mismatch_factor : float, optional, default=1.5\n",
    "        A factor to identify anomalies based on the range mismatch between original and reconstructed fluxes.\n",
    "\n",
    "    overall_anomaly_threshold : float, optional, default=0.3\n",
    "        The minimum fraction of a spectrum that must be flagged as anomalous to classify the entire spectrum as anomalous.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    anomalies : list of np.ndarray\n",
    "        A list of boolean arrays, each representing a spectrum. Each array has True at positions corresponding to\n",
    "        flagged regions in the spectrum and False otherwise.\n",
    "\n",
    "    spectrum_anomalies : np.ndarray\n",
    "        A 1D boolean array where each element represents whether the corresponding spectrum is classified as anomalous.\n",
    "\n",
    "    anomaly_metadata : list of list of dict\n",
    "        Metadata for each flagged region in each spectrum, with information on the range of the anomaly,\n",
    "        the type of anomaly (high absolute residual, high relative residual, or range mismatch), and the value that triggered the flag.\n",
    "\n",
    "    abs_residual_threshold : float\n",
    "        The fixed threshold value for absolute residuals beyond which regions are flagged as anomalous.\n",
    "\n",
    "    rel_residual_threshold : float\n",
    "        The fixed threshold value for relative residuals beyond which regions are flagged as anomalous.\n",
    "\n",
    "    range_mismatch_factor : float\n",
    "        The factor used to determine if the reconstructed flux range mismatch flags a region as anomalous.\n",
    "    \"\"\"\n",
    "    num_spectra, spectrum_length = original_fluxes.shape\n",
    "    absolute_residuals = np.abs(original_fluxes - reconstructed_fluxes)\n",
    "    relative_residuals = np.abs((original_fluxes - reconstructed_fluxes) / (original_fluxes + 1e-5))\n",
    "\n",
    "    anomalies, spectrum_anomalies = [], np.zeros(num_spectra, dtype=bool)\n",
    "    anomaly_metadata = []\n",
    "\n",
    "    for i in range(num_spectra):\n",
    "        spectrum_anomalies_count = 0\n",
    "        spectrum_anomaly_flags = np.zeros(spectrum_length, dtype=bool)\n",
    "        anomaly_justification = []\n",
    "\n",
    "        for start in range(0, spectrum_length - window_size + 1, window_size // 2):\n",
    "            end = start + window_size\n",
    "            window_abs_residual = float(np.mean(absolute_residuals[i, start:end]))\n",
    "            window_rel_residual = float(np.mean(relative_residuals[i, start:end]))\n",
    "            original_range = float(np.ptp(original_fluxes[i, start:end]))\n",
    "            reconstructed_range = float(np.ptp(reconstructed_fluxes[i, start:end]))\n",
    "\n",
    "            if (window_abs_residual > abs_residual_threshold or\n",
    "                window_rel_residual > rel_residual_threshold or\n",
    "                reconstructed_range > original_range * range_mismatch_factor):\n",
    "                \n",
    "                spectrum_anomaly_flags[start:end] = True\n",
    "                spectrum_anomalies_count += end - start\n",
    "\n",
    "                reason = {\n",
    "                    \"range_start\": start,\n",
    "                    \"range_end\": end,\n",
    "                    \"reason_type\": (\n",
    "                        \"high_absolute_residual\" if window_abs_residual > abs_residual_threshold else\n",
    "                        \"high_relative_residual\" if window_rel_residual > rel_residual_threshold else\n",
    "                        \"range_mismatch\"\n",
    "                    ),\n",
    "                    \"trigger_value\": (\n",
    "                        window_abs_residual if window_abs_residual > abs_residual_threshold else\n",
    "                        window_rel_residual if window_rel_residual > rel_residual_threshold else\n",
    "                        reconstructed_range / (original_range + 1e-5)\n",
    "                    )\n",
    "                }\n",
    "                anomaly_justification.append(reason)\n",
    "\n",
    "        spectrum_anomalies[i] = spectrum_anomalies_count / spectrum_length > overall_anomaly_threshold \n",
    "        anomalies.append(spectrum_anomaly_flags)\n",
    "        anomaly_metadata.append(anomaly_justification)\n",
    "\n",
    "    return anomalies, spectrum_anomalies, anomaly_metadata, abs_residual_threshold, rel_residual_threshold, range_mismatch_factor\n",
    "\n",
    "def fitness_function(params, data):\n",
    "    \"\"\"\n",
    "    Calculates the fitness score for anomaly detection thresholds.\n",
    "    \n",
    "    Parameters:\n",
    "    - params (dict): Contains thresholds for:\n",
    "        - abs_residual_threshold\n",
    "        - rel_residual_threshold\n",
    "        - range_mismatch_factor\n",
    "        - overall_anomaly_threshold\n",
    "    - data (dict): Contains:\n",
    "        - fluxes (original spectra)\n",
    "        - reconstructed_fluxes (from the model)\n",
    "        \n",
    "    Returns:\n",
    "    - float: Fitness score (closer to 0 is better).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        abs_residual_threshold = params[\"abs_residual_threshold\"]\n",
    "        rel_residual_threshold = params[\"rel_residual_threshold\"]\n",
    "        range_mismatch_factor = params[\"range_mismatch_factor\"]\n",
    "        overall_anomaly_threshold = params[\"overall_anomaly_threshold\"]\n",
    "\n",
    "        original_fluxes = data[\"fluxes\"]\n",
    "        reconstructed_fluxes = data[\"reconstructed_fluxes\"]\n",
    "\n",
    "        anomalies, spectrum_anomalies, _, _, _, _ = detect_anomalous_regions(\n",
    "            original_fluxes=original_fluxes,\n",
    "            reconstructed_fluxes=reconstructed_fluxes,\n",
    "            abs_residual_threshold=abs_residual_threshold,\n",
    "            rel_residual_threshold=rel_residual_threshold,\n",
    "            range_mismatch_factor=range_mismatch_factor,\n",
    "            overall_anomaly_threshold=overall_anomaly_threshold,\n",
    "        )\n",
    "\n",
    "        reconstruction_errors = compute_reconstruction_errors(original_fluxes, reconstructed_fluxes, method=\"mse\")\n",
    "        mean_reconstruction_error = np.mean(reconstruction_errors)\n",
    "\n",
    "        anomaly_ratio = np.mean(spectrum_anomalies)\n",
    "\n",
    "        false_negative_penalty = np.mean([\n",
    "            1 if not spectrum_anomalies[i] and np.any(anomalies[i]) else 0\n",
    "            for i in range(len(original_fluxes))\n",
    "        ])\n",
    "        false_positive_penalty = np.mean([\n",
    "            1 if spectrum_anomalies[i] and not np.any(anomalies[i]) else 0\n",
    "            for i in range(len(original_fluxes))\n",
    "        ])\n",
    "\n",
    "        # Add a clear penalty for excessive anomalies\n",
    "        excessive_anomaly_penalty = max(0, anomaly_ratio - 0.5) ** 2  # Penalize ratios above 50%\n",
    "\n",
    "        fitness = (\n",
    "            -mean_reconstruction_error * 10\n",
    "            - false_negative_penalty * 5\n",
    "            - false_positive_penalty * 5\n",
    "            - excessive_anomaly_penalty * 20  \n",
    "        )\n",
    "        return fitness,\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Fitness function failed: {e}\")\n",
    "        return -1e6,\n",
    "\n",
    "\n",
    "def detection_parameters(data, param_bounds, pop_size=20, generations=50, mutation_rate=0.2):\n",
    "    \"\"\"\n",
    "    Optimise anomaly detection thresholds using a genetic algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    - data (dict): Contains 'fluxes' and 'reconstructed_fluxes'.\n",
    "    - param_bounds (dict): Parameter boundaries for optimisation.\n",
    "    - pop_size (int): Population size.\n",
    "    - generations (int): Number of generations.\n",
    "    - mutation_rate (float): Probability of mutation.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Best parameter configuration found.\n",
    "    \"\"\"\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    for key, bounds in param_bounds.items():\n",
    "        toolbox.register(f\"attr_{key}\", random.uniform, bounds[0], bounds[1])\n",
    "\n",
    "    toolbox.register(\n",
    "        \"individual\",\n",
    "        tools.initCycle,\n",
    "        creator.Individual,\n",
    "        tuple(getattr(toolbox, f\"attr_{key}\") for key in param_bounds.keys()),\n",
    "        n=1,\n",
    "    )\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "    def deap_fitness(individual):\n",
    "        params = {key: individual[idx] for idx, key in enumerate(param_bounds.keys())}\n",
    "        return fitness_function(params, data)\n",
    "\n",
    "    toolbox.register(\"evaluate\", deap_fitness)\n",
    "\n",
    "    min_bounds = [bounds[0] for bounds in param_bounds.values()]\n",
    "    max_bounds = [bounds[1] for bounds in param_bounds.values()]\n",
    "    toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
    "    toolbox.register(\"mutate\", tools.mutPolynomialBounded, eta=20.0, low=min_bounds, up=max_bounds, indpb=mutation_rate)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "    population = toolbox.population(n=pop_size)\n",
    "\n",
    "    for gen in tqdm(range(generations), desc=\"Genetic Algorithm Progress\"):\n",
    "        offspring = toolbox.select(population, len(population))\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if random.random() < 0.5:\n",
    "                toolbox.mate(child1, child2)\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "\n",
    "        for mutant in offspring:\n",
    "            if random.random() < mutation_rate:\n",
    "                toolbox.mutate(mutant)\n",
    "                del mutant.fitness.values\n",
    "\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        population[:] = offspring\n",
    "\n",
    "    best_individual = tools.selBest(population, k=1)[0]\n",
    "    best_params = {key: best_individual[idx] for idx, key in enumerate(param_bounds.keys())}\n",
    "    best_fitness = best_individual.fitness.values[0]\n",
    "    print(f\"Optimised Parameters: {best_params}\")\n",
    "    print(f\"Best Fitness: {best_fitness}\")\n",
    "    return best_params, best_fitness\n",
    "def compute_dynamic_anomaly_ratio(reconstruction_errors, mad_multiplier=3):\n",
    "    \"\"\"\n",
    "    Dynamically computes the anomaly ratio using the Median Absolute Deviation (MAD).\n",
    "\n",
    "    Parameters:\n",
    "    - reconstruction_errors (np.ndarray): Array of reconstruction errors.\n",
    "    - mad_multiplier (float): Multiplier for the MAD to define the threshold.\n",
    "\n",
    "    Returns:\n",
    "    - float: Target anomaly ratio based on the error distribution.\n",
    "    \"\"\"\n",
    "    median_error = np.median(reconstruction_errors)\n",
    "    mad_error = np.median(np.abs(reconstruction_errors - median_error))\n",
    "    threshold = median_error + mad_multiplier * mad_error\n",
    "\n",
    "    anomalies = reconstruction_errors > threshold\n",
    "    target_anomaly_ratio = np.mean(anomalies)  \n",
    "\n",
    "    print(f\"Dynamic Anomaly Ratio: {target_anomaly_ratio:.4f} (Threshold: {threshold:.4f})\")\n",
    "    return target_anomaly_ratio\n",
    "def compute_reconstruction_errors(original_fluxes, reconstructed_fluxes, method=\"mse\"):\n",
    "    \"\"\"\n",
    "    Computes reconstruction errors between original and reconstructed fluxes.\n",
    "\n",
    "    Parameters:\n",
    "    - original_fluxes (np.ndarray): Original flux values.\n",
    "    - reconstructed_fluxes (np.ndarray): Reconstructed flux values from the model.\n",
    "    - method (str): Error computation method, either 'mse' for mean squared error or 'mae' for mean absolute error.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Array of errors with shape (num_spectra, num_wavelengths).\n",
    "    \"\"\"\n",
    "    if method == \"mse\":\n",
    "        errors = (original_fluxes - reconstructed_fluxes) ** 2\n",
    "    elif method == \"mae\":\n",
    "        errors = np.abs(original_fluxes - reconstructed_fluxes)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'mse' or 'mae'\")\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7752a8a7-bdea-4c27-a953-b3c0786606c2",
   "metadata": {},
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5a2ccde-21fa-447c-aaae-af6ebe611bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Parameter Space ---\n",
    "param_space = [\n",
    "    Real(0.0, 1.0, name=\"abs_residual_threshold\"),\n",
    "    Real(0.05, 2.0, name=\"rel_residual_threshold\"),\n",
    "    Real(1.0, 3.0, name=\"range_mismatch_factor\"),\n",
    "    Real(0.0, 1.0, name=\"overall_anomaly_threshold\"),\n",
    "]\n",
    "\n",
    "# --- Define Objective Function ---\n",
    "@use_named_args(param_space)\n",
    "def objective_function(**params):\n",
    "    fitness, = fitness_function(params, data_for_bo)  \n",
    "    return -fitness\n",
    "def plot_bo_results(bo_results, param_names):\n",
    "    \"\"\"\n",
    "    Generates visualizations for Bayesian Optimization results:\n",
    "    1. Convergence plot (Fitness improvement over iterations)\n",
    "    2. Parameter evaluations (How parameters were sampled)\n",
    "    3. Objective function heatmap (Relationship between parameters)\n",
    "\n",
    "    Parameters:\n",
    "    - bo_results: The result object from gp_minimize\n",
    "    - param_names: List of parameter names corresponding to bo_results\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(10, 18))  # More vertical space\n",
    "    \n",
    "    # Plot convergence (Fitness over iterations)\n",
    "    plot_convergence(bo_results, ax=axes[0])\n",
    "    axes[0].set_title(\"Bayesian Optimization Convergence\", fontsize=14)\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # Plot evaluations (Parameter search distribution)\n",
    "    plot_evaluations(bo_results, dimensions=param_names, ax=axes[1])\n",
    "    axes[1].set_title(\"Parameter Evaluations\", fontsize=14)\n",
    "    \n",
    "    # Plot objective function relationships\n",
    "    plot_objective(bo_results, dimensions=param_names, ax=axes[2])\n",
    "    axes[2].set_title(\"Parameter Relationships\", fontsize=14)\n",
    "\n",
    "    # Adjust layout for better spacing\n",
    "    plt.tight_layout(pad=4.0, h_pad=4.0)  # Adds space between subplots\n",
    "    plt.show()\n",
    "def plot_bo_3d_grid(bo_results, save_directory='SpectralCNNAutoencoder_output'):\n",
    "    \"\"\"\n",
    "    Generates a structured grid of visualizations for Bayesian Optimization results:\n",
    "    - Diagonal: 1D Histograms of individual parameter distributions.\n",
    "    - Lower triangle: 3D surface plots with fitness as the Z-axis.\n",
    "    - Upper triangle: 2D KDE density plots to show optimizer search behavior.\n",
    "\n",
    "    Parameters:\n",
    "    - bo_results: The result object from `gp_minimize`\n",
    "    - save_directory: Directory to save the figure.\n",
    "    \"\"\"\n",
    "\n",
    "    param_names = [\"Absolute Threshold\", \"Relative Threshold\", \"Range Disparity\", \"Overall Threshold\"]\n",
    "    param_values = np.array(bo_results.x_iters)\n",
    "    fitness_vals = np.array(bo_results.func_vals)\n",
    "    num_params = len(param_names)\n",
    "\n",
    "    fig, axes = plt.subplots(num_params, num_params, figsize=(18, 18)) \n",
    "\n",
    "    for i in range(num_params):\n",
    "        for j in range(num_params):\n",
    "            ax = axes[i, j]\n",
    "\n",
    "            if i == j:\n",
    "                ax.hist(param_values[:, i], bins=20, color=\"steelblue\", alpha=0.7, edgecolor=\"black\")\n",
    "                ax.set_xlabel(param_names[i], fontsize=14, labelpad=10)\n",
    "                ax.set_ylabel(\"Frequency\", fontsize=14, labelpad=10)\n",
    "                ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "            elif i > j:\n",
    "                ax.remove()\n",
    "                ax = fig.add_subplot(num_params, num_params, i*num_params + j + 1, projection='3d')\n",
    "                x_vals, y_vals = param_values[:, j], param_values[:, i]\n",
    "                z_vals = fitness_vals  \n",
    "                grid_x, grid_y = np.meshgrid(\n",
    "                    np.linspace(min(x_vals), max(x_vals), 50),\n",
    "                                    np.linspace(min(y_vals), max(y_vals), 50)\n",
    "                )\n",
    "                grid_z = griddata((x_vals, y_vals), z_vals, (grid_x, grid_y), method='cubic', fill_value=np.nan)\n",
    "                grid_z = np.nan_to_num(grid_z, nan=np.nanmin(grid_z))\n",
    "                \n",
    "                ax.plot_surface(grid_x, grid_y, grid_z, cmap=\"coolwarm_r\", alpha=0.8, edgecolor=\"k\", linewidth=0.3)\n",
    "\n",
    "                ax.set_xlabel(param_names[j], fontsize=14, labelpad=10)\n",
    "                ax.set_ylabel(param_names[i], fontsize=14, labelpad=10)\n",
    "                ax.set_zlabel(\"Fitness\", fontsize=14, labelpad=10)\n",
    "\n",
    "                ax.zaxis.labelpad = -150 \n",
    "                ax.zaxis.label.set_rotation(90)  \n",
    "                ax.zaxis.label.set_verticalalignment('bottom')  \n",
    "                \n",
    "                ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "            else:\n",
    "                x_vals, y_vals = param_values[:, j], param_values[:, i]\n",
    "\n",
    "                xy = np.vstack([x_vals, y_vals])\n",
    "                density = gaussian_kde(xy)(xy)\n",
    "\n",
    "                grid_x, grid_y = np.meshgrid(\n",
    "                    np.linspace(min(x_vals), max(x_vals), 100),\n",
    "                    np.linspace(min(y_vals), max(y_vals), 100)\n",
    "                )\n",
    "                grid_z = griddata((x_vals, y_vals), density, (grid_x, grid_y), method='cubic')\n",
    "\n",
    "                ax.contourf(grid_x, grid_y, grid_z, levels=10, cmap=\"coolwarm\", alpha=0.7)  \n",
    "                ax.scatter(x_vals, y_vals, c=density, cmap=\"coolwarm\", s=50, alpha=0.8, edgecolor=\"black\")\n",
    "\n",
    "                ax.set_xlabel(param_names[j], fontsize=14, labelpad=10)\n",
    "                ax.set_ylabel(param_names[i], fontsize=14, labelpad=10)\n",
    "                ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "                \n",
    "    cbar_ax = fig.add_axes([0.15, 0.02, 0.7, 0.015])\n",
    "    sm = plt.cm.ScalarMappable(cmap=\"coolwarm_r\", norm=plt.Normalize(min(fitness_vals), max(fitness_vals)))\n",
    "    sm.set_array([])  \n",
    "    cbar = fig.colorbar(sm, cax=cbar_ax, orientation='horizontal')\n",
    "    cbar.set_label(\"Fitness Score (Lower is better)\", fontsize=16)\n",
    "\n",
    "    plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.08, wspace=0.3, hspace=0.3)\n",
    "\n",
    "    base_filename = \"bo_results_3d_grid\"\n",
    "    save_path = create_save_path(save_directory, base_filename)\n",
    "\n",
    "    plt.savefig(save_path, dpi=600, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Plot saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f262e38-144e-40ee-800f-6dede74973f7",
   "metadata": {},
   "source": [
    "# Visualisation and Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4876f74-3ed5-4531-9552-70c0be4afece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectra_combined(\n",
    "    original_fluxes, reconstructed_fluxes, wavelengths, spec_data,\n",
    "    anomalous_regions, spectrum_anomalies,\n",
    "    save_directory='SpectralCNNAutoencoder_output',\n",
    "    max_samples=10, seed=None, only_anomalous=False\n",
    "):\n",
    "    def fetch_image(ra, dec, pixscale=0.262, width=256, height=256, survey=\"ls-dr10\"):\n",
    "        url = f\"https://www.legacysurvey.org/viewer/jpeg-cutout?ra={ra}&dec={dec}&pixscale={pixscale}&layer={survey}&size={max(width, height)}\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                return Image.open(BytesIO(response.content)).convert(\"RGBA\")\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    if seed is None:\n",
    "        seed = random.randint(0, 10000)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    indices = [i for i, is_anom in enumerate(spectrum_anomalies) if is_anom] if only_anomalous else list(range(len(original_fluxes)))\n",
    "    if not indices:\n",
    "        print(\"No spectra to plot.\")\n",
    "        return [], [], seed\n",
    "\n",
    "    selected_indices = random.sample(indices, min(max_samples, len(indices)))\n",
    "    fig, axes = plt.subplots(len(selected_indices), 2, figsize=(20, 6 * len(selected_indices)), gridspec_kw={'width_ratios': [3, 1]})\n",
    "    if len(selected_indices) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for idx, i in enumerate(selected_indices):\n",
    "        ra, dec = spec_data.iloc[i]['mean_fiber_ra'], spec_data.iloc[i]['mean_fiber_dec']\n",
    "        x_axis = wavelengths if wavelengths is not None else range(len(original_fluxes[i]))\n",
    "\n",
    "        ax_spectra = axes[idx][0] if isinstance(axes[idx], (list, tuple)) else axes[0]\n",
    "        ax_spectra.plot(x_axis, original_fluxes[i], label=\"Original\", color='#2c7bb6', linewidth=0.5)\n",
    "        ax_spectra.plot(x_axis, reconstructed_fluxes[i], label=\"Reconstructed\", color='#d7191c', alpha=0.7, linewidth=0.5)\n",
    "\n",
    "        in_anomaly = False\n",
    "        anomaly_start = 0\n",
    "        for j in range(len(x_axis)):\n",
    "            if anomalous_regions[i][j] and not in_anomaly:\n",
    "                anomaly_start = j\n",
    "                in_anomaly = True\n",
    "            elif not anomalous_regions[i][j] and in_anomaly:\n",
    "                ax_spectra.axvspan(x_axis[anomaly_start], x_axis[j], color='#fdae61', alpha=0.7)\n",
    "                in_anomaly = False\n",
    "        if in_anomaly:\n",
    "            ax_spectra.axvspan(x_axis[anomaly_start], x_axis[-1], color='#fdae61', alpha=0.7)\n",
    "\n",
    "        if spectrum_anomalies[i]:\n",
    "            ax_spectra.set_facecolor('#fee090')\n",
    "            ax_spectra.set_title(f\"Spectrum ID: {spec_data['targetid'].iloc[i]} - Anomalous\", color='red', fontsize=23)\n",
    "        else:\n",
    "            ax_spectra.set_title(f\"Spectrum ID: {spec_data['targetid'].iloc[i]}\", color='black', fontsize=23)\n",
    "\n",
    "        ax_spectra.xaxis.set_visible(False)\n",
    "        ax_spectra.set_ylabel(\"Flux (normalised)\", fontsize=20)\n",
    "        ax_spectra.legend()\n",
    "\n",
    "        divider = make_axes_locatable(ax_spectra)\n",
    "        ax_residual = divider.append_axes(\"bottom\", size=\"25%\", pad=0, sharex=ax_spectra)\n",
    "        ax_residual.plot(x_axis, original_fluxes[i] - reconstructed_fluxes[i], color='#4dac26', linewidth=0.5)\n",
    "        ax_residual.axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "        ax_residual.set_ylabel(\"Residual\", fontsize=20)\n",
    "        ax_residual.set_xlabel(\"Wavelength (Å)\", fontsize=20)\n",
    "\n",
    "        ax_image = axes[idx][1] if isinstance(axes[idx], (list, tuple)) else axes[1]\n",
    "        img_data = fetch_image(ra, dec)\n",
    "        if img_data:\n",
    "            ax_image.imshow(img_data)\n",
    "            ax_image.set_title(f\"Galaxy Image\\nRA={ra:.4f}, Dec={dec:.4f}\", fontsize=23)\n",
    "            circle = patches.Circle((img_data.width / 2, img_data.height / 2), 30, edgecolor=\"white\", facecolor=\"none\", linewidth=2)\n",
    "            ax_image.add_patch(circle)\n",
    "        else:\n",
    "            ax_image.text(0.5, 0.5, \"Image Not Found\", ha=\"center\", va=\"center\", fontsize=14)\n",
    "        ax_image.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    base_filename = \"anomalous_spectra\" if only_anomalous else \"spectra_reconstruction\"\n",
    "    save_path = create_save_path(save_directory, base_filename)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved plot to {save_path}, seed: {seed}\")\n",
    "    return indices, selected_indices, seed\n",
    "\n",
    "def plot_reconstruction_error_distribution(reconstruction_errors, wavelengths, save_directory='output_images'):\n",
    "    \"\"\"\n",
    "    Visualises the reconstruction error distribution across wavelengths with both linear and log-scale plots.\n",
    "\n",
    "    Parameters:\n",
    "    - reconstruction_errors (np.ndarray): Array of reconstruction errors per spectrum and wavelength.\n",
    "    - wavelengths (array-like): Array of wavelength values corresponding to each error point.\n",
    "    - save_directory (str, optional): Directory path to save the generated plot image.\n",
    "    \"\"\"\n",
    "    if wavelengths is None or len(wavelengths) != reconstruction_errors.shape[1]:\n",
    "        raise ValueError(\"Wavelengths array must be provided and must match the number of wavelengths in reconstruction errors.\")\n",
    "    \n",
    "    mean_errors = np.mean(reconstruction_errors, axis=0)\n",
    "\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "    # --- Linear Scale Plot ---\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(wavelengths, mean_errors, label='Mean Reconstruction Error', color='#0571b0', linewidth=0.5)\n",
    "    plt.fill_between(wavelengths, mean_errors, color='#67a9cf', alpha=0.7)\n",
    "    plt.xlabel(\"Wavelength (Å)\")\n",
    "    plt.ylabel(\"Mean Reconstruction Error\")\n",
    "    plt.legend()\n",
    "    \n",
    "    linear_plot_path = create_save_path(save_directory, 'mean_reconstruction_error_linear')\n",
    "    plt.savefig(linear_plot_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Linear plot saved to {linear_plot_path}\")\n",
    "\n",
    "    # --- Log Scale Plot ---\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(wavelengths, mean_errors, label='Mean Reconstruction Error (Log Scale)', color='#0571b0', linewidth=0.5)\n",
    "    plt.fill_between(wavelengths, mean_errors, color='#67a9cf', alpha=0.7)\n",
    "    plt.xlabel(\"Wavelength (Å)\")\n",
    "    plt.ylabel(\"Mean Reconstruction Error (Log Scale)\")\n",
    "    plt.yscale(\"log\")  # Log scale applied here\n",
    "    plt.legend()\n",
    "    log_plot_path = create_save_path(save_directory, 'mean_reconstruction_error_log')\n",
    "    plt.savefig(log_plot_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Log plot saved to {log_plot_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807b21dd-e691-4e6d-a2a4-ce6640a5676b",
   "metadata": {},
   "source": [
    "# External Database Cross-Matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b7aa583-1db5-4627-9762-cb8782337d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_anomalies(spectrum_anomalies, spec_data, radius_arcsec=1.5):\n",
    "    \"\"\"\n",
    "    Searches multiple spectral databases (ZTF, Gaia Alerts) for known anomalies and retrieves images.\n",
    "    Identifies whether an object has had an alert and extracts alert classifications.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    spectrum_anomalies : np.ndarray\n",
    "        Boolean array indicating which spectra are anomalous.\n",
    "\n",
    "    spec_data : pd.DataFrame\n",
    "        Metadata containing RA/DEC for each spectrum, allowing for database lookups.\n",
    "\n",
    "    radius_arcsec : float, default=1.5\n",
    "        Search radius in arcseconds.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    results : dict\n",
    "        Dictionary mapping coordinates to anomaly data, alert sources, and retrieved images.\n",
    "    \"\"\"\n",
    "    anomalous_indices = [i for i, is_anomalous in enumerate(spectrum_anomalies) if is_anomalous]\n",
    "\n",
    "    if not anomalous_indices:\n",
    "        print(\"No anomalous spectra detected.\")\n",
    "        return {}\n",
    "\n",
    "    anomalous_coords = [\n",
    "        (float(spec_data.iloc[i]['mean_fiber_ra']), float(spec_data.iloc[i]['mean_fiber_dec']))\n",
    "        for i in anomalous_indices\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for ra, dec in tqdm(anomalous_coords, desc=\"Searching anomalies\", unit=\"coord\"):\n",
    "        query_results = {}\n",
    "        alert_sources = []\n",
    "\n",
    "        print(f\"\\nSearching for anomalies at RA: {ra}, DEC: {dec} (±{radius_arcsec} arcsec)\")\n",
    "\n",
    "        ztf_url = f\"https://irsa.ipac.caltech.edu/cgi-bin/ZTF/nph_light_curves?POS={ra},{dec}&SIZE={radius_arcsec/3600}&FORMAT=json\"\n",
    "        print(f\"ZTF Query URL: {ztf_url}\")\n",
    "\n",
    "        try:\n",
    "            ztf_response = requests.get(ztf_url, timeout=5)\n",
    "            if ztf_response.status_code == 200:\n",
    "                ztf_data = ztf_response.json()\n",
    "                if ztf_data and \"data\" in ztf_data and len(ztf_data[\"data\"]) > 0:\n",
    "                    alert_sources.append(\"ZTF\")\n",
    "                    print(f\"ZTF Alert Found: {ztf_data['data']}\")\n",
    "                else:\n",
    "                    print(\"No ZTF alerts found.\")\n",
    "            else:\n",
    "                print(\"ZTF query failed.\")\n",
    "        except requests.RequestException:\n",
    "            print(\"ZTF request failed.\")\n",
    "        \n",
    "        query_results[\"ZTF\"] = ztf_data if \"ztf_data\" in locals() else \"Query failed\"\n",
    "\n",
    "        gaia_url = f\"http://gsaweb.ast.cam.ac.uk/alerts/alerts/cone_search?ra={ra}&dec={dec}&radius={radius_arcsec/3600}\"\n",
    "        print(f\"Gaia Query URL: {gaia_url}\")\n",
    "\n",
    "        try:\n",
    "            gaia_response = requests.get(gaia_url, timeout=5)\n",
    "            if gaia_response.status_code == 200 and \"transient\" in gaia_response.text:\n",
    "                alert_sources.append(\"Gaia\")\n",
    "                print(f\"Gaia Alert Found: {gaia_response.text}\")\n",
    "            else:\n",
    "                print(\"No Gaia alerts found.\")\n",
    "        except requests.RequestException:\n",
    "            print(\"Gaia request failed.\")\n",
    "\n",
    "        query_results[\"Gaia\"] = gaia_response.text if \"gaia_response\" in locals() else \"Query failed\"\n",
    "\n",
    "        image_sources = {\n",
    "            \"SDSS\": f\"https://skyserver.sdss.org/dr16/SkyServerWS/ImgCutout/getjpeg?ra={ra}&dec={dec}&scale=0.2&width=300&height=300\",\n",
    "            \"Legacy Survey\": f\"https://www.legacysurvey.org/viewer/jpeg-cutout?ra={ra}&dec={dec}&layer=ls-dr10&size=300\"\n",
    "        }\n",
    "\n",
    "        fig, axes = plt.subplots(1, len(image_sources), figsize=(10, 5))\n",
    "\n",
    "        if len(image_sources) == 1:\n",
    "            axes = [axes]  \n",
    "\n",
    "        for idx, (source, url) in enumerate(image_sources.items()):\n",
    "            try:\n",
    "                img = Image.open(BytesIO(requests.get(url, timeout=5).content)).convert(\"RGBA\")\n",
    "                draw = ImageDraw.Draw(img)\n",
    "                width, height = img.size\n",
    "                center = (width // 2, height // 2)\n",
    "\n",
    "                radius_pixels = int((radius_arcsec / 0.04) * (width / 300))\n",
    "\n",
    "                draw.ellipse([\n",
    "                    (center[0] - radius_pixels, center[1] - radius_pixels),\n",
    "                    (center[0] + radius_pixels, center[1] + radius_pixels)\n",
    "                ], outline=\"white\", width=2)\n",
    "\n",
    "                axes[idx].imshow(img)\n",
    "                axes[idx].set_title(f\"{source} Image\")\n",
    "                axes[idx].axis(\"off\")\n",
    "            except:\n",
    "                axes[idx].set_title(f\"{source} Image Not Found\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        results[(ra, dec)] = {\n",
    "            \"database_results\": query_results,\n",
    "            \"alert_sources\": alert_sources,\n",
    "            \"image_urls\": image_sources\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612320de-e5f0-4023-9fc3-d38cbd81f8d7",
   "metadata": {},
   "source": [
    "# Information Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5673ed0e-8416-442b-b9f5-f502138bac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_save_path(save_directory, base_filename):\n",
    "    \"\"\"\n",
    "    Generates a unique file path for PNG files in the specified directory by appending\n",
    "    a sequential numeric suffix to avoid overwriting existing files. This ensures that\n",
    "    saved file names are uniquely numbered.\n",
    "\n",
    "    Parameters:\n",
    "    - save_directory (str): The directory where the file should be saved. It will be created if it doesn't exist.\n",
    "    - base_filename (str): The base name for the file, to which a numeric suffix will be added.\n",
    "\n",
    "    Returns:\n",
    "    - str: A full path to the new file with a sequentially numbered suffix in the specified directory,\n",
    "           following the format '{base_filename}_{number}.png'.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    existing_files = os.listdir(save_directory)\n",
    "    numbers = [\n",
    "        int(re.search(r'\\d+', f).group())\n",
    "        for f in existing_files if re.search(fr'{base_filename}_(\\d+)\\.png', f)\n",
    "    ]\n",
    "    next_number = max(numbers) + 1 if numbers else 1\n",
    "    full_path = os.path.join(save_directory, f'{base_filename}_{next_number}.png')\n",
    "    \n",
    "    relative_path = os.path.relpath(full_path, start=os.path.dirname(save_directory))\n",
    "    \n",
    "    return relative_path\n",
    "    \n",
    "def create_json_save_path(save_directory, base_filename):\n",
    "    \"\"\"\n",
    "    Generates a unique file path for JSON files in the specified directory by appending\n",
    "    a sequential numeric suffix to avoid overwriting existing files. This ensures that\n",
    "    saved JSON file names are uniquely numbered.\n",
    "\n",
    "    Parameters:\n",
    "    - save_directory (str): The directory where the JSON file should be saved. It will be created if it doesn't exist.\n",
    "    - base_filename (str): The base name for the JSON file, to which a numeric suffix will be added.\n",
    "\n",
    "    Returns:\n",
    "    - str: A full path to the new JSON file with a sequentially numbered suffix in the specified directory,\n",
    "           following the format '{base_filename}_{number}.json'.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    existing_files = os.listdir(save_directory)\n",
    "    numbers = [\n",
    "        int(re.search(rf'{base_filename}_(\\d+)\\.json', f).group(1))\n",
    "        for f in existing_files if re.search(rf'{base_filename}_(\\d+)\\.json', f)\n",
    "    ]\n",
    "    next_number = max(numbers) + 1 if numbers else 1\n",
    "    full_path = os.path.join(save_directory, f'{base_filename}_{next_number}.json')\n",
    "    relative_path = os.path.relpath(full_path, start=os.path.dirname(save_directory))\n",
    "    \n",
    "    return relative_path\n",
    "def save_sampling_info(galaxy_ids, plot_galaxy_ids, seed, anomaly_metadata, abs_residual_threshold, \n",
    "                       rel_residual_threshold, range_mismatch_factor, json_directory, spec_data):\n",
    "    \"\"\"\n",
    "    Saves detailed sampling information for detected anomalous spectra to a JSON file. This includes \n",
    "    metadata on each detected anomaly, threshold values, and unique target IDs associated with the anomalies.\n",
    "\n",
    "    Parameters:\n",
    "    - galaxy_ids (list of int): Indices of galaxies identified as anomalous, corresponding to row indices in `spec_data`.\n",
    "    - plot_galaxy_ids (list of int): Indices of galaxies selected for plotting, used to match target IDs if needed.\n",
    "    - seed (int): Random seed used during sampling to ensure reproducibility.\n",
    "    - anomaly_metadata (list of list of dict): Metadata for each galaxy's anomaly detection results, where each inner list\n",
    "      contains dictionaries with specific anomaly information (e.g., range, justification, trigger value).\n",
    "    - abs_residual_threshold (float): Threshold for detecting high absolute residual anomalies.\n",
    "    - rel_residual_threshold (float): Threshold for detecting high relative residual anomalies.\n",
    "    - range_mismatch_factor (float): Factor threshold for range mismatches between original and reconstructed spectra.\n",
    "    - json_directory (str): Directory path where the JSON file should be saved. Will be created if it doesn't exist.\n",
    "    - spec_data (DataFrame): DataFrame containing galaxy catalog information, including `targetid` for each galaxy.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function saves a JSON file containing the sampling and anomaly data.\n",
    "\n",
    "    Process:\n",
    "    1. Maps `galaxy_ids` to actual `targetid` values using `spec_data` to provide unique identifiers for each galaxy.\n",
    "    2. Constructs a dictionary `sampling_info` to store:\n",
    "       - Count of anomalies detected.\n",
    "       - Sampling seed for reproducibility.\n",
    "       - Thresholds for absolute residual, relative residual, and range mismatch.\n",
    "       - An `anomalous_spectra` dictionary keyed by `targetid`, where each entry holds `anomaly_ranges` metadata.\n",
    "    3. For each `targetid`, populates `anomaly_ranges` with details on each anomaly's range, justification, \n",
    "       and the value triggering the anomaly.\n",
    "    4. Uses `create_json_save_path` to generate a unique, sequentially numbered file path for saving.\n",
    "    5. Writes `sampling_info` as JSON to the generated path and confirms save location with a printed message.\n",
    "    \"\"\"\n",
    "    target_ids = [int(spec_data['targetid'].iloc[idx]) for idx in galaxy_ids]\n",
    "\n",
    "    sampling_info = {\n",
    "        \"Number of Anomalies\": len(target_ids),\n",
    "        \"Seed\": int(seed),\n",
    "        \"absolute_residual_threshold\": float(abs_residual_threshold),\n",
    "        \"relative_residual_threshold\": float(rel_residual_threshold),\n",
    "        \"range_mismatch_factor\": float(range_mismatch_factor),\n",
    "        \"anomalous_spectra\": {}\n",
    "    }\n",
    "\n",
    "    for idx, target_id in enumerate(target_ids):\n",
    "        metadata = anomaly_metadata[idx]\n",
    "        anomaly_ranges = []\n",
    "\n",
    "        for reason in metadata:\n",
    "            anomaly_ranges.append({\n",
    "                \"range_start\": reason[\"range_start\"],\n",
    "                \"range_end\": reason[\"range_end\"],\n",
    "                \"justification\": reason[\"reason_type\"],\n",
    "                \"value\": reason.get(\"trigger_value\", None)  \n",
    "            })\n",
    "\n",
    "        sampling_info[\"anomalous_spectra\"][target_id] = {\n",
    "            \"anomaly_ranges\": anomaly_ranges\n",
    "        }\n",
    "\n",
    "    json_path = create_json_save_path(json_directory, 'sampling_info')\n",
    "    secondary_path = create_json_save_path(OUT_DIR, 'sampling_info')\n",
    "\n",
    "    with open(json_path, 'w') as json_file:\n",
    "        json.dump(sampling_info, json_file, indent=4)\n",
    "    with open(secondary_path, 'w') as json_file:\n",
    "        json.dump(sampling_info, json_file, indent=4)\n",
    "    \n",
    "    print(f\"Sampling information saved to {json_path} and {secondary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7a39f-4439-43c0-b2da-3d4a676d7fe2",
   "metadata": {},
   "source": [
    "# Main Execution Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd73d423-4b8f-47cf-b3ab-3aa76513eeee",
   "metadata": {},
   "source": [
    "## --- Data Loading ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d3c1806-2afd-4cdb-88c4-5582cb5c0726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /Users/elicox/Desktop/Mac/Work/Yr4 Work/Project/CNN-auto/spectra_data_2.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing spectra in batches: 100%|██████████| 28/28 [08:56<00:00, 19.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving updated DataFrame with flux and wavelength data to /Users/elicox/Desktop/Mac/Work/Yr4 Work/Project/CNN-auto/spectra_data_2.csv...\n"
     ]
    }
   ],
   "source": [
    "spec_data = load_or_query_data(CSV_PATH)\n",
    "all_fluxes, all_wavelengths, all_errors = process_spectra_data(spec_data)\n",
    "max_length = max(len(f) for f in all_fluxes if f is not None)\n",
    "\n",
    "padded_fluxes = pad_or_truncate(all_fluxes, max_length)\n",
    "padded_wavelengths = pad_or_truncate(all_wavelengths, max_length)\n",
    "padded_errors = pad_or_truncate(all_errors, max_length)\n",
    "mask = np.array(create_padding_mask(padded_fluxes, max_length), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9056d12-26f8-4ae9-94d2-b52671b515dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.19003381, Time: 3.70s\n",
      "Epoch [2/50], Loss: 0.08363794, Time: 3.70s\n",
      "Epoch [3/50], Loss: 0.06887204, Time: 3.26s\n",
      "Epoch [4/50], Loss: 0.06665526, Time: 3.17s\n",
      "Epoch [5/50], Loss: 0.06624749, Time: 3.12s\n",
      "Epoch [6/50], Loss: 0.06588427, Time: 3.26s\n",
      "Epoch [7/50], Loss: 0.06579071, Time: 3.23s\n",
      "Epoch [8/50], Loss: 0.06545078, Time: 3.11s\n",
      "Epoch [9/50], Loss: 0.06530042, Time: 3.10s\n",
      "Epoch [10/50], Loss: 0.06537464, Time: 3.11s\n",
      "Epoch [11/50], Loss: 0.06508280, Time: 3.25s\n",
      "Epoch [12/50], Loss: 0.06514013, Time: 3.30s\n",
      "Epoch [13/50], Loss: 0.06509494, Time: 3.24s\n",
      "Epoch [14/50], Loss: 0.06499017, Time: 3.09s\n",
      "Epoch [15/50], Loss: 0.06495630, Time: 3.37s\n",
      "Epoch [16/50], Loss: 0.06493631, Time: 3.15s\n",
      "Epoch [17/50], Loss: 0.06495822, Time: 3.11s\n",
      "Epoch [18/50], Loss: 0.06484919, Time: 3.13s\n",
      "Epoch [19/50], Loss: 0.06473063, Time: 3.08s\n",
      "Epoch [20/50], Loss: 0.06483282, Time: 3.21s\n",
      "Epoch [21/50], Loss: 0.06480446, Time: 3.27s\n",
      "Epoch [22/50], Loss: 0.06480990, Time: 3.20s\n",
      "Epoch [23/50], Loss: 0.06478366, Time: 3.23s\n",
      "Epoch [24/50], Loss: 0.06478400, Time: 3.29s\n",
      "Epoch [25/50], Loss: 0.06505696, Time: 3.21s\n",
      "Epoch [26/50], Loss: 0.06473672, Time: 3.30s\n",
      "Epoch [27/50], Loss: 0.06461589, Time: 3.17s\n",
      "Epoch [28/50], Loss: 0.06455287, Time: 3.18s\n",
      "Epoch [29/50], Loss: 0.06452926, Time: 3.18s\n",
      "Epoch [30/50], Loss: 0.06458616, Time: 3.19s\n",
      "Epoch [31/50], Loss: 0.06468491, Time: 4.19s\n",
      "Epoch [32/50], Loss: 0.06450318, Time: 4.28s\n",
      "Epoch [33/50], Loss: 0.06470195, Time: 3.60s\n",
      "Epoch [34/50], Loss: 0.06450652, Time: 4.72s\n",
      "Epoch [35/50], Loss: 0.06445188, Time: 2.96s\n",
      "Epoch [36/50], Loss: 0.06449981, Time: 5.58s\n",
      "Epoch [37/50], Loss: 0.06474395, Time: 4.11s\n",
      "Epoch [38/50], Loss: 0.06464180, Time: 3.51s\n",
      "Epoch [39/50], Loss: 0.06449966, Time: 3.39s\n",
      "Epoch [40/50], Loss: 0.06442393, Time: 3.41s\n",
      "Epoch [41/50], Loss: 0.06440867, Time: 3.97s\n",
      "Epoch [42/50], Loss: 0.06457289, Time: 3.63s\n",
      "Epoch [43/50], Loss: 0.06454946, Time: 3.98s\n",
      "Epoch [44/50], Loss: 0.06460829, Time: 3.80s\n",
      "Epoch [45/50], Loss: 0.06442624, Time: 3.44s\n",
      "Epoch [46/50], Loss: 0.06437584, Time: 3.47s\n",
      "Epoch [47/50], Loss: 0.06439505, Time: 3.46s\n",
      "Epoch [48/50], Loss: 0.06441453, Time: 3.31s\n",
      "Epoch [49/50], Loss: 0.06443966, Time: 3.27s\n",
      "Epoch [50/50], Loss: 0.06449973, Time: 3.46s\n",
      "Final Loss after 50 epochs: 0.06449973\n",
      "Saved plot to SpectralCNNAutoencoder_output/spectra_with_uncertainty_2.png, seed: 3953\n",
      "Linear plot saved to SpectralCNNAutoencoder_output/mean_reconstruction_error_linear_2.png\n",
      "Log plot saved to SpectralCNNAutoencoder_output/mean_reconstruction_error_log_2.png\n",
      "Plot saved to SpectralCNNAutoencoder_output/bo_results_3d_grid_4.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Dataset/Dataloader ---\n",
    "dataset = SpectralDataset(padded_fluxes, padded_wavelengths, padded_errors, mask)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "\n",
    "# --- Extract for Filtering ---\n",
    "full_fluxes, full_wavelengths, full_errors = [], [], []\n",
    "for batch in dataloader:\n",
    "    f, w, e, _ = batch\n",
    "    full_fluxes.extend(f.squeeze(1).numpy())\n",
    "    full_wavelengths.extend(w.squeeze(1).numpy())\n",
    "    full_errors.extend(e.squeeze(1).numpy())\n",
    "\n",
    "filtered_wavelengths, filtered_fluxes, filtered_errors = filter_spectral_data(\n",
    "    np.array(full_wavelengths), np.array(full_fluxes), np.array(full_errors), min_wavelength=4000\n",
    ")\n",
    "\n",
    "# --- Pad Again Post-Filtering ---\n",
    "max_length = max(len(f) for f in filtered_fluxes)\n",
    "padded_fluxes = pad_or_truncate(filtered_fluxes, max_length)\n",
    "padded_wavelengths = pad_or_truncate(filtered_wavelengths, max_length)\n",
    "padded_errors = pad_or_truncate(filtered_errors, max_length)\n",
    "filtered_mask_tensor = (np.array(padded_fluxes) > 0).astype(np.float32)\n",
    "\n",
    "# --- Tensors ---\n",
    "flux_tensor = torch.tensor(padded_fluxes, dtype=torch.float32).unsqueeze(1)\n",
    "error_tensor = torch.tensor(padded_errors, dtype=torch.float32).unsqueeze(1)\n",
    "mask_tensor = torch.tensor(filtered_mask_tensor, dtype=torch.float32).unsqueeze(1)\n",
    "wavelength_tensor = torch.tensor(padded_wavelengths, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# --- Train Model ---\n",
    "autoencoder = CNNAutoencoderWithSkip()\n",
    "train_autoencoder(autoencoder, flux_tensor, mask_tensor, error_tensor)\n",
    "\n",
    "# --- Reconstruct ---\n",
    "reconstructed_fluxes = []\n",
    "for batch in DataLoader(SpectralDataset(flux_tensor, wavelength_tensor, error_tensor, mask_tensor), batch_size=16):\n",
    "    inputs, _, _, _ = batch\n",
    "    with torch.no_grad():\n",
    "        outputs = autoencoder(inputs).squeeze(1).cpu().numpy()\n",
    "        reconstructed_fluxes.extend(outputs)\n",
    "\n",
    "# --- Anomaly Detection with Optimised Parameters ---\n",
    "filtered_flux_np = flux_tensor.squeeze(1).cpu().numpy()\n",
    "reconstructed_flux_np = np.array(reconstructed_fluxes)\n",
    "\n",
    "data_for_bo = {\"fluxes\": filtered_flux_np, \"reconstructed_fluxes\": reconstructed_flux_np}\n",
    "\n",
    "# Run BO\n",
    "bo_results = gp_minimize(\n",
    "    func=objective_function,\n",
    "    dimensions=param_space,\n",
    "    acq_func=\"EI\",\n",
    "    n_calls=300,\n",
    "    n_random_starts=50,\n",
    "    n_jobs=15,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Extract best parameters\n",
    "best_params = {dim.name: val for dim, val in zip(param_space, bo_results.x)}\n",
    "best_fitness = -bo_results.fun\n",
    "\n",
    "# Detect anomalies\n",
    "(anomalous_regions, spectrum_anomalies, anomaly_metadata, \n",
    " abs_thresh, rel_thresh, range_factor) = detect_anomalous_regions(\n",
    "    original_fluxes=filtered_flux_np,\n",
    "    reconstructed_fluxes=reconstructed_flux_np,\n",
    "    window_size=50,\n",
    "    abs_residual_threshold=best_params[\"abs_residual_threshold\"],\n",
    "    rel_residual_threshold=best_params[\"rel_residual_threshold\"],\n",
    "    range_mismatch_factor=best_params[\"range_mismatch_factor\"],\n",
    "    overall_anomaly_threshold=best_params[\"overall_anomaly_threshold\"]\n",
    ")\n",
    "\n",
    "# --- Visualisation ---\n",
    "wavelengths = np.mean(padded_wavelengths, axis=0)\n",
    "\n",
    "plot_spectra_combined(\n",
    "    original_fluxes=filtered_flux_np,\n",
    "    reconstructed_fluxes=reconstructed_flux_np,\n",
    "    anomalous_regions=anomalous_regions,\n",
    "    spectrum_anomalies=spectrum_anomalies,\n",
    "    spec_data=spec_data,\n",
    "    wavelengths=wavelengths,\n",
    "    only_anomalous=True,\n",
    "    max_samples=10\n",
    ")\n",
    "\n",
    "plot_reconstruction_error_distribution(\n",
    "    reconstruction_errors=compute_reconstruction_errors(filtered_flux_np, reconstructed_flux_np),\n",
    "    wavelengths=wavelengths,\n",
    "    save_directory=OUT_DIR\n",
    ")\n",
    "plot_bo_3d_grid(bo_results, save_directory=OUT_DIR)\n",
    "# plot_bo_results(bo_results, [dim.name for dim in param_space]) -- commented out for storage reasons\n",
    "\n",
    "# search_for_anomalies(spectrum_anomalies, spec_data) -- commented out as this can take a lot of storage when training is not perfect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
