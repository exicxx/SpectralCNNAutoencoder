{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bedd8aa-b0a1-4e21-a986-d8f7adbc2821",
   "metadata": {},
   "source": [
    "This file has code to try to save the data to a csv file but note, it does not work in this file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbbf189-9fd7-4792-b714-a551ea8bf93f",
   "metadata": {},
   "source": [
    "# Goals\n",
    "- Access and filter DESI EDR galaxy spectra data from a database using SPARCL.\n",
    "- Process and normalize the spectra data to prepare it for model training.\n",
    "- Develop a CNN autoencoder with skip connections to perform dimensionality reduction and reconstruction of the spectra.\n",
    "- Train the autoencoder model using a weighted mean squared error (MSE) loss function to emphasize critical spectral features.\n",
    "- Identify and visualize anomalies in the galaxy spectra based on high reconstruction errors.\n",
    "- Provide visual representations of detected anomalies and evaluate the model's performance through training loss metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7961d6d0-48c6-48bf-99f2-f5c303f5e9ad",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This project leverages the Dark Energy Spectroscopic Instrument (DESI) Early Data Release (EDR) dataset to train a Convolutional Neural Network (CNN) autoencoder for anomaly detection in galaxy spectra. The code retrieves galaxy spectra data from a database, processes and normalizes it, and then applies an autoencoder with skip connections to reconstruct the spectra. The reconstruction errors are used to identify anomalous spectra, which may indicate unusual features or observational issues in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b23f113-050b-4ca1-92a5-3ee98f4f33eb",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a1de084-baa5-4745-9969-19f963924355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "from sparcl.client import SparclClient\n",
    "from dl import queryClient as qc, authClient as ac\n",
    "from getpass import getpass\n",
    "import os\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.ticker import AutoMinorLocator  # Use AutoMinorLocator from ticker\n",
    "from torchviz import make_dot\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5db018df-184f-48a3-9d4c-96618fa95321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure file directories\n",
    "DATA_DIR = '/Users/elicox/Desktop/Mac/Work/Yr4 Work/Project/CNN-auto/'\n",
    "IMG_DIR = os.path.join(DATA_DIR, 'output_images')\n",
    "CSV_PATH = os.path.join(DATA_DIR, 'spectra_data.csv')\n",
    "os.makedirs(IMG_DIR, exist_ok=True)\n",
    "#os.environ[\"PATH\"] += os.pathsep + \"/opt/homebrew/bin\" # uncomment when CNN visualisation is needed\n",
    "\n",
    "# Initialize SPARCL client\n",
    "client = SparclClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c523b7-3358-483a-914d-5a19436c1d47",
   "metadata": {},
   "source": [
    "# Data Loading and Saving Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19115282-83b0-40f3-8aa5-c32ca3d1488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query the database\n",
    "def query_spectra_data():\n",
    "    \"\"\"\n",
    "    Queries the DESI EDR database to retrieve primary galaxy spectra.\n",
    "    Returns a DataFrame with the spectra data.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT zp.targetid, zp.survey, zp.program, zp.healpix,  \n",
    "           zp.z, zp.zwarn, zp.coadd_fiberstatus, zp.spectype, \n",
    "           zp.mean_fiber_ra, zp.mean_fiber_dec, zp.zcat_nspec, \n",
    "           zp.desi_target, zp.sv1_desi_target, zp.sv2_desi_target, zp.sv3_desi_target\n",
    "    FROM desi_edr.zpix AS zp\n",
    "    WHERE zp.zcat_primary = 't'\n",
    "      AND zp.zcat_nspec > 3\n",
    "      AND zp.spectype = 'GALAXY'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        zpix_cat = qc.query(sql=query, fmt='table')\n",
    "        df = zpix_cat.to_pandas()\n",
    "        print(f\"Retrieved {len(df)} records from the database.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying data: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_to_csv(df, path=CSV_PATH):\n",
    "    \"\"\"\n",
    "    Saves the spectra DataFrame including flux and wavelength data to a CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert lists of flux and wavelength to CSV-compatible string format\n",
    "        df['flux'] = df['flux'].apply(lambda x: ','.join(map(str, x)))\n",
    "        df['wavelength'] = df['wavelength'].apply(lambda x: ','.join(map(str, x)))\n",
    "        df.to_csv(path, index=False)\n",
    "        print(f\"Data saved to {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to file: {e}\")\n",
    "\n",
    "# Load spectra data including flux, wavelength, and all necessary metadata from CSV\n",
    "def load_spectra_data():\n",
    "    \"\"\"\n",
    "    Loads spectra data including flux, wavelength, and other metadata from a CSV file.\n",
    "    Converts CSV string data back into lists of floats for flux and wavelength.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(CSV_PATH)\n",
    "        \n",
    "        # Convert comma-separated strings back to arrays of floats for flux and wavelength columns\n",
    "        data['flux'] = data['flux'].apply(lambda x: np.array(list(map(float, x.split(','))) if isinstance(x, str) else []))\n",
    "        data['wavelength'] = data['wavelength'].apply(lambda x: np.array(list(map(float, x.split(','))) if isinstance(x, str) else []))\n",
    "        \n",
    "        print(f\"Data loaded from {CSV_PATH}\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {CSV_PATH}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading data: {e}\")\n",
    "        return None\n",
    "# Load or fetch spectra data, with the prompt to save after processing\n",
    "def load_or_fetch_spectra_data():\n",
    "    \"\"\"\n",
    "    Prompts the user to query data from the database or load from a local CSV file.\n",
    "    Optionally saves the data to file after processing.\n",
    "    \"\"\"\n",
    "    user_choice = input(\"Do you want to try querying the database first? (y/n): \").strip().lower()\n",
    "\n",
    "    if user_choice == \"y\":\n",
    "        zpix_cat = query_spectra_data()\n",
    "        if zpix_cat is not None:\n",
    "            save_file_query = input(\"Do you want to save the data to a file? (y/n): \").strip().lower()\n",
    "            if save_file_query == \"y\":\n",
    "                zpix_cat = process_spectra_data(zpix_cat)\n",
    "                save_to_csv(zpix_cat)\n",
    "            return zpix_cat\n",
    "        else:\n",
    "            print(\"Query failed. Attempting to load data from file instead...\")\n",
    "\n",
    "    # Fallback to loading from file if query fails or is declined\n",
    "    data = load_spectra_data()\n",
    "    if data is not None:\n",
    "        # Ensure each 'flux' entry is valid\n",
    "        data['flux'] = data['flux'].apply(lambda f: np.array(f) if isinstance(f, np.ndarray) and len(f) > 0 else None)\n",
    "        data = data.dropna(subset=['flux'])  # Drop any rows where flux is empty or invalid\n",
    "        return data\n",
    "\n",
    "    print(\"No data available for processing.\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a900283b-b35e-4558-9806-a7603966df14",
   "metadata": {},
   "source": [
    "# Data Preparation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75b06a4d-0da6-4bc9-bfcb-3aa1d9642bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_spectra_data(zpix_cat):\n",
    "    \"\"\"\n",
    "    Retrieves and normalizes flux values for each target from the SPARCL client.\n",
    "    Returns the DataFrame with additional flux and wavelength columns for saving.\n",
    "    \"\"\"\n",
    "    flux_data, wavelength_data = [], []\n",
    "    \n",
    "    for i in tqdm(range(len(zpix_cat)), desc=\"Processing spectra\"):\n",
    "        targetid = int(zpix_cat['targetid'].iloc[i])\n",
    "        \n",
    "        # Retrieve spectra details\n",
    "        inc = ['specid', 'redshift', 'flux', 'wavelength', 'spectype', 'specprimary', 'survey', 'program', 'targetid', 'coadd_fiberstatus']\n",
    "        res = client.retrieve_by_specid(specid_list=[targetid], include=inc, dataset_list=['DESI-EDR'])\n",
    "        \n",
    "        for record in res.records:\n",
    "            if record['specprimary']:\n",
    "                flux = record['flux']\n",
    "                wavelength = record['wavelength']\n",
    "                # Normalize the flux\n",
    "                flux_min, flux_max = np.min(flux), np.max(flux)\n",
    "                flux_norm = (flux - flux_min) / (flux_max - flux_min)\n",
    "                \n",
    "                # Store normalized flux and wavelength data\n",
    "                flux_data.append(flux_norm)\n",
    "                wavelength_data.append(wavelength)\n",
    "                break  # Only take the primary spectrum\n",
    "    \n",
    "    # Add the processed flux and wavelength data to the DataFrame\n",
    "    zpix_cat['flux'] = flux_data\n",
    "    zpix_cat['wavelength'] = wavelength_data\n",
    "    return zpix_cat\n",
    "\n",
    "def pad_spectra(fluxes, target_length):\n",
    "    \"\"\"\n",
    "    Pads or truncates each flux array to match the target length.\n",
    "    Returns a numpy array of padded flux data.\n",
    "    \"\"\"\n",
    "    padded_fluxes = []\n",
    "    num_padded = 0  # Track only the padded spectra\n",
    "\n",
    "    for flux in fluxes:\n",
    "        # Skip empty flux arrays to avoid padding errors\n",
    "        if len(flux) == 0:\n",
    "            print(\"Warning: Encountered an empty flux array. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Apply padding or truncation\n",
    "        if len(flux) < target_length:\n",
    "            padding = np.zeros(target_length - len(flux))\n",
    "            padded_flux = np.concatenate([flux, padding])\n",
    "            num_padded += 1\n",
    "        else:\n",
    "            padded_flux = flux[:target_length]\n",
    "        padded_fluxes.append(padded_flux)\n",
    "\n",
    "    # Inform user if any padding was applied\n",
    "    if num_padded > 0:\n",
    "        print(f\"Padding applied to {num_padded} spectra to match the target length.\")\n",
    "    else:\n",
    "        print(\"No padding was necessary; all spectra are of equal length.\")\n",
    "\n",
    "    return np.array(padded_fluxes)\n",
    "# Create padding mask\n",
    "def create_padding_mask(fluxes, target_length):\n",
    "    \"\"\"\n",
    "    Generates a binary mask for padded values in the flux arrays.\n",
    "    Used in the loss calculation to ignore padded regions.\n",
    "    \"\"\"\n",
    "    masks = []\n",
    "    for flux in fluxes:\n",
    "        mask = np.ones_like(flux)\n",
    "        if len(flux) < target_length:\n",
    "            mask = np.concatenate([mask, np.zeros(target_length - len(flux))])\n",
    "        masks.append(mask[:target_length])\n",
    "    return np.array(masks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3170b744-e2c8-4b91-b002-987caaa6bd72",
   "metadata": {},
   "source": [
    "# Autoencoder Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86c6013e-b078-4616-8219-6b96646b2a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Autoencoder with skip connections\n",
    "class CNNAutoencoderWithSkip(nn.Module):\n",
    "    \"\"\"\n",
    "    A CNN-based autoencoder with skip connections to reconstruct spectra data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CNNAutoencoderWithSkip, self).__init__()\n",
    "        # Encoder layers\n",
    "        self.encoder1 = nn.Conv1d(1,128, kernel_size=3, stride=2, padding=1)\n",
    "        self.encoder2 = nn.Conv1d(128, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.encoder3 = nn.Conv1d(64, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.encoder4 = nn.Conv1d(32, 16, kernel_size=3, stride=2, padding=1) \n",
    "\n",
    "        # Decoder layers with skip connections\n",
    "        self.decoder4 = nn.ConvTranspose1d(16, 32, kernel_size=3, stride=2, padding=1, output_padding=1) \n",
    "        self.decoder3 = nn.ConvTranspose1d(32, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.decoder2 = nn.ConvTranspose1d(64, 128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.decoder1 = nn.ConvTranspose1d(128, 1, kernel_size=3, stride=2, padding=1, output_padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding with skip connections\n",
    "        x1 = F.relu(self.encoder1(x))  # Save this for skip connection\n",
    "        x2 = F.relu(self.encoder2(x1))  # Save this for skip connection\n",
    "        x3 = F.relu(self.encoder3(x2))  # Save this for skip connection\n",
    "        x4 = F.relu(self.encoder4(x3))  # Last encoding layer\n",
    "\n",
    "        # Decoding with skip connections, ensuring matching tensor sizes\n",
    "        x = F.relu(self.decoder4(x4))\n",
    "\n",
    "        # Adjust sizes for skip connection with x3\n",
    "        if x3.size(2) > x.size(2):\n",
    "            x3 = x3[:, :, :x.size(2)]\n",
    "        elif x3.size(2) < x.size(2):\n",
    "            x = x[:, :, :x3.size(2)]\n",
    "        x = F.relu(self.decoder3(x + x3))\n",
    "\n",
    "        # Adjust sizes for skip connection with x2\n",
    "        if x2.size(2) > x.size(2):\n",
    "            x2 = x2[:, :, :x.size(2)]\n",
    "        elif x2.size(2) < x.size(2):\n",
    "            x = x[:, :, :x2.size(2)]\n",
    "        x = F.relu(self.decoder2(x + x2))\n",
    "\n",
    "        # Adjust sizes for skip connection with x1\n",
    "        if x1.size(2) > x.size(2):\n",
    "            x1 = x1[:, :, :x.size(2)]\n",
    "        elif x1.size(2) < x.size(2):\n",
    "            x = x[:, :, :x1.size(2)]\n",
    "        x = self.decoder1(x + x1)\n",
    "\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cccd34d-19bb-44b9-84ec-96679dfcb785",
   "metadata": {},
   "source": [
    "# Custom Loss Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d062b54-70fa-4b8a-b69c-4e68aaf177ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom weighted MSE loss function\n",
    "def weighted_mse_loss(output, target, mask, weight_factor=10):\n",
    "    \"\"\"\n",
    "    Calculates MSE loss, amplifying high-residual areas with weight based on spectral gradient.\n",
    "    \"\"\"\n",
    "    mse_loss = (output - target) ** 2\n",
    "    gradient = torch.abs(target[:, :, 1:] - target[:, :, :-1])\n",
    "    weighted_loss = mse_loss[:, :, 1:] * (1 + weight_factor * gradient)\n",
    "    return (weighted_loss * mask[:, :, 1:]).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284f9dc0-7f9c-4766-8c71-acffcca5c7eb",
   "metadata": {},
   "source": [
    "# Training the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92113172-b222-4970-8721-25dd057499d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(model, data, mask, epochs=50, batch_size=64, lr=0.001):\n",
    "    \"\"\"\n",
    "    Trains the model and prints the duration of each epoch.\n",
    "    \n",
    "    Parameters:\n",
    "        model (nn.Module): The autoencoder model.\n",
    "        data (Tensor): Training data tensor.\n",
    "        mask (Tensor): Padding mask tensor for the data.\n",
    "        epochs (int): Number of training epochs.\n",
    "        batch_size (int): Size of each training batch.\n",
    "        lr (float): Learning rate for the optimizer.\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()  # Start timing for the epoch\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            batch_data, mask_batch = data[i:i+batch_size], mask[i:i+batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            loss = weighted_mse_loss(model(batch_data), batch_data, mask_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Calculate and print the epoch duration\n",
    "        epoch_duration = time.time() - start_time\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(data):.4f}, Time: {epoch_duration:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c570b2-cefd-4bb2-afc7-0437fe55a118",
   "metadata": {},
   "source": [
    "# Anomaly Detection and Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b452bcb-c624-4ef6-96ce-1fac8069a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dynamic save path with numbering\n",
    "def create_save_path(save_directory, base_filename):\n",
    "    \"\"\"\n",
    "    Creates a new file path in the specified directory with a unique numeric suffix.\n",
    "    Ensures saved file names are sequentially numbered.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    existing_files = os.listdir(save_directory)\n",
    "    numbers = [\n",
    "        int(re.search(r'\\d+', f).group())\n",
    "        for f in existing_files if re.search(fr'{base_filename}_(\\d+)\\.png', f)\n",
    "    ]\n",
    "    next_number = max(numbers) + 1 if numbers else 1\n",
    "    return os.path.join(save_directory, f'{base_filename}_{next_number}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "794883d6-ae86-41a5-b8f6-43b26973b34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies in  --- feels like this could be optimised \n",
    "def detect_anomalous_regions(original_fluxes, reconstructed_fluxes, window_size=50, percentile_threshold=95, range_mismatch_factor=1.5, overall_anomaly_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Identifies anomalous regions within spectra using residuals and a mismatch factor.\n",
    "    \"\"\"\n",
    "    num_spectra, spectrum_length = original_fluxes.shape\n",
    "    absolute_residuals = np.abs(original_fluxes - reconstructed_fluxes)\n",
    "    relative_residuals = np.abs((original_fluxes - reconstructed_fluxes) / (original_fluxes + 1e-5))\n",
    "\n",
    "    abs_residual_threshold = np.percentile(\n",
    "        [np.mean(absolute_residuals[:, i:i+window_size], axis=1)\n",
    "        for i in range(0, spectrum_length - window_size + 1, window_size // 2)], percentile_threshold)\n",
    "    rel_residual_threshold = np.percentile(\n",
    "        [np.mean(relative_residuals[:, i:i+window_size], axis=1)\n",
    "        for i in range(0, spectrum_length - window_size + 1, window_size // 2)], percentile_threshold)\n",
    "    \n",
    "    anomalies, spectrum_anomalies = [], np.zeros(num_spectra, dtype=bool)\n",
    "    for i in range(num_spectra):\n",
    "        spectrum_anomalies_count = 0\n",
    "        spectrum_anomaly_flags = np.zeros(spectrum_length, dtype=bool)\n",
    "        \n",
    "        for start in range(0, spectrum_length - window_size + 1, window_size // 2):\n",
    "            end = start + window_size\n",
    "            window_abs_residual, window_rel_residual = np.mean(absolute_residuals[i, start:end]), np.mean(relative_residuals[i, start:end])\n",
    "            original_range, reconstructed_range = np.ptp(original_fluxes[i, start:end]), np.ptp(reconstructed_fluxes[i, start:end])\n",
    "            \n",
    "            if (window_abs_residual > abs_residual_threshold or\n",
    "                window_rel_residual > rel_residual_threshold or\n",
    "                reconstructed_range > original_range * range_mismatch_factor):\n",
    "                spectrum_anomaly_flags[start:end] = True\n",
    "                spectrum_anomalies_count += end - start\n",
    "\n",
    "        spectrum_anomalies[i] = spectrum_anomalies_count / spectrum_length > overall_anomaly_threshold\n",
    "        anomalies.append(spectrum_anomaly_flags)\n",
    "    return anomalies, spectrum_anomalies\n",
    "\n",
    "def plot_spectra(original_fluxes, reconstructed_fluxes, anomalous_regions, spectrum_anomalies, zpix_cat, wavelengths=None, save_directory='output_images', max_samples=20):\n",
    "    \"\"\"\n",
    "    Plots a subset of the original and reconstructed spectra, highlighting anomalies and saving to a dynamic path.\n",
    "    Limits the number of spectra plotted to `max_samples`.\n",
    "    \"\"\"\n",
    "    num_spectra = len(original_fluxes)\n",
    "    indices = list(range(num_spectra))\n",
    "    \n",
    "    # Sample `max_samples` if the dataset is large\n",
    "    if num_spectra > max_samples:\n",
    "        print(f\"Too many spectra to plot ({num_spectra}). Sampling {max_samples} spectra.\")\n",
    "        indices = random.sample(indices, max_samples)\n",
    "    else:\n",
    "        print(f\"Plotting all {num_spectra} spectra.\")\n",
    "\n",
    "    fig, axes = plt.subplots(len(indices), 1, figsize=(10, 6 * len(indices)), sharex=True)\n",
    "    if len(indices) == 1:\n",
    "        axes = [axes]  # Ensure axes is iterable for a single plot\n",
    "\n",
    "    for idx, i in enumerate(indices):\n",
    "        ax = axes[idx]\n",
    "        x_axis = wavelengths if wavelengths is not None else range(len(original_fluxes[i]))\n",
    "        \n",
    "        # Plot original and reconstructed spectra\n",
    "        ax.plot(x_axis, original_fluxes[i], label=\"Original\", color='#2c7bb6', linewidth=0.5)\n",
    "        ax.plot(x_axis, reconstructed_fluxes[i], label=\"Reconstructed\", color='#d7191c', alpha=0.7, linewidth=0.5)\n",
    "\n",
    "        # Highlight anomalous regions\n",
    "        in_anomaly, anomaly_start = False, 0\n",
    "        for j in range(len(x_axis)):\n",
    "            if anomalous_regions[i][j] and not in_anomaly:\n",
    "                anomaly_start = j\n",
    "                in_anomaly = True\n",
    "            elif not anomalous_regions[i][j] and in_anomaly:\n",
    "                ax.axvspan(x_axis[anomaly_start], x_axis[j], color='#fdae61', alpha=0.7)\n",
    "                in_anomaly = False\n",
    "        if in_anomaly:\n",
    "            ax.axvspan(x_axis[anomaly_start], x_axis[-1], color='#fdae61', alpha=0.7)\n",
    "\n",
    "        # Set background color for anomalous spectra\n",
    "        if spectrum_anomalies[i]:\n",
    "            ax.set_facecolor('#fee090')  # Yellow background\n",
    "            ax.set_title(f\"Spectrum {i+1} (ID: {zpix_cat['targetid'].iloc[i]}) - Anomalous\", color='red')\n",
    "        else:\n",
    "            ax.set_title(f\"Spectrum {i+1} (ID: {zpix_cat['targetid'].iloc[i]})\", color='black')\n",
    "\n",
    "        # Add residuals plot\n",
    "        divider = make_axes_locatable(ax)\n",
    "        ax_residual = divider.append_axes(\"bottom\", size=\"25%\", pad=0, sharex=ax)\n",
    "        ax_residual.plot(x_axis, original_fluxes[i] - reconstructed_fluxes[i], color='#4dac26', linewidth=0.5)\n",
    "        ax_residual.set_ylabel(\"Residuals\")\n",
    "        ax.set_ylabel(\"Flux (normalized)\")\n",
    "        ax.legend()\n",
    "\n",
    "    plt.xlabel(\"Wavelength (Å)\")\n",
    "    plt.tight_layout()\n",
    "    save_path = create_save_path(save_directory, 'spectra_reconstruction')\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    print(f\"Figure saved to {save_path}\")\n",
    "    plt.show()\n",
    "#Save anomalies to CSV for analysis\n",
    "def save_anomaly_data(zpix_cat, spectrum_anomalies, file_path='anomaly_data.csv'):\n",
    "    \"\"\"\n",
    "    Saves information about detected anomalies to a CSV file for further analysis.\n",
    "    Each row includes the target ID and an anomaly indicator.\n",
    "    \"\"\"\n",
    "    with open(file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['targetid', 'is_anomalous'])  # Header row\n",
    "        for i in range(len(zpix_cat)):\n",
    "            writer.writerow([zpix_cat['targetid'].iloc[i], int(spectrum_anomalies[i])])\n",
    "    print(f\"Anomaly data saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b052ad9-acde-41d6-97a8-71c828627cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the full autoencoder\n",
    "def visualize_autoencoder(autoencoder, input_data):\n",
    "    \"\"\"\n",
    "    Visualizes the entire autoencoder model structure, saving it as an image.\n",
    "    Takes input data, passes it through the model, and generates a network graph.\n",
    "    \"\"\"\n",
    "    outputs = autoencoder(input_data)\n",
    "    model_viz = make_dot(outputs, params=dict(autoencoder.named_parameters()))\n",
    "    model_viz.format = \"png\"\n",
    "    save_path = create_save_path(IMG_DIR, 'autoencoder_visualization')\n",
    "    model_viz.render(save_path.replace(\".png\", \"\"))\n",
    "    print(f\"Full autoencoder visualization saved to {save_path}\")\n",
    "\n",
    "# Visualize only the encoder part\n",
    "def visualize_encoder(autoencoder, input_data):\n",
    "    \"\"\"\n",
    "    Visualizes the encoder portion of the autoencoder model, saving it as an image.\n",
    "    Passes input data through only the encoder layers and generates a graph.\n",
    "    \"\"\"\n",
    "    encoder_output = autoencoder.encoder3(autoencoder.encoder2(autoencoder.encoder1(input_data)))\n",
    "    encoder_viz = make_dot(encoder_output, params=dict(autoencoder.named_parameters()), \n",
    "                           show_attrs=True, show_saved=True)\n",
    "    encoder_viz.format = \"png\"\n",
    "    save_path = create_save_path(IMG_DIR, 'encoder_visualization')\n",
    "    encoder_viz.render(save_path.replace(\".png\", \"\"))\n",
    "    print(f\"Encoder visualization saved to {save_path}\")\n",
    "\n",
    "\n",
    "# Visualize only the decoder part, given an encoded input\n",
    "def visualize_decoder(autoencoder, encoded_input):\n",
    "    \"\"\"\n",
    "    Visualizes the decoder portion of the autoencoder model, saving it as an image.\n",
    "    Takes an encoded input and generates a graph of the decoder layers.\n",
    "    \"\"\"\n",
    "    decoder_output = autoencoder.decoder1(autoencoder.decoder2(autoencoder.decoder3(encoded_input)))\n",
    "    decoder_viz = make_dot(decoder_output, params=dict(autoencoder.named_parameters()), \n",
    "                           show_attrs=True, show_saved=True)\n",
    "    decoder_viz.format = \"png\"\n",
    "    save_path = create_save_path(IMG_DIR, 'decoder_visualization')\n",
    "    decoder_viz.render(save_path.replace(\".png\", \"\"))\n",
    "    print(f\"Decoder visualization saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564197ce-d2a7-4b9b-b1af-866c1b839956",
   "metadata": {},
   "source": [
    "# Autoencoder Memory Wipe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cbbc12c-d384-4dda-a3a4-cfefad311722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_model_weights(model):\n",
    "    \"\"\"\n",
    "    Wipes the memory of the model by reinitialising all weights.\n",
    "    This allows the model to be retrained from scratch.\n",
    "    \"\"\"\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, (nn.Conv1d, nn.ConvTranspose1d, nn.Linear)):\n",
    "            # Reinitialize weights and biases for layers that have them\n",
    "            layer.reset_parameters()\n",
    "    print(\"Model weights and biases have been reset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3ac33f-a5bc-4145-9c93-b08cf08ad946",
   "metadata": {},
   "source": [
    "# Main Execution Flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbad36dd-b45b-4b00-b8e2-a8dee5bb34df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to try querying the database first? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 19 records from the database.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to save the data to a file? (y/n):  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing spectra: 100%|██████████| 19/19 [00:46<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to /Users/elicox/Desktop/Mac/Work/Yr4 Work/Project/CNN-auto/spectra_data.csv\n"
     ]
    }
   ],
   "source": [
    "# # Main execution flow\n",
    "# zpix_cat = load_or_fetch_spectra_data()\n",
    "# if zpix_cat is not None:\n",
    "#     # Filter and validate flux data as arrays with non-zero length\n",
    "#     all_fluxes = [f for f in zpix_cat['flux'] if isinstance(f, np.ndarray) and len(f) > 0]\n",
    "    \n",
    "#     if all_fluxes:\n",
    "#         max_length = max(len(f) for f in all_fluxes)\n",
    "#         all_fluxes_padded = pad_spectra(all_fluxes, max_length)\n",
    "#         mask = create_padding_mask(all_fluxes, max_length)\n",
    "# else:\n",
    "#     print(\"No data available for processing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "597f59f5-f2cb-467f-b78a-e7592c1bbf4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_fluxes_padded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Prepare tensors for model training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m all_fluxes_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mall_fluxes_padded\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m mask_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(mask, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Model setup and initialization\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_fluxes_padded' is not defined"
     ]
    }
   ],
   "source": [
    "# # Prepare tensors for model training\n",
    "# all_fluxes_tensor = torch.tensor(all_fluxes_padded, dtype=torch.float32).unsqueeze(1)\n",
    "# mask_tensor = torch.tensor(mask, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# # Model setup and initialization\n",
    "# autoencoder = CNNAutoencoderWithSkip()\n",
    "# reset_model_weights(autoencoder)\n",
    "\n",
    "# # Train the model\n",
    "# train_autoencoder(autoencoder, all_fluxes_tensor, mask_tensor)\n",
    "\n",
    "# # Evaluate the model and perform anomaly detection\n",
    "# autoencoder.eval()\n",
    "# reconstructed_fluxes = autoencoder(all_fluxes_tensor).detach().numpy().squeeze()\n",
    "\n",
    "# # Optimized anomaly detection with region-specific analysis\n",
    "# anomalous_regions, spectrum_anomalies = detect_anomalous_regions(\n",
    "#     original_fluxes=all_fluxes_tensor.numpy().squeeze(),\n",
    "#     reconstructed_fluxes=reconstructed_fluxes,\n",
    "#     window_size=50,\n",
    "#     percentile_threshold=95,\n",
    "#     range_mismatch_factor=1.5,\n",
    "#     overall_anomaly_threshold=0.3\n",
    "# )\n",
    "# print(f\"Detected anomalous regions in spectra with improved method.\")\n",
    "\n",
    "# # Plot the results, using dynamic naming\n",
    "# plot_spectra(\n",
    "#     original_fluxes=all_fluxes_tensor.numpy().squeeze(),\n",
    "#     reconstructed_fluxes=reconstructed_fluxes,\n",
    "#     anomalous_regions=anomalous_regions,\n",
    "#     spectrum_anomalies=spectrum_anomalies,\n",
    "#     zpix_cat=zpix_cat,\n",
    "#     save_directory=IMG_DIR  # Using dynamic naming path\n",
    "# )\n",
    "\n",
    "# # Optional: Visualize the autoencoder and its components if needed\n",
    "# #real_input = all_fluxes_tensor[0].unsqueeze(0)  # Select a real input sample\n",
    "\n",
    "# ####### Uncomment when visualizations are required #########\n",
    "# # visualize_autoencoder(autoencoder, real_input)\n",
    "# # visualize_encoder(autoencoder, real_input)\n",
    "# # encoded_input = autoencoder.encoder3(autoencoder.encoder2(autoencoder.encoder1(real_input)))\n",
    "# # visualize_decoder(autoencoder, encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30be98ae-57dd-48ac-9ee3-d1e7f4fa0274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to try querying the database first? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 19 records from the database.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to save the data to a file? (y/n):  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing spectra: 100%|██████████| 19/19 [00:42<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to /Users/elicox/Desktop/Mac/Work/Yr4 Work/Project/CNN-auto/spectra_data.csv\n",
      "No valid flux data found. Exiting.\n"
     ]
    }
   ],
   "source": [
    "# Main execution flow\n",
    "zpix_cat = load_or_fetch_spectra_data()\n",
    "\n",
    "if zpix_cat is not None:\n",
    "    # Ensure all elements in 'flux' column are non-empty arrays\n",
    "    all_fluxes = [np.array(f) for f in zpix_cat['flux'] if isinstance(f, (list, np.ndarray)) and len(f) > 0]\n",
    "    \n",
    "    if not all_fluxes:\n",
    "        print(\"No valid flux data found. Exiting.\")\n",
    "    else:\n",
    "        # Define max length for padding\n",
    "        max_length = max(len(f) for f in all_fluxes)\n",
    "        \n",
    "        # Pad flux data and create mask\n",
    "        all_fluxes_padded = pad_spectra(all_fluxes, max_length)\n",
    "        mask = create_padding_mask(all_fluxes, max_length)\n",
    "        \n",
    "        # Prepare tensors for model training\n",
    "        all_fluxes_tensor = torch.tensor(all_fluxes_padded, dtype=torch.float32).unsqueeze(1)\n",
    "        mask_tensor = torch.tensor(mask, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        # Model setup and initialization\n",
    "        autoencoder = CNNAutoencoderWithSkip()\n",
    "        reset_model_weights(autoencoder)\n",
    "\n",
    "        # Train the model\n",
    "        train_autoencoder(autoencoder, all_fluxes_tensor, mask_tensor)\n",
    "\n",
    "        # Evaluate the model and perform anomaly detection\n",
    "        autoencoder.eval()\n",
    "        reconstructed_fluxes = autoencoder(all_fluxes_tensor).detach().numpy().squeeze()\n",
    "\n",
    "        # Optimized anomaly detection with region-specific analysis\n",
    "        anomalous_regions, spectrum_anomalies = detect_anomalous_regions(\n",
    "            original_fluxes=all_fluxes_tensor.numpy().squeeze(),\n",
    "            reconstructed_fluxes=reconstructed_fluxes,\n",
    "            window_size=50,\n",
    "            percentile_threshold=95,\n",
    "            range_mismatch_factor=1.5,\n",
    "            overall_anomaly_threshold=0.3\n",
    "        )\n",
    "        print(f\"Detected anomalous regions in spectra with improved method.\")\n",
    "\n",
    "        # Plot the results, using dynamic naming\n",
    "        plot_spectra(\n",
    "            original_fluxes=all_fluxes_tensor.numpy().squeeze(),\n",
    "            reconstructed_fluxes=reconstructed_fluxes,\n",
    "            anomalous_regions=anomalous_regions,\n",
    "            spectrum_anomalies=spectrum_anomalies,\n",
    "            zpix_cat=zpix_cat,\n",
    "            save_directory=IMG_DIR  # Using dynamic naming path\n",
    "        )\n",
    "else:\n",
    "    print(\"No data available for processing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29baef7-f6a9-4183-9f97-69ca2f038784",
   "metadata": {},
   "source": [
    "# vvvvvvvvvvv DEBUG ZONE vvvvvvvvvvv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aed6dec-8d39-40f7-b3d5-ed1f8292eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Query database and retrieve spectra\n",
    "# def query_spectra_data():\n",
    "#     \"\"\"\n",
    "#     Queries the DESI EDR database to retrieve primary galaxy spectra.\n",
    "#     Returns a DataFrame with the spectra data.\n",
    "#     \"\"\"\n",
    "#     query = \"\"\"\n",
    "#     SELECT zp.targetid, zp.survey, zp.program, zp.healpix,  \n",
    "#            zp.z, zp.zwarn, zp.coadd_fiberstatus, zp.spectype, \n",
    "#            zp.mean_fiber_ra, zp.mean_fiber_dec, zp.zcat_nspec, \n",
    "#            zp.desi_target, zp.sv1_desi_target, zp.sv2_desi_target, zp.sv3_desi_target\n",
    "#     FROM desi_edr.zpix AS zp\n",
    "#     WHERE zp.zcat_primary = 't'\n",
    "#       AND zp.zcat_nspec > 3\n",
    "#       AND zp.spectype = 'GALAXY'\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         zpix_cat = qc.query(sql=query, fmt='table')\n",
    "#         df = zpix_cat.to_pandas()\n",
    "#         print(f\"Retrieved {len(df)} records from the database.\")\n",
    "#         return df\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error querying data: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Process spectra data including flux and wavelength retrieval\n",
    "# def process_spectra_data(zpix_cat):\n",
    "#     \"\"\"\n",
    "#     Retrieves and normalizes flux values for each target from the SPARCL client.\n",
    "#     Adds flux and wavelength data to the DataFrame for saving or further processing.\n",
    "#     \"\"\"\n",
    "#     flux_data, wavelength_data = [], []\n",
    "    \n",
    "#     for i in tqdm(range(len(zpix_cat)), desc=\"Processing spectra\"):\n",
    "#         targetid = int(zpix_cat['targetid'].iloc[i])\n",
    "        \n",
    "#         # Retrieve spectra details\n",
    "#         inc = ['specid', 'redshift', 'flux', 'wavelength', 'spectype', 'specprimary', 'survey', 'program', 'targetid', 'coadd_fiberstatus']\n",
    "#         res = client.retrieve_by_specid(specid_list=[targetid], include=inc, dataset_list=['DESI-EDR'])\n",
    "        \n",
    "#         for record in res.records:\n",
    "#             if record['specprimary']:\n",
    "#                 flux = record['flux']\n",
    "#                 wavelength = record['wavelength']\n",
    "#                 # Normalize the flux\n",
    "#                 if len(flux) > 0:  # Check to avoid issues with empty flux arrays\n",
    "#                     flux_min, flux_max = np.min(flux), np.max(flux)\n",
    "#                     flux_norm = (flux - flux_min) / (flux_max - flux_min)\n",
    "#                     flux_data.append(flux_norm)\n",
    "#                     wavelength_data.append(wavelength)\n",
    "#                 else:\n",
    "#                     print(f\"Warning: Empty flux array for target {targetid}. Skipping this record.\")\n",
    "#                 break  # Only take the primary spectrum\n",
    "    \n",
    "#     zpix_cat['flux'] = flux_data\n",
    "#     zpix_cat['wavelength'] = wavelength_data\n",
    "#     return zpix_cat\n",
    "\n",
    "# # Save DataFrame to CSV with flux and wavelength data\n",
    "# def save_to_csv(df, path=CSV_PATH):\n",
    "#     \"\"\"\n",
    "#     Saves the spectra DataFrame including flux and wavelength data to a CSV file.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Convert lists of flux and wavelength to CSV-compatible string format\n",
    "#         df['flux'] = df['flux'].apply(lambda x: ','.join(map(str, x)))\n",
    "#         df['wavelength'] = df['wavelength'].apply(lambda x: ','.join(map(str, x)))\n",
    "#         df.to_csv(path, index=False)\n",
    "#         print(f\"Data saved to {path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error saving data to file: {e}\")\n",
    "\n",
    "# # Load spectra data including flux and wavelength from CSV\n",
    "# def load_spectra_data():\n",
    "#     \"\"\"\n",
    "#     Loads spectra data including flux and wavelength data from a CSV file.\n",
    "#     Converts CSV string data back into lists of floats.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         data = pd.read_csv(CSV_PATH)\n",
    "#         data['flux'] = data['flux'].apply(lambda x: list(map(float, x.split(','))))\n",
    "#         data['wavelength'] = data['wavelength'].apply(lambda x: list(map(float, x.split(','))))\n",
    "#         print(f\"Data loaded from {CSV_PATH}\")\n",
    "#         return data\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"File not found: {CSV_PATH}\")\n",
    "#         return None\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred while loading data: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Load or fetch spectra data with a prompt to save after processing\n",
    "# def load_or_fetch_spectra_data():\n",
    "#     \"\"\"\n",
    "#     Prompts the user to query data from the database or load from a local CSV file.\n",
    "#     Optionally saves the data to file after processing.\n",
    "#     \"\"\"\n",
    "#     user_choice = input(\"Do you want to try querying the database first? (y/n): \").strip().lower()\n",
    "    \n",
    "#     if user_choice == \"y\":\n",
    "#         zpix_cat = query_spectra_data()\n",
    "#         if zpix_cat is not None:\n",
    "#             save_file_query = input(\"Do you want to save the data to a file? (y/n): \").strip().lower()\n",
    "#             if save_file_query == \"y\":\n",
    "#                 zpix_cat = process_spectra_data(zpix_cat)\n",
    "#                 save_to_csv(zpix_cat)\n",
    "#             return zpix_cat\n",
    "            \n",
    "#         else:\n",
    "#             print(\"Query failed. Attempting to load data from file instead...\")\n",
    "    \n",
    "#     # Fallback to loading from file\n",
    "#     return load_spectra_data()\n",
    "\n",
    "# # Updated pad_spectra function with additional diagnostics\n",
    "# def pad_spectra(fluxes, target_length):\n",
    "#     \"\"\"\n",
    "#     Pads or truncates each flux array to match the target length.\n",
    "#     Returns a numpy array of padded flux data.\n",
    "#     \"\"\"\n",
    "#     padded_fluxes = []\n",
    "#     num_padded = 0  # Track only the padded spectra\n",
    "    \n",
    "#     for idx, flux in enumerate(fluxes):\n",
    "#         # Convert flux to a one-dimensional numpy array, if not already\n",
    "#         flux = np.asarray(flux).flatten()\n",
    "\n",
    "#         if flux.size == 0:\n",
    "#             print(f\"Warning: Skipping empty flux array at index {idx}.\")\n",
    "#             continue\n",
    "        \n",
    "#         # Apply padding or truncation\n",
    "#         if flux.size < target_length:\n",
    "#             padding = np.zeros(target_length - flux.size)\n",
    "#             padded_flux = np.concatenate([flux, padding])\n",
    "#             num_padded += 1\n",
    "#         else:\n",
    "#             padded_flux = flux[:target_length]\n",
    "        \n",
    "#         # Log each step for debugging\n",
    "#         print(f\"Processed flux at index {idx}: original length={len(flux)}, padded length={len(padded_flux)}\")\n",
    "        \n",
    "#         padded_fluxes.append(padded_flux)\n",
    "\n",
    "#     # Final confirmation\n",
    "#     if num_padded > 0:\n",
    "#         print(f\"Padding applied to {num_padded} spectra to match the target length.\")\n",
    "#     else:\n",
    "#         print(\"No padding was necessary; all spectra are of equal length.\")\n",
    "\n",
    "\n",
    "#     return np.array(padded_fluxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3705d5-dea8-4d37-b1f2-33e2b69a7c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure all elements in 'all_fluxes' are arrays with non-zero length\n",
    "# zpix_cat = load_or_fetch_spectra_data()\n",
    "# if zpix_cat is not None:\n",
    "#     # Filter and validate flux data\n",
    "#     all_fluxes = [np.array(f) for f in zpix_cat['flux'].tolist() if isinstance(f, (list, np.ndarray)) and len(f) > 0]\n",
    "    \n",
    "#     # Confirm all_fluxes content\n",
    "#     if not all_fluxes:\n",
    "#         print(\"No valid flux data found. Exiting.\")\n",
    "#     else:\n",
    "#         max_length = max(len(f) for f in all_fluxes)\n",
    "#         all_fluxes_padded = pad_spectra(all_fluxes, max_length)\n",
    "#         mask = create_padding_mask(all_fluxes, max_length)\n",
    "# else:\n",
    "#     print(\"No data available for processing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5789755a-8c86-4486-9db2-fbf87647f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load spectra data including flux, wavelength, and all necessary metadata from CSV\n",
    "# def load_spectra_data():\n",
    "#     \"\"\"\n",
    "#     Loads spectra data including flux, wavelength, and other metadata from a CSV file.\n",
    "#     Converts CSV string data back into lists of floats for flux and wavelength.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         data = pd.read_csv(CSV_PATH)\n",
    "        \n",
    "#         # Convert comma-separated strings back to arrays of floats for flux and wavelength columns\n",
    "#         data['flux'] = data['flux'].apply(lambda x: np.array(list(map(float, x.split(',')))))\n",
    "#         data['wavelength'] = data['wavelength'].apply(lambda x: np.array(list(map(float, x.split(',')))))\n",
    "        \n",
    "#         print(f\"Data loaded from {CSV_PATH}\")\n",
    "#         return data\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"File not found: {CSV_PATH}\")\n",
    "#         return None\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred while loading data: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # # Now load data and process spectra as before\n",
    "# # zpix_cat = load_spectra_data()\n",
    "# # if zpix_cat is not None:\n",
    "# #     # Extract flux data while ensuring no zero-length arrays\n",
    "# #     all_fluxes = [f for f in zpix_cat['flux'] if len(f) > 0]\n",
    "    \n",
    "# #     if all_fluxes:\n",
    "# #         max_length = max(len(f) for f in all_fluxes)\n",
    "# #         all_fluxes_padded = pad_spectra(all_fluxes, max_length)\n",
    "# #         mask = create_padding_mask(all_fluxes, max_length)\n",
    "# # else:\n",
    "# #     print(\"No data available for processing.\")\n",
    "# # Main execution\n",
    "# zpix_cat = load_or_fetch_spectra_data()\n",
    "# if zpix_cat is not None:\n",
    "#     # Convert valid flux entries to a list and filter\n",
    "#     all_fluxes = [np.array(f) for f in zpix_cat['flux'] if f is not None and len(f) > 0]\n",
    "    \n",
    "#     if not all_fluxes:\n",
    "#         print(\"No valid flux data found. Exiting.\")\n",
    "#     else:\n",
    "#         max_length = max(len(f) for f in all_fluxes)\n",
    "#         all_fluxes_padded = pad_spectra(all_fluxes, max_length)\n",
    "#         mask = create_padding_mask(all_fluxes, max_length)\n",
    "# else:\n",
    "#     print(\"No data available for processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926998ff-cb6a-4c1c-b821-b5d0331ca747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load spectra data including flux, wavelength, and all necessary metadata from CSV\n",
    "# def load_spectra_data():\n",
    "#     \"\"\"\n",
    "#     Loads spectra data including flux, wavelength, and other metadata from a CSV file.\n",
    "#     Converts CSV string data back into lists of floats for flux and wavelength.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         data = pd.read_csv(CSV_PATH)\n",
    "        \n",
    "#         # Convert comma-separated strings back to arrays of floats for flux and wavelength columns\n",
    "#         data['flux'] = data['flux'].apply(lambda x: np.array(list(map(float, x.split(','))) if isinstance(x, str) else []))\n",
    "#         data['wavelength'] = data['wavelength'].apply(lambda x: np.array(list(map(float, x.split(','))) if isinstance(x, str) else []))\n",
    "        \n",
    "#         print(f\"Data loaded from {CSV_PATH}\")\n",
    "#         return data\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"File not found: {CSV_PATH}\")\n",
    "#         return None\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred while loading data: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Load or fetch spectra data, with the prompt to save after processing\n",
    "# def load_or_fetch_spectra_data():\n",
    "#     \"\"\"\n",
    "#     Prompts the user to query data from the database or load from a local CSV file.\n",
    "#     Optionally saves the data to file after processing.\n",
    "#     \"\"\"\n",
    "#     user_choice = input(\"Do you want to try querying the database first? (y/n): \").strip().lower()\n",
    "\n",
    "#     if user_choice == \"y\":\n",
    "#         zpix_cat = query_spectra_data()\n",
    "#         if zpix_cat is not None:\n",
    "#             save_file_query = input(\"Do you want to save the data to a file? (y/n): \").strip().lower()\n",
    "#             if save_file_query == \"y\":\n",
    "#                 zpix_cat = process_spectra_data(zpix_cat)\n",
    "#                 save_to_csv(zpix_cat)\n",
    "#             return zpix_cat\n",
    "#         else:\n",
    "#             print(\"Query failed. Attempting to load data from file instead...\")\n",
    "\n",
    "#     # Fallback to loading from file if query fails or is declined\n",
    "#     data = load_spectra_data()\n",
    "#     if data is not None:\n",
    "#         # Ensure each 'flux' entry is valid\n",
    "#         data['flux'] = data['flux'].apply(lambda f: np.array(f) if isinstance(f, np.ndarray) and len(f) > 0 else None)\n",
    "#         data = data.dropna(subset=['flux'])  # Drop any rows where flux is empty or invalid\n",
    "#         return data\n",
    "\n",
    "#     print(\"No data available for processing.\")\n",
    "#     return None\n",
    "\n",
    "# # Main execution flow\n",
    "# zpix_cat = load_or_fetch_spectra_data()\n",
    "# if zpix_cat is not None:\n",
    "#     # Filter and validate flux data as arrays with non-zero length\n",
    "#     all_fluxes = [f for f in zpix_cat['flux'] if isinstance(f, np.ndarray) and len(f) > 0]\n",
    "    \n",
    "#     if all_fluxes:\n",
    "#         max_length = max(len(f) for f in all_fluxes)\n",
    "#         all_fluxes_padded = pad_spectra(all_fluxes, max_length)\n",
    "#         mask = create_padding_mask(all_fluxes, max_length)\n",
    "# else:\n",
    "#     print(\"No data available for processing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78632b84-98e5-4315-b862-3bb0362ba6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare tensors for model training\n",
    "# all_fluxes_tensor = torch.tensor(all_fluxes_padded, dtype=torch.float32).unsqueeze(1)\n",
    "# mask_tensor = torch.tensor(mask, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# # Model setup and initialization\n",
    "# autoencoder = CNNAutoencoderWithSkip()\n",
    "# reset_model_weights(autoencoder)\n",
    "\n",
    "# # Train the model\n",
    "# train_autoencoder(autoencoder, all_fluxes_tensor, mask_tensor)\n",
    "\n",
    "# # Evaluate the model and perform anomaly detection\n",
    "# autoencoder.eval()\n",
    "# reconstructed_fluxes = autoencoder(all_fluxes_tensor).detach().numpy().squeeze()\n",
    "\n",
    "# # Optimized anomaly detection with region-specific analysis\n",
    "# anomalous_regions, spectrum_anomalies = detect_anomalous_regions(\n",
    "#     original_fluxes=all_fluxes_tensor.numpy().squeeze(),\n",
    "#     reconstructed_fluxes=reconstructed_fluxes,\n",
    "#     window_size=50,\n",
    "#     percentile_threshold=95,\n",
    "#     range_mismatch_factor=1.5,\n",
    "#     overall_anomaly_threshold=0.3\n",
    "# )\n",
    "# print(f\"Detected anomalous regions in spectra with improved method.\")\n",
    "\n",
    "# # Plot the results, using dynamic naming\n",
    "# plot_spectra(\n",
    "#     original_fluxes=all_fluxes_tensor.numpy().squeeze(),\n",
    "#     reconstructed_fluxes=reconstructed_fluxes,\n",
    "#     anomalous_regions=anomalous_regions,\n",
    "#     spectrum_anomalies=spectrum_anomalies,\n",
    "#     zpix_cat=zpix_cat,\n",
    "#     save_directory=IMG_DIR  # Using dynamic naming path\n",
    "# )\n",
    "\n",
    "# # Optional: Visualize the autoencoder and its components if needed\n",
    "# #real_input = all_fluxes_tensor[0].unsqueeze(0)  # Select a real input sample\n",
    "\n",
    "# ####### Uncomment when visualizations are required #########\n",
    "# # visualize_autoencoder(autoencoder, real_input)\n",
    "# # visualize_encoder(autoencoder, real_input)\n",
    "# # encoded_input = autoencoder.encoder3(autoencoder.encoder2(autoencoder.encoder1(real_input)))\n",
    "# # visualize_decoder(autoencoder, encoded_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
