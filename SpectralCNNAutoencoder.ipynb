{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5917234b-e981-47c5-a2fb-a2448a8bf23f",
   "metadata": {},
   "source": [
    "# Goals\n",
    "- Access and filter DESI EDR galaxy spectra data from a database using SPARCL.\n",
    "- Process and normalize the spectra data to prepare it for model training.\n",
    "- Develop a CAE with skip connections to perform dimensionality reduction and reconstruction of the spectra.\n",
    "- Train the autoencoder model using a weighted mean squared error (MSE) loss function to emphasize critical spectral features.\n",
    "- Identify and visualize anomalies in the galaxy spectra based on high reconstruction errors.\n",
    "- Provide visual representations of detected anomalies and evaluate the model's performance through training loss metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7961d6d0-48c6-48bf-99f2-f5c303f5e9ad",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This project leverages the Dark Energy Spectroscopic Instrument (DESI) Early Data Release (EDR) dataset to train a convolutional autoencoder (CAE) for anomaly detection in galaxy spectra. The code retrieves galaxy spectra data from a database, processes and normalizes it, and then applies an autoencoder with skip connections to reconstruct the spectra. The reconstruction errors are used to identify anomalous spectra, which may indicate unusual features or observational issues in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2216cae-752c-4f4d-9b73-7799fbbd15a1",
   "metadata": {},
   "source": [
    "## Commands to run in terminal to upload changes to GitHub \n",
    "##### Note, to save and exit the commit comment section: Esc, \":wq\", Enter\n",
    "\n",
    "\n",
    "cd \"/Users/elicox/Desktop/Mac/Work/Yr4 Work/Project/CNN-auto\"\n",
    "\n",
    "git add SpectralCNNAutoencoder.ipynb\n",
    "\n",
    "output_dir=\"SpectralCNNAutoencoder_output\"\n",
    "\n",
    "latest_mean_reconstruction_error=$(ls -t output_images/mean_reconstruction_error_*.png | head -n 1)\n",
    "\n",
    "latest_anomalous_spectra=$(ls -t output_images/anomalous_spectra_*.png | head -n 1)\n",
    "\n",
    "latest_sampling_info=$(ls -t anomalous_regions/sampling_info_*.json | head -n 1)\n",
    "\n",
    "if [ -f \"$latest_mean_reconstruction_error\" ]; then\n",
    "    mv \"$latest_mean_reconstruction_error\" \"$output_dir/\"\n",
    "    git add \"$output_dir/$(basename \"$latest_mean_reconstruction_error\")\"\n",
    "fi\n",
    "\n",
    "if [ -f \"$latest_anomalous_spectra\" ]; then\n",
    "    mv \"$latest_anomalous_spectra\" \"$output_dir/\"\n",
    "    git add \"$output_dir/$(basename \"$latest_anomalous_spectra\")\"\n",
    "fi\n",
    "\n",
    "if [ -f \"$latest_sampling_info\" ]; then\n",
    "    mv \"$latest_sampling_info\" \"$output_dir/\"\n",
    "    git add \"$output_dir/$(basename \"$latest_sampling_info\")\"\n",
    "fi\n",
    "\n",
    "git commit \n",
    "\n",
    "git push origin main\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2c2c70-ed5f-4ce7-addb-78e17e483ad8",
   "metadata": {},
   "source": [
    "# Change log\n",
    "<i> Place to log changes before they are recorded in a github update:\n",
    "\n",
    "A better fitness function has been developed.\n",
    "\n",
    "Integration with the BlueBEAR supercomputer has been improved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b23f113-050b-4ca1-92a5-3ee98f4f33eb",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a1de084-baa5-4745-9969-19f963924355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "from sparcl.client import SparclClient\n",
    "from dl import queryClient as qc, authClient as ac\n",
    "from getpass import getpass\n",
    "import os\n",
    "import re\n",
    "import csv \n",
    "import torch.nn.functional as F\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "from torchviz import make_dot\n",
    "import time\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed \n",
    "import logging\n",
    "import json\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import matplotlib.patches as patches\n",
    "import requests\n",
    "from PIL import Image, ImageDraw\n",
    "from deap import base, creator, tools, algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6dd06e5-7c40-49eb-b115-73266fdfc97e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'emc180'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#token = ac.login(input(\"Enter user name: (+ENTER) \"),getpass(\"Enter password: (+ENTER) \"))\n",
    "ac.whoAmI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5db018df-184f-48a3-9d4c-96618fa95321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure file directories\n",
    "DATA_DIR = '/Users/elicox/Desktop/Mac/Work/Yr4 Work/Project/CNN-auto/'\n",
    "OUT_DIR = os.path.join(DATA_DIR, 'SpectralCNNAutoencoder_output')\n",
    "CSV_PATH = os.path.join(DATA_DIR, 'spectra_data.csv')\n",
    "JSON_DIR = os.path.join(DATA_DIR, 'anomalous_regions')\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "# os.environ[\"PATH\"] += os.pathsep + \"/opt/homebrew/bin\" # uncomment when CNN visualisation is needed\n",
    "\n",
    "# Initialize SPARCL client\n",
    "client = SparclClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c523b7-3358-483a-914d-5a19436c1d47",
   "metadata": {},
   "source": [
    "# Data Loading and Saving Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daf5ad06-b35f-4c28-ac9f-ad4f0e1e2c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_spectra_data():\n",
    "    \"\"\"\n",
    "    Queries the DESI Early Data Release (EDR) database to retrieve galaxy spectra data, \n",
    "    filtering for primary spectra with a minimum number of coadded spectra and good quality flags.\n",
    "    \n",
    "    This function selects observational fields from the DESI EDR database:\n",
    "        - `targetid`: Unique identifier for each galaxy.\n",
    "        - `z`: Spectroscopic redshift.\n",
    "        - `zwarn`: Redshift quality flag, where `zwarn = 0` indicates reliable data.\n",
    "        - `coadd_fiberstatus`: Status of the fiber used for observation, with `0` indicating no issues.\n",
    "        - `spectype`: Spectral type, filtered here to include only galaxies.\n",
    "        - `mean_fiber_ra`, `mean_fiber_dec`: Mean Right Ascension and Declination of the fiber position.\n",
    "        - `zcat_nspec`: Number of coadded spectra for the target.\n",
    "        - `zcat_primary`: Indicates the primary spectrum for the target.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with galaxy spectra data including flux error if available.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT zp.targetid, zp.z, zp.zwarn, zp.coadd_fiberstatus, zp.spectype, \n",
    "    zp.mean_fiber_ra, zp.mean_fiber_dec\n",
    "    FROM desi_edr.zpix AS zp\n",
    "    WHERE zp.zcat_primary = 't'\n",
    "      AND zp.zcat_nspec > 2\n",
    "      AND zp.z <= 0.5\n",
    "      AND zp.spectype = 'GALAXY'\n",
    "      AND zp.zwarn = '0'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Querying SPARCL database...\")\n",
    "        zpix_cat = qc.query(sql=query, fmt='table')\n",
    "        df = zpix_cat.to_pandas()\n",
    "        print(f\"Retrieved {len(df)} records from the database.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying data: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_or_query_data(csv_path):\n",
    "    \"\"\"\n",
    "    Loads data from CSV if available; otherwise, queries SPARCL and saves the results.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_path (str): Path to the CSV file to save or load data.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Loaded or queried data as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    if os.path.exists(csv_path):\n",
    "        print(f\"Loading data from {csv_path}...\")\n",
    "        try:\n",
    "            return pd.read_csv(csv_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading CSV file: {e}\")\n",
    "            print(\"Attempting to query the database instead...\")\n",
    "    data = query_spectra_data()\n",
    "    if data is not None:\n",
    "        try:\n",
    "            print(f\"Saving queried data to {csv_path}...\")\n",
    "            data.to_csv(csv_path, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data to CSV: {e}\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2485d340-21a3-48c8-8f53-6a27454165df",
   "metadata": {},
   "source": [
    "## Information about the query\n",
    "- targetid, survey, program -- unique identifiers for a given spectrum\n",
    "- healpix -- healpix number for the target\n",
    "- z -- spectroscopic redshift of the target\n",
    "- zwarn -- encoded information regarding the redshift (zwarn = 0 is good)\n",
    "- coadd_fiberstatus -- encoded information regarding the fiber that is assigned to the target (coadd_fiberstatus = 0 is good)\n",
    "- spectype -- Spectral type of the target: STAR | GALAXY | QSO\n",
    "- mean_fiber_ra, mean_fiber_dec -- Mean R.A. and Dec. of the fiber position from all the observations of the target\n",
    "- zcat_nspec -- Number of coadded spectra that are available for a given target\n",
    "- zcat_primary -- Whether or not a given coadded spectrum is the primary spectrum. zcat_primary = True for the \"best\" spectrum.\n",
    "- CASTing this column as an INT: zcat_primary = 1 for the \"best\" spectrum.\n",
    "- desi_target -- encodes main survey's DESI targeting information - explained in detail below\n",
    "- sv1_desi_target -- encodes sv1 desi targeting information\n",
    "- sv2_desi_target -- encodes sv2 desi targeting information\n",
    "- sv3_desi_target -- encodes sv3 desi targeting information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a900283b-b35e-4558-9806-a7603966df14",
   "metadata": {},
   "source": [
    "# Data Preparation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75b06a4d-0da6-4bc9-bfcb-3aa1d9642bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_flux(targetid, inc, retries=5, delay=3):\n",
    "    \"\"\"\n",
    "    Retrieves and normalizes flux data, wavelength, and calculates error for a single target ID from the DESI database, with retry logic.\n",
    "    \n",
    "    Parameters:\n",
    "    - targetid (int): The ID of the target for which flux data is requested.\n",
    "    - inc (list of str): List of attributes to include in the data retrieval (e.g., ['specid', 'flux', 'wavelength', 'ivar']).\n",
    "    - retries (int, optional): The number of retry attempts if data retrieval fails (default is 5).\n",
    "    - delay (int, optional): The base delay in seconds between retries, with exponential backoff (default is 3).\n",
    "\n",
    "    Returns:\n",
    "    - tuple (np.array, np.array, np.array) or None: Normalized flux, wavelength, and error arrays if retrieval is successful; otherwise, None if all retries fail.\n",
    "    \"\"\"\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            res = client.retrieve_by_specid(specid_list=[targetid], include=inc, dataset_list=['DESI-EDR'])\n",
    "            for record in res.records:\n",
    "                if record['specprimary']:\n",
    "                    flux = record['flux']\n",
    "                    wavelength = record['wavelength']\n",
    "                    ivar = record['ivar'] \n",
    "                    \n",
    "                    flux_min, flux_max = np.min(flux), np.max(flux)\n",
    "                    normalized_flux = (flux - flux_min) / (flux_max - flux_min)\n",
    "                    \n",
    "                    error = np.sqrt(1 / np.where(ivar == 0, 1e-10, ivar))\n",
    "\n",
    "                    \n",
    "                    return normalized_flux, wavelength, error  \n",
    "        except Exception:\n",
    "            if attempt < retries:\n",
    "                time.sleep(delay * (2 ** (attempt - 1)))  # Exponential backoff\n",
    "    return None  \n",
    "\n",
    "\n",
    "def process_spectra_data(zpix_cat, batch_size=20, max_workers=10):\n",
    "    \"\"\"\n",
    "    Retrieves and processes flux data in parallel for a DataFrame of galaxy spectra from the DESI database,\n",
    "    including flux, wavelength, and error data.\n",
    "\n",
    "    Parameters:\n",
    "    - zpix_cat (pd.DataFrame): DataFrame containing metadata for the galaxy spectra to be processed, including target IDs.\n",
    "    - batch_size (int, optional): The number of spectra to retrieve in each batch (default is 20).\n",
    "    - max_workers (int, optional): The maximum number of parallel threads for retrieving data (default is 10).\n",
    "\n",
    "    Returns:\n",
    "    - tuple (np.array, np.array, np.array): Arrays of normalized flux data, corresponding wavelength data, and error data for each galaxy spectrum.\n",
    "    \"\"\"\n",
    "    all_fluxes = []\n",
    "    all_wavelengths = []\n",
    "    all_errors = []\n",
    "    total_records = len(zpix_cat)\n",
    "    inc = ['specid', 'redshift', 'flux', 'wavelength', 'ivar', 'spectype', \n",
    "           'specprimary', 'survey', 'program', 'targetid', 'coadd_fiberstatus']\n",
    "\n",
    "    for start_idx in tqdm(range(0, total_records, batch_size), desc=\"Processing spectra in batches\"):\n",
    "        batch = zpix_cat.iloc[start_idx:start_idx+batch_size]\n",
    "        batch_fluxes = []\n",
    "        batch_wavelengths = []\n",
    "        batch_errors = []\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {executor.submit(retrieve_flux, int(row['targetid']), inc): row['targetid'] for _, row in batch.iterrows()}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    flux, wavelength, error = result  \n",
    "                    batch_fluxes.append(flux)\n",
    "                    batch_wavelengths.append(wavelength)\n",
    "                    batch_errors.append(error)\n",
    "                else:\n",
    "                    print(f\"Warning: Missing flux for target ID {futures[future]}\") \n",
    "\n",
    "        all_fluxes.extend(batch_fluxes)\n",
    "        all_wavelengths.extend(batch_wavelengths)\n",
    "        all_errors.extend(batch_errors)\n",
    "\n",
    "    if not all_fluxes:\n",
    "        raise ValueError(\"Error: No flux data retrieved. Check data source or retrieval logic.\")\n",
    "\n",
    "    return np.array(all_fluxes), np.array(all_wavelengths), np.array(all_errors)\n",
    "    \n",
    "def pad_spectra(fluxes, target_length):\n",
    "    \"\"\"\n",
    "    Pads or truncates each flux array in a collection to match a specified target length.\n",
    "    \n",
    "    Parameters:\n",
    "    - fluxes (list of np.array): List of flux arrays for each spectrum.\n",
    "    - target_length (int): Desired length for each flux array, used for padding or truncation.\n",
    "    \n",
    "    Returns:\n",
    "    - np.array: Array of padded or truncated flux data.\n",
    "    \n",
    "    Notes:\n",
    "    - Adds zeros to the end of shorter spectra to match the target length.\n",
    "    - Truncates longer spectra to fit the specified target length.\n",
    "    - Prints a count of spectra that required padding.\n",
    "    \"\"\"\n",
    "    padded_fluxes = []\n",
    "    num_padded = 0  \n",
    "\n",
    "    for flux in fluxes:\n",
    "        if len(flux) < target_length:\n",
    "            padding = np.zeros(target_length - len(flux))\n",
    "            padded_flux = np.concatenate([flux, padding])\n",
    "            num_padded += 1\n",
    "        else:\n",
    "            padded_flux = flux[:target_length]\n",
    "        padded_fluxes.append(padded_flux)\n",
    "\n",
    "    if num_padded > 0:\n",
    "        print(f\"Padding applied to {num_padded} spectra to match the target length.\")\n",
    "    else:\n",
    "        print(\"No padding was necessary; all spectra are of equal length.\")\n",
    "\n",
    "    return np.array(padded_fluxes)\n",
    "\n",
    "\n",
    "\n",
    "def create_padding_mask(fluxes, target_length):\n",
    "    \"\"\"\n",
    "    Generates a binary mask indicating the padded regions in each flux array.\n",
    "    \n",
    "    Parameters:\n",
    "    - fluxes (list of np.array): List of flux arrays for each spectrum.\n",
    "    - target_length (int): Length of the output mask arrays, matching the padded flux array length.\n",
    "    \n",
    "    Returns:\n",
    "    - np.array: Array of binary masks for each flux array, where 1 represents original data and 0 represents padded values.\n",
    "    \n",
    "    Notes:\n",
    "    - The mask is used during loss calculation to ignore the padded regions of the spectra.\n",
    "    \"\"\"\n",
    "    masks = []\n",
    "    for flux in fluxes:\n",
    "        mask = np.ones_like(flux)\n",
    "        if len(flux) < target_length:\n",
    "            mask = np.concatenate([mask, np.zeros(target_length - len(flux))])\n",
    "        masks.append(mask[:target_length])\n",
    "    return np.array(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e95c8f4-0b81-431a-a9a0-552e04c2e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_spectral_data(wavelengths, fluxes, errors, min_wavelength=4000):\n",
    "    \"\"\"\n",
    "    Filters spectral data to retain only wavelengths above a specified minimum value.\n",
    "\n",
    "    Parameters:\n",
    "    - wavelengths (np.ndarray): 2D array of wavelength values (one spectrum per row).\n",
    "    - fluxes (np.ndarray): 2D array of flux values (one spectrum per row).\n",
    "    - errors (np.ndarray): 2D array of error values (one spectrum per row).\n",
    "    - min_wavelength (float): Minimum wavelength to retain in Ångstrom.\n",
    "\n",
    "    Returns:\n",
    "    - filtered_wavelengths (list of np.ndarray): List of filtered wavelength arrays.\n",
    "    - filtered_fluxes (list of np.ndarray): List of filtered flux arrays corresponding to wavelengths.\n",
    "    - filtered_errors (list of np.ndarray): List of filtered error arrays corresponding to wavelengths.\n",
    "    \"\"\"\n",
    "    filtered_wavelengths, filtered_fluxes, filtered_errors = [], [], []\n",
    "\n",
    "    for i in range(len(wavelengths)):\n",
    "        mask = wavelengths[i] >= min_wavelength\n",
    "        filtered_wavelengths.append(wavelengths[i][mask])\n",
    "        filtered_fluxes.append(fluxes[i][mask])\n",
    "        filtered_errors.append(errors[i][mask])\n",
    "\n",
    "    return filtered_wavelengths, filtered_fluxes, filtered_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3170b744-e2c8-4b91-b002-987caaa6bd72",
   "metadata": {},
   "source": [
    "# Autoencoder Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86c6013e-b078-4616-8219-6b96646b2a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNAutoencoderWithSkip(nn.Module):\n",
    "    \"\"\"\n",
    "    A Convolutional Neural Network (CNN)-based autoencoder with skip connections for reconstructing spectral data.\n",
    "    \n",
    "    This model is designed to reduce the dimensionality of spectral data, capturing essential features \n",
    "    during encoding and reconstructing the data during decoding. Skip connections help retain \n",
    "    detailed information that may otherwise be lost in deeper layers, improving reconstruction \n",
    "    quality for tasks like anomaly detection or dimensionality reduction.\n",
    "    \n",
    "    Attributes:\n",
    "    - encoder1 (nn.Conv1d): First convolutional layer of the encoder, reducing input dimensions.\n",
    "    - encoder2 (nn.Conv1d): Second convolutional layer of the encoder, further reducing dimensions.\n",
    "    - encoder3 (nn.Conv1d): Final convolutional layer of the encoder, creating a compressed representation.\n",
    "    - decoder3 (nn.ConvTranspose1d): First transposed convolutional layer of the decoder.\n",
    "    - decoder2 (nn.ConvTranspose1d): Second transposed convolutional layer of the decoder.\n",
    "    - decoder1 (nn.ConvTranspose1d): Final transposed convolutional layer of the decoder, outputting the reconstructed data.\n",
    "    \n",
    "    Methods:\n",
    "    - forward(x): Defines the forward pass of the autoencoder. The input data `x` passes through the encoder \n",
    "      layers to compress it, and then through the decoder layers to reconstruct it. Skip connections are \n",
    "      used to combine encoder and decoder layers at corresponding depths.\n",
    "      \n",
    "    Returns:\n",
    "    - torch.Tensor: The reconstructed tensor, with values squashed between 0 and 1 using a sigmoid activation.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CNNAutoencoderWithSkip, self).__init__()\n",
    "        # Encoder layers with reduced channels\n",
    "        self.encoder1 = nn.Conv1d(1, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.encoder2 = nn.Conv1d(64, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.encoder3 = nn.Conv1d(32, 16, kernel_size=3, stride=2, padding=1)  \n",
    "\n",
    "        # Decoder layers with reduced channels\n",
    "        self.decoder3 = nn.ConvTranspose1d(16, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.decoder2 = nn.ConvTranspose1d(32, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.decoder1 = nn.ConvTranspose1d(64, 1, kernel_size=3, stride=2, padding=1, output_padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding with skip connections\n",
    "        x1 = F.relu(self.encoder1(x))  # Save this for skip connection\n",
    "        x2 = F.relu(self.encoder2(x1))  # Save this for skip connection\n",
    "        x3 = F.relu(self.encoder3(x2))  # Last encoding layer\n",
    "\n",
    "        # Decoding with skip connections\n",
    "        x = F.relu(self.decoder3(x3))\n",
    "        \n",
    "        # Adjust sizes for skip connection with x2\n",
    "        if x2.size(2) > x.size(2):\n",
    "            x2 = x2[:, :, :x.size(2)]\n",
    "        elif x2.size(2) < x.size(2):\n",
    "            x = x[:, :, :x2.size(2)]\n",
    "        x = F.relu(self.decoder2(x + x2))\n",
    "        \n",
    "        # Adjust sizes for skip connection with x1\n",
    "        if x1.size(2) > x.size(2):\n",
    "            x1 = x1[:, :, :x.size(2)]\n",
    "        elif x1.size(2) < x.size(2):\n",
    "            x = x[:, :, :x1.size(2)]\n",
    "        x = self.decoder1(x + x1)\n",
    "\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cccd34d-19bb-44b9-84ec-96679dfcb785",
   "metadata": {},
   "source": [
    "# Custom Loss Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccb89526-7a46-477d-aae7-29757cb5cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mse_loss(observed_flux, target_flux, mask, error_on_observed_flux):\n",
    "    \"\"\"\n",
    "    Calculates a modified Mean Squared Error (MSE) loss based on the \"significance difference\"\n",
    "    between observed and target flux, adjusted by the observational error.\n",
    "\n",
    "    Parameters:\n",
    "    - observed_flux (torch.Tensor): The model's predicted flux values (reconstructed spectra).\n",
    "    - target_flux (torch.Tensor): The ground truth flux values to be compared against observed_flux.\n",
    "    - mask (torch.Tensor): A binary mask that identifies which regions in the spectra should be included\n",
    "                           in the loss calculation, typically to ignore padded regions.\n",
    "    - error_on_observed_flux (torch.Tensor): Observational error associated with each flux point.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: The mean significance-based MSE loss, adjusted by the mask to ignore specific regions.\n",
    "\n",
    "    Process:\n",
    "    1. Compute the \"significance difference\" by normalizing the difference between observed and target flux\n",
    "       using `error_on_observed_flux`, which represents the observational uncertainty.\n",
    "    2. Square the resulting \"significance difference\" to get a MSE-like loss, which is focused on relative\n",
    "       significance rather than absolute difference.\n",
    "    3. Apply the mask to this significance-based MSE loss to focus only on relevant parts of the spectrum.\n",
    "    4. Return the mean of the masked, significance-based MSE loss.\n",
    "    \"\"\"\n",
    "    significance_difference = (observed_flux - target_flux) / (error_on_observed_flux + 1e-5)\n",
    "    mse_loss = significance_difference ** 2\n",
    "    \n",
    "    return (mse_loss * mask).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284f9dc0-7f9c-4766-8c71-acffcca5c7eb",
   "metadata": {},
   "source": [
    "# Training the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92113172-b222-4970-8721-25dd057499d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(model, data, mask, errors, epochs=50, batch_size=32, lr=0.001, grad_clip=1.0):\n",
    "    \"\"\"\n",
    "    Trains an autoencoder model using mini-batch gradient descent with gradient clipping \n",
    "    and a custom weighted MSE loss function that incorporates errors.\n",
    "    \n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The autoencoder model to be trained.\n",
    "    - data (torch.Tensor): Input data tensor (spectra).\n",
    "    - mask (torch.Tensor): Padding mask tensor.\n",
    "    - errors (torch.Tensor): Errors associated with the data.\n",
    "    - epochs, batch_size, lr, grad_clip: Training parameters.\n",
    "    \n",
    "    Process:\n",
    "    1. Initializes the optimizer and trains the model over epochs.\n",
    "    2. Passes errors to the loss function for significance-based weighting.\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0\n",
    "        optimizer.zero_grad()  \n",
    "        \n",
    "        for i in range(0, len(data), batch_size):\n",
    "            batch_data, mask_batch, error_batch = data[i:i+batch_size], mask[i:i+batch_size], errors[i:i+batch_size]\n",
    "            loss = weighted_mse_loss(model(batch_data), batch_data, mask_batch, error_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()  \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_duration = time.time() - start_time\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(data):.8f}, Time: {epoch_duration:.2f}s\")\n",
    "    print(f\"Final Loss after {epochs} epochs: {epoch_loss/len(data):.8f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c570b2-cefd-4bb2-afc7-0437fe55a118",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf594d12-32e8-42b0-888d-e8c0e6bfdbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalous_regions(original_fluxes, reconstructed_fluxes, window_size=50, \n",
    "                             abs_residual_threshold=0.1, rel_residual_threshold=0.1, \n",
    "                             range_mismatch_factor=1.5, overall_anomaly_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Identifies anomalous regions within spectra using residuals and mismatch factors,\n",
    "    with added logging to justify each detected anomaly and the triggering value.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    original_fluxes : np.ndarray\n",
    "        A 2D array of original flux values from the spectra, with shape (num_spectra, spectrum_length).\n",
    "        Each row represents a spectrum, and each column corresponds to a specific wavelength or flux point.\n",
    "\n",
    "    reconstructed_fluxes : np.ndarray\n",
    "        A 2D array of reconstructed flux values from the model, with the same shape as `original_fluxes`.\n",
    "        These are the model’s best attempt to recreate the original spectra, used to compute errors.\n",
    "\n",
    "    window_size : int, optional, default=50\n",
    "        The size of the sliding window (in data points) used to analyze local regions within each spectrum.\n",
    "\n",
    "    abs_residual_threshold : float, optional, default=0.1\n",
    "        The fixed threshold value for absolute residuals beyond which regions are flagged as anomalous.\n",
    "\n",
    "    rel_residual_threshold : float, optional, default=0.1\n",
    "        The fixed threshold value for relative residuals beyond which regions are flagged as anomalous.\n",
    "\n",
    "    range_mismatch_factor : float, optional, default=1.5\n",
    "        A factor to identify anomalies based on the range mismatch between original and reconstructed fluxes.\n",
    "\n",
    "    overall_anomaly_threshold : float, optional, default=0.3\n",
    "        The minimum fraction of a spectrum that must be flagged as anomalous to classify the entire spectrum as anomalous.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    anomalies : list of np.ndarray\n",
    "        A list of boolean arrays, each representing a spectrum. Each array has True at positions corresponding to\n",
    "        flagged regions in the spectrum and False otherwise.\n",
    "\n",
    "    spectrum_anomalies : np.ndarray\n",
    "        A 1D boolean array where each element represents whether the corresponding spectrum is classified as anomalous.\n",
    "\n",
    "    anomaly_metadata : list of list of dict\n",
    "        Metadata for each flagged region in each spectrum, with information on the range of the anomaly,\n",
    "        the type of anomaly (high absolute residual, high relative residual, or range mismatch), and the value that triggered the flag.\n",
    "\n",
    "    abs_residual_threshold : float\n",
    "        The fixed threshold value for absolute residuals beyond which regions are flagged as anomalous.\n",
    "\n",
    "    rel_residual_threshold : float\n",
    "        The fixed threshold value for relative residuals beyond which regions are flagged as anomalous.\n",
    "\n",
    "    range_mismatch_factor : float\n",
    "        The factor used to determine if the reconstructed flux range mismatch flags a region as anomalous.\n",
    "    \"\"\"\n",
    "    num_spectra, spectrum_length = original_fluxes.shape\n",
    "    absolute_residuals = np.abs(original_fluxes - reconstructed_fluxes)\n",
    "    relative_residuals = np.abs((original_fluxes - reconstructed_fluxes) / (original_fluxes + 1e-5))\n",
    "\n",
    "    anomalies, spectrum_anomalies = [], np.zeros(num_spectra, dtype=bool)\n",
    "    anomaly_metadata = []\n",
    "\n",
    "    for i in range(num_spectra):\n",
    "        spectrum_anomalies_count = 0\n",
    "        spectrum_anomaly_flags = np.zeros(spectrum_length, dtype=bool)\n",
    "        anomaly_justification = []\n",
    "\n",
    "        for start in range(0, spectrum_length - window_size + 1, window_size // 2):\n",
    "            end = start + window_size\n",
    "            window_abs_residual = float(np.mean(absolute_residuals[i, start:end]))\n",
    "            window_rel_residual = float(np.mean(relative_residuals[i, start:end]))\n",
    "            original_range = float(np.ptp(original_fluxes[i, start:end]))\n",
    "            reconstructed_range = float(np.ptp(reconstructed_fluxes[i, start:end]))\n",
    "\n",
    "            if (window_abs_residual > abs_residual_threshold or\n",
    "                window_rel_residual > rel_residual_threshold or\n",
    "                reconstructed_range > original_range * range_mismatch_factor):\n",
    "                \n",
    "                spectrum_anomaly_flags[start:end] = True\n",
    "                spectrum_anomalies_count += end - start\n",
    "\n",
    "                reason = {\n",
    "                    \"range_start\": start,\n",
    "                    \"range_end\": end,\n",
    "                    \"reason_type\": (\n",
    "                        \"high_absolute_residual\" if window_abs_residual > abs_residual_threshold else\n",
    "                        \"high_relative_residual\" if window_rel_residual > rel_residual_threshold else\n",
    "                        \"range_mismatch\"\n",
    "                    ),\n",
    "                    \"trigger_value\": (\n",
    "                        window_abs_residual if window_abs_residual > abs_residual_threshold else\n",
    "                        window_rel_residual if window_rel_residual > rel_residual_threshold else\n",
    "                        reconstructed_range / (original_range + 1e-5)\n",
    "                    )\n",
    "                }\n",
    "                anomaly_justification.append(reason)\n",
    "\n",
    "        spectrum_anomalies[i] = spectrum_anomalies_count / spectrum_length > overall_anomaly_threshold \n",
    "        anomalies.append(spectrum_anomaly_flags)\n",
    "        anomaly_metadata.append(anomaly_justification)\n",
    "\n",
    "    return anomalies, spectrum_anomalies, anomaly_metadata, abs_residual_threshold, rel_residual_threshold, range_mismatch_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "995f4d13-2b52-482d-9666-3a7c68193fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_function(params, data):\n",
    "    \"\"\"\n",
    "    Calculates the fitness score for anomaly detection thresholds.\n",
    "    \n",
    "    Parameters:\n",
    "    - params (dict): Contains thresholds for:\n",
    "        - abs_residual_threshold\n",
    "        - rel_residual_threshold\n",
    "        - range_mismatch_factor\n",
    "        - overall_anomaly_threshold\n",
    "    - data (dict): Contains:\n",
    "        - fluxes (original spectra)\n",
    "        - reconstructed_fluxes (from the model)\n",
    "        \n",
    "    Returns:\n",
    "    - float: Fitness score (closer to 0 is better).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        abs_residual_threshold = params[\"abs_residual_threshold\"]\n",
    "        rel_residual_threshold = params[\"rel_residual_threshold\"]\n",
    "        range_mismatch_factor = params[\"range_mismatch_factor\"]\n",
    "        overall_anomaly_threshold = params[\"overall_anomaly_threshold\"]\n",
    "\n",
    "        original_fluxes = data[\"fluxes\"]\n",
    "        reconstructed_fluxes = data[\"reconstructed_fluxes\"]\n",
    "\n",
    "        anomalies, spectrum_anomalies, _, _, _, _ = detect_anomalous_regions(\n",
    "            original_fluxes=original_fluxes,\n",
    "            reconstructed_fluxes=reconstructed_fluxes,\n",
    "            abs_residual_threshold=abs_residual_threshold,\n",
    "            rel_residual_threshold=rel_residual_threshold,\n",
    "            range_mismatch_factor=range_mismatch_factor,\n",
    "            overall_anomaly_threshold=overall_anomaly_threshold,\n",
    "        )\n",
    "\n",
    "        reconstruction_errors = compute_reconstruction_errors(original_fluxes, reconstructed_fluxes, method=\"mse\")\n",
    "        mean_reconstruction_error = np.mean(reconstruction_errors)\n",
    "\n",
    "        anomaly_ratio = np.sum(spectrum_anomalies) / len(original_fluxes)  \n",
    "        false_negative_penalty = np.mean([\n",
    "            1 if not spectrum_anomalies[i] and np.any(anomalies[i]) else 0\n",
    "            for i in range(len(original_fluxes))\n",
    "        ])\n",
    "        false_positive_penalty = np.mean([\n",
    "            1 if spectrum_anomalies[i] and not np.any(anomalies[i]) else 0\n",
    "            for i in range(len(original_fluxes))\n",
    "        ])\n",
    "\n",
    "        threshold_penalty = overall_anomaly_threshold ** 2  \n",
    "\n",
    "        compactness_penalty = np.mean([\n",
    "            1 - (np.sum(anomalies[i]) / len(anomalies[i]))\n",
    "            for i in range(len(anomalies))\n",
    "        ])\n",
    "\n",
    "        fitness = (\n",
    "            -mean_reconstruction_error * 10  # Strong emphasis on reconstruction\n",
    "            - false_negative_penalty * 5  # Reduce false negatives\n",
    "            - false_positive_penalty * 2  # Balance false positives\n",
    "            - threshold_penalty * 5  # Penalize high thresholds\n",
    "            - compactness_penalty * 1  # Light penalty for scattered anomalies\n",
    "        )\n",
    "        return fitness,\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Fitness function failed: {e}\")\n",
    "        return -1e6,  # Large penalty for failures\n",
    "\n",
    "def detection_parameters(data, param_bounds, pop_size=20, generations=50, mutation_rate=0.2):\n",
    "    \"\"\"\n",
    "    Optimize anomaly detection thresholds using a genetic algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    - data (dict): Contains 'fluxes' and 'reconstructed_fluxes'.\n",
    "    - param_bounds (dict): Parameter boundaries for optimization.\n",
    "    - pop_size (int): Population size.\n",
    "    - generations (int): Number of generations.\n",
    "    - mutation_rate (float): Probability of mutation.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Best parameter configuration found.\n",
    "    \"\"\"\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    for key, bounds in param_bounds.items():\n",
    "        toolbox.register(f\"attr_{key}\", random.uniform, bounds[0], bounds[1])\n",
    "\n",
    "    toolbox.register(\n",
    "        \"individual\",\n",
    "        tools.initCycle,\n",
    "        creator.Individual,\n",
    "        tuple(getattr(toolbox, f\"attr_{key}\") for key in param_bounds.keys()),\n",
    "        n=1,\n",
    "    )\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "    def deap_fitness(individual):\n",
    "        params = {key: individual[idx] for idx, key in enumerate(param_bounds.keys())}\n",
    "        return fitness_function(params, data)\n",
    "\n",
    "    toolbox.register(\"evaluate\", deap_fitness)\n",
    "\n",
    "    min_bounds = [bounds[0] for bounds in param_bounds.values()]\n",
    "    max_bounds = [bounds[1] for bounds in param_bounds.values()]\n",
    "    toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
    "    toolbox.register(\"mutate\", tools.mutPolynomialBounded, eta=20.0, low=min_bounds, up=max_bounds, indpb=mutation_rate)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "    population = toolbox.population(n=pop_size)\n",
    "\n",
    "    for gen in tqdm(range(generations), desc=\"Genetic Algorithm Progress\"):\n",
    "        offspring = toolbox.select(population, len(population))\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if random.random() < 0.5:\n",
    "                toolbox.mate(child1, child2)\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "\n",
    "        for mutant in offspring:\n",
    "            if random.random() < mutation_rate:\n",
    "                toolbox.mutate(mutant)\n",
    "                del mutant.fitness.values\n",
    "\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        population[:] = offspring\n",
    "\n",
    "    best_individual = tools.selBest(population, k=1)[0]\n",
    "    best_params = {key: best_individual[idx] for idx, key in enumerate(param_bounds.keys())}\n",
    "    best_fitness = best_individual.fitness.values[0]\n",
    "    print(f\"Optimized Parameters: {best_params}\")\n",
    "    print(f\"Best Fitness: {best_fitness}\")\n",
    "    return best_params, best_fitness\n",
    "def compute_dynamic_anomaly_ratio(reconstruction_errors, mad_multiplier=3):\n",
    "    \"\"\"\n",
    "    Dynamically computes the anomaly ratio using the Median Absolute Deviation (MAD).\n",
    "\n",
    "    Parameters:\n",
    "    - reconstruction_errors (np.ndarray): Array of reconstruction errors.\n",
    "    - mad_multiplier (float): Multiplier for the MAD to define the threshold.\n",
    "\n",
    "    Returns:\n",
    "    - float: Target anomaly ratio based on the error distribution.\n",
    "    \"\"\"\n",
    "    median_error = np.median(reconstruction_errors)\n",
    "    mad_error = np.median(np.abs(reconstruction_errors - median_error))\n",
    "    threshold = median_error + mad_multiplier * mad_error\n",
    "\n",
    "    anomalies = reconstruction_errors > threshold\n",
    "    target_anomaly_ratio = np.mean(anomalies)  \n",
    "\n",
    "    print(f\"Dynamic Anomaly Ratio: {target_anomaly_ratio:.4f} (Threshold: {threshold:.4f})\")\n",
    "    return target_anomaly_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d2f44-2ea6-4682-8e2f-b0f1f3848964",
   "metadata": {},
   "source": [
    "# Spectra Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b452bcb-c624-4ef6-96ce-1fac8069a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_save_path(save_directory, base_filename):\n",
    "    \"\"\"\n",
    "    Generates a unique file path for PNG files in the specified directory by appending\n",
    "    a sequential numeric suffix to avoid overwriting existing files. This ensures that\n",
    "    saved file names are uniquely numbered.\n",
    "\n",
    "    Parameters:\n",
    "    - save_directory (str): The directory where the file should be saved. It will be created if it doesn't exist.\n",
    "    - base_filename (str): The base name for the file, to which a numeric suffix will be added.\n",
    "\n",
    "    Returns:\n",
    "    - str: A full path to the new file with a sequentially numbered suffix in the specified directory,\n",
    "           following the format '{base_filename}_{number}.png'.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    existing_files = os.listdir(save_directory)\n",
    "    numbers = [\n",
    "        int(re.search(r'\\d+', f).group())\n",
    "        for f in existing_files if re.search(fr'{base_filename}_(\\d+)\\.png', f)\n",
    "    ]\n",
    "    next_number = max(numbers) + 1 if numbers else 1\n",
    "    full_path = os.path.join(save_directory, f'{base_filename}_{next_number}.png')\n",
    "    \n",
    "    relative_path = os.path.relpath(full_path, start=os.path.dirname(save_directory))\n",
    "    \n",
    "    return relative_path\n",
    "    \n",
    "def create_json_save_path(save_directory, base_filename):\n",
    "    \"\"\"\n",
    "    Generates a unique file path for JSON files in the specified directory by appending\n",
    "    a sequential numeric suffix to avoid overwriting existing files. This ensures that\n",
    "    saved JSON file names are uniquely numbered.\n",
    "\n",
    "    Parameters:\n",
    "    - save_directory (str): The directory where the JSON file should be saved. It will be created if it doesn't exist.\n",
    "    - base_filename (str): The base name for the JSON file, to which a numeric suffix will be added.\n",
    "\n",
    "    Returns:\n",
    "    - str: A full path to the new JSON file with a sequentially numbered suffix in the specified directory,\n",
    "           following the format '{base_filename}_{number}.json'.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    existing_files = os.listdir(save_directory)\n",
    "    numbers = [\n",
    "        int(re.search(rf'{base_filename}_(\\d+)\\.json', f).group(1))\n",
    "        for f in existing_files if re.search(rf'{base_filename}_(\\d+)\\.json', f)\n",
    "    ]\n",
    "    next_number = max(numbers) + 1 if numbers else 1\n",
    "    full_path = os.path.join(save_directory, f'{base_filename}_{next_number}.json')\n",
    "    relative_path = os.path.relpath(full_path, start=os.path.dirname(save_directory))\n",
    "    \n",
    "    return relative_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7599b66d-e5da-4766-b709-6c62a6af0768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sampling_info(galaxy_ids, plot_galaxy_ids, seed, anomaly_metadata, abs_residual_threshold, \n",
    "                       rel_residual_threshold, range_mismatch_factor, json_directory, zpix_cat):\n",
    "    \"\"\"\n",
    "    Saves detailed sampling information for detected anomalous spectra to a JSON file. This includes \n",
    "    metadata on each detected anomaly, threshold values, and unique target IDs associated with the anomalies.\n",
    "\n",
    "    Parameters:\n",
    "    - galaxy_ids (list of int): Indices of galaxies identified as anomalous, corresponding to row indices in `zpix_cat`.\n",
    "    - plot_galaxy_ids (list of int): Indices of galaxies selected for plotting, used to match target IDs if needed.\n",
    "    - seed (int): Random seed used during sampling to ensure reproducibility.\n",
    "    - anomaly_metadata (list of list of dict): Metadata for each galaxy's anomaly detection results, where each inner list\n",
    "      contains dictionaries with specific anomaly information (e.g., range, justification, trigger value).\n",
    "    - abs_residual_threshold (float): Threshold for detecting high absolute residual anomalies.\n",
    "    - rel_residual_threshold (float): Threshold for detecting high relative residual anomalies.\n",
    "    - range_mismatch_factor (float): Factor threshold for range mismatches between original and reconstructed spectra.\n",
    "    - json_directory (str): Directory path where the JSON file should be saved. Will be created if it doesn't exist.\n",
    "    - zpix_cat (DataFrame): DataFrame containing galaxy catalog information, including `targetid` for each galaxy.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function saves a JSON file containing the sampling and anomaly data.\n",
    "\n",
    "    Process:\n",
    "    1. Maps `galaxy_ids` to actual `targetid` values using `zpix_cat` to provide unique identifiers for each galaxy.\n",
    "    2. Constructs a dictionary `sampling_info` to store:\n",
    "       - Count of anomalies detected.\n",
    "       - Sampling seed for reproducibility.\n",
    "       - Thresholds for absolute residual, relative residual, and range mismatch.\n",
    "       - An `anomalous_spectra` dictionary keyed by `targetid`, where each entry holds `anomaly_ranges` metadata.\n",
    "    3. For each `targetid`, populates `anomaly_ranges` with details on each anomaly's range, justification, \n",
    "       and the value triggering the anomaly.\n",
    "    4. Uses `create_json_save_path` to generate a unique, sequentially numbered file path for saving.\n",
    "    5. Writes `sampling_info` as JSON to the generated path and confirms save location with a printed message.\n",
    "    \"\"\"\n",
    "    target_ids = [int(zpix_cat['targetid'].iloc[idx]) for idx in galaxy_ids]\n",
    "\n",
    "    sampling_info = {\n",
    "        \"Number of Anomalies\": len(target_ids),\n",
    "        \"Seed\": int(seed),\n",
    "        \"absolute_residual_threshold\": float(abs_residual_threshold),\n",
    "        \"relative_residual_threshold\": float(rel_residual_threshold),\n",
    "        \"range_mismatch_factor\": float(range_mismatch_factor),\n",
    "        \"anomalous_spectra\": {}\n",
    "    }\n",
    "\n",
    "    for idx, target_id in enumerate(target_ids):\n",
    "        metadata = anomaly_metadata[idx]\n",
    "        anomaly_ranges = []\n",
    "\n",
    "        for reason in metadata:\n",
    "            anomaly_ranges.append({\n",
    "                \"range_start\": reason[\"range_start\"],\n",
    "                \"range_end\": reason[\"range_end\"],\n",
    "                \"justification\": reason[\"reason_type\"],\n",
    "                \"value\": reason.get(\"trigger_value\", None)  \n",
    "            })\n",
    "\n",
    "        sampling_info[\"anomalous_spectra\"][target_id] = {\n",
    "            \"anomaly_ranges\": anomaly_ranges\n",
    "        }\n",
    "\n",
    "    json_path = create_json_save_path(json_directory, 'sampling_info')\n",
    "    secondary_path = create_json_save_path(OUT_DIR, 'sampling_info')\n",
    "\n",
    "    with open(json_path, 'w') as json_file:\n",
    "        json.dump(sampling_info, json_file, indent=4)\n",
    "    with open(secondary_path, 'w') as json_file:\n",
    "        json.dump(sampling_info, json_file, indent=4)\n",
    "    \n",
    "    print(f\"Sampling information saved to {json_path} and {secondary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ffdb8e1-82ef-4f02-85e9-88da79d9eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_image(ra, dec, pixscale=0.262, width=256, height=256, survey=\"ls-dr10\"):\n",
    "    \"\"\"\n",
    "    Fetches an image from the Legacy Survey for the specified coordinates and overlays a circle representing \n",
    "    the DESI fiber diameter (1.5 arcseconds).\n",
    "\n",
    "    Parameters:\n",
    "    - ra (float): Right Ascension (RA) in degrees for the target.\n",
    "    - dec (float): Declination (DEC) in degrees for the target.\n",
    "    - pixscale (float): Pixel scale in arcseconds per pixel. Default is 0.262 for Legacy Survey.\n",
    "    - width (int): Width of the returned image in pixels. Default is 256.\n",
    "    - height (int): Height of the returned image in pixels. Default is 256.\n",
    "    - survey (str): Legacy Survey data release, e.g., \"ls-dr10\".\n",
    "\n",
    "    Returns:\n",
    "    - Image: PIL Image object with the circle overlayed.\n",
    "    - None: Returns None if the image retrieval fails.\n",
    "    \"\"\"\n",
    "    fiber_diameter_arcsec = 1.5\n",
    "\n",
    "    fiber_radius_pixels = fiber_diameter_arcsec / (2 * pixscale)\n",
    "\n",
    "    url = f\"https://www.legacysurvey.org/viewer/jpeg-cutout?ra={ra}&dec={dec}\" \\\n",
    "          f\"&pixscale={pixscale}&layer={survey}&size={max(width, height)}\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        img_data = BytesIO(response.content)\n",
    "        img = Image.open(img_data).convert(\"RGBA\")  \n",
    "\n",
    "        draw = ImageDraw.Draw(img)\n",
    "\n",
    "        center_x, center_y = width // 2, height // 2\n",
    "\n",
    "        draw.ellipse(\n",
    "            [\n",
    "                (center_x - fiber_radius_pixels, center_y - fiber_radius_pixels),\n",
    "                (center_x + fiber_radius_pixels, center_y + fiber_radius_pixels)\n",
    "            ],\n",
    "            outline=\"white\",  \n",
    "            width=2           \n",
    "        )\n",
    "        return img\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def plot_spectra(\n",
    "    original_fluxes, reconstructed_fluxes, anomalous_regions, spectrum_anomalies,\n",
    "    zpix_cat, wavelengths=None, save_directory='SpectralCNNAutoencoder_output', max_samples=20, \n",
    "    seed=None, only_anomalous=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots original and reconstructed spectra for multiple galaxies, highlighting anomalous spectra.\n",
    "    \n",
    "    Parameters:\n",
    "    - original_fluxes (list of ndarray): The original flux values for each spectrum.\n",
    "    - reconstructed_fluxes (list of ndarray): Reconstructed flux values, for comparison with original fluxes.\n",
    "    - anomalous_regions (list of lists of bool): Each list contains boolean values indicating anomalous regions in each spectrum.\n",
    "    - spectrum_anomalies (list of bool): Indicates if a given spectrum is anomalous. Used to filter if only_anomalous is set to True.\n",
    "    - zpix_cat (DataFrame): Metadata with RA/Dec information for each spectrum, allowing for SDSS image retrieval.\n",
    "    - wavelengths (ndarray, optional): Wavelengths to use for x-axis; defaults to index if not provided.\n",
    "    - save_directory (str, optional): Directory path to save the generated plot image. Default is 'output_images'.\n",
    "    - max_samples (int, optional): Maximum number of spectra to plot. Useful for limiting data when dataset is large. Default is 20.\n",
    "    - seed (int, optional): Seed for random sampling, ensuring reproducibility. If None, a random seed is used. Default is None.\n",
    "    - only_anomalous (bool, optional): If True, plots only the spectra identified as anomalous. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (anomalous_indices, plot_indices, seed)\n",
    "        - anomalous_indices (list): List of indices for spectra flagged as anomalous.\n",
    "        - plot_indices (list): Indices of spectra that were plotted.\n",
    "        - seed (int): Random seed used for sampling.\n",
    "    \"\"\"\n",
    "    \n",
    "    anomalous_indices = [i for i, is_anomalous in enumerate(spectrum_anomalies) if is_anomalous] if only_anomalous else list(range(len(original_fluxes)))\n",
    "    \n",
    "    if not anomalous_indices:\n",
    "        print(\"No anomalous spectra detected.\")\n",
    "        return [], [], None  \n",
    "        \n",
    "    if seed is None:\n",
    "        seed = random.randint(0, 10000)\n",
    "    random.seed(seed)\n",
    "    print(f\"Random sampling seed: {seed}\")\n",
    "\n",
    "    plot_indices = random.sample(anomalous_indices, min(max_samples, len(anomalous_indices)))\n",
    "\n",
    "    image_width, image_height = 256, 256\n",
    "\n",
    "    fig, axes = plt.subplots(len(plot_indices), 2, figsize=(20, 6 * len(plot_indices)), gridspec_kw={'width_ratios': [3, 1]})\n",
    "    if len(plot_indices) == 1:\n",
    "        axes = [axes] \n",
    "\n",
    "    for idx, i in enumerate(plot_indices):\n",
    "        ra, dec = zpix_cat.loc[zpix_cat['targetid'] == zpix_cat['targetid'].iloc[i], ['mean_fiber_ra', 'mean_fiber_dec']].values[0]\n",
    "        x_axis = wavelengths if wavelengths is not None else range(len(original_fluxes[i]))\n",
    "\n",
    "        ax_spectra = axes[idx, 0]\n",
    "        ax_spectra.plot(x_axis, original_fluxes[i], label=\"Original\", color='#2c7bb6', linewidth=0.5)\n",
    "        ax_spectra.plot(x_axis, reconstructed_fluxes[i], label=\"Reconstructed\", color='#d7191c', alpha=0.7, linewidth=0.5)\n",
    "\n",
    "        in_anomaly, anomaly_start = False, 0\n",
    "        for j in range(len(x_axis)):\n",
    "            if anomalous_regions[i][j] and not in_anomaly:\n",
    "                anomaly_start = j\n",
    "                in_anomaly = True\n",
    "            elif not anomalous_regions[i][j] and in_anomaly:\n",
    "                ax_spectra.axvspan(x_axis[anomaly_start], x_axis[j], color='#fdae61', alpha=0.7)\n",
    "                in_anomaly = False\n",
    "        if in_anomaly:\n",
    "            ax_spectra.axvspan(x_axis[anomaly_start], x_axis[-1], color='#fdae61', alpha=0.7)\n",
    "\n",
    "        if spectrum_anomalies[i]:\n",
    "            ax_spectra.set_facecolor('#fee090')\n",
    "            ax_spectra.set_title(f\"Spectrum ID: {zpix_cat['targetid'].iloc[i]} - Anomalous\", color='red',fontsize=23)\n",
    "        else:\n",
    "            ax_spectra.set_title(f\"Spectrum ID: {zpix_cat['targetid'].iloc[i]}\", color='black',fontsize=23)\n",
    "        ax_spectra.xaxis.set_visible(False)  \n",
    "\n",
    "        divider = make_axes_locatable(ax_spectra)\n",
    "        ax_residual = divider.append_axes(\"bottom\", size=\"25%\", pad=0, sharex=ax_spectra)\n",
    "        ax_residual.plot(x_axis, original_fluxes[i] - reconstructed_fluxes[i], color='#4dac26', linewidth=0.5)\n",
    "        ax_residual.set_ylabel(\"Residuals\",fontsize=20)\n",
    "        ax_spectra.set_ylabel(\"Flux (normalized)\",fontsize=20)\n",
    "        ax_residual.set_xlabel(\"Wavelength (Å)\",fontsize=20)\n",
    "        ax_spectra.legend()\n",
    "\n",
    "        img_data = fetch_image(ra, dec)\n",
    "        ax_image = axes[idx, 1]\n",
    "        if img_data:\n",
    "            img = img_data\n",
    "            ax_image.imshow(img)\n",
    "            ax_image.axis(\"off\")\n",
    "            ax_image.set_title(f\"Galaxy Image\\nRA={ra:.4f}, Dec={dec:.4f}\",fontsize=23)\n",
    "            circle_radius = 30\n",
    "            circle = patches.Circle((image_width / 2, image_height / 2), circle_radius, transform=ax_image.transData, \n",
    "                                    edgecolor=\"white\", facecolor=\"none\", linewidth=2)\n",
    "            ax_image.add_patch(circle)\n",
    "        else:\n",
    "            ax_image.text(0.5, 0.5, \"Image Not Found\", ha=\"center\", va=\"center\")\n",
    "        ax_image.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    base_filename = \"anomalous_spectra\" if only_anomalous else \"spectra_reconstruction\"\n",
    "    save_path = create_save_path(save_directory, base_filename)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "    print(f\"Figure saved to {save_path}\")\n",
    "    print(f\"Number of anomalous spectra: {len(anomalous_indices)}\")\n",
    "    return anomalous_indices, plot_indices, seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b052ad9-acde-41d6-97a8-71c828627cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_autoencoder(autoencoder, input_data):\n",
    "    \"\"\"\n",
    "    Visualizes the entire structure of the autoencoder model by generating a graph of the\n",
    "    network's architecture, including both encoder and decoder layers, and saves it as an image.\n",
    "\n",
    "    Parameters:\n",
    "    - autoencoder (nn.Module): The autoencoder model instance to visualize.\n",
    "    - input_data (torch.Tensor): A sample input tensor for passing through the model to generate \n",
    "                                 the visualization. This input should match the model's expected input shape.\n",
    "    Returns:\n",
    "    - None. The function saves the visualization image file to `OUT_DIR` and outputs the path.\n",
    "    \"\"\"\n",
    "    outputs = autoencoder(input_data)\n",
    "    model_viz = make_dot(outputs, params=dict(autoencoder.named_parameters()))\n",
    "    model_viz.format = \"png\"\n",
    "    save_path = create_save_path(OUT_DIR, 'autoencoder_visualization')\n",
    "    model_viz.render(save_path.replace(\".png\", \"\"))\n",
    "    print(f\"Full autoencoder visualization saved to {save_path}\")\n",
    "\n",
    "def visualize_encoder(autoencoder, input_data):\n",
    "    \"\"\"\n",
    "    Visualizes the encoder section of the autoencoder model, which compresses input data into a \n",
    "    lower-dimensional representation. Saves the encoder visualization as an image.\n",
    "\n",
    "    Parameters:\n",
    "    - autoencoder (nn.Module): The autoencoder model instance containing the encoder layers.\n",
    "    - input_data (torch.Tensor): A sample input tensor for passing through only the encoder \n",
    "                                 layers. The input shape should match the encoder's expected input.\n",
    "\n",
    "    Returns:\n",
    "    - None. The function saves the encoder visualization image to `OUT_DIR` and outputs the path.\n",
    "    \"\"\"\n",
    "    encoder_output = autoencoder.encoder3(autoencoder.encoder2(autoencoder.encoder1(input_data)))\n",
    "    encoder_viz = make_dot(encoder_output, params=dict(autoencoder.named_parameters()), \n",
    "                           show_attrs=True, show_saved=True)\n",
    "    encoder_viz.format = \"png\"\n",
    "    save_path = create_save_path(OUT_DIR, 'encoder_visualization')\n",
    "    encoder_viz.render(save_path.replace(\".png\", \"\"))\n",
    "    print(f\"Encoder visualization saved to {save_path}\")\n",
    "\n",
    "\n",
    "def visualize_decoder(autoencoder, encoded_input):\n",
    "    \"\"\"\n",
    "    Visualizes the decoder portion of the autoencoder model, which reconstructs the original \n",
    "    data from a lower-dimensional encoding. Saves the decoder visualization as an image.\n",
    "\n",
    "    Parameters:\n",
    "    - autoencoder (nn.Module): The autoencoder model instance containing the decoder layers.\n",
    "    - encoded_input (torch.Tensor): An encoded representation that serves as the input to the \n",
    "                                    decoder layers. This should match the expected shape of the \n",
    "                                    decoder's input.\n",
    "    Returns:\n",
    "    - None. The function saves the decoder visualization image to `OUT_DIR` and outputs the path.\n",
    "    \"\"\"\n",
    "    decoder_output = autoencoder.decoder1(autoencoder.decoder2(autoencoder.decoder3(encoded_input)))\n",
    "    decoder_viz = make_dot(decoder_output, params=dict(autoencoder.named_parameters()), \n",
    "                           show_attrs=True, show_saved=True)\n",
    "    decoder_viz.format = \"png\"\n",
    "    save_path = create_save_path(OUT_DIR, 'decoder_visualization')\n",
    "    decoder_viz.render(save_path.replace(\".png\", \"\"))\n",
    "    print(f\"Decoder visualization saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e54b111b-4923-485b-9a1f-fa9e6831870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reconstruction_errors(original_fluxes, reconstructed_fluxes, method=\"mse\"):\n",
    "    \"\"\"\n",
    "    Computes reconstruction errors between original and reconstructed fluxes.\n",
    "\n",
    "    Parameters:\n",
    "    - original_fluxes (np.ndarray): Original flux values.\n",
    "    - reconstructed_fluxes (np.ndarray): Reconstructed flux values from the model.\n",
    "    - method (str): Error computation method, either 'mse' for mean squared error or 'mae' for mean absolute error.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Array of errors with shape (num_spectra, num_wavelengths).\n",
    "    \"\"\"\n",
    "    if method == \"mse\":\n",
    "        errors = (original_fluxes - reconstructed_fluxes) ** 2\n",
    "    elif method == \"mae\":\n",
    "        errors = np.abs(original_fluxes - reconstructed_fluxes)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'mse' or 'mae'\")\n",
    "    \n",
    "    return errors\n",
    "\n",
    "def plot_reconstruction_error_distribution(reconstruction_errors, wavelengths, save_directory='output_images'):\n",
    "    \"\"\"\n",
    "    Visualizes the reconstruction error distribution across wavelengths.\n",
    "\n",
    "    Parameters:\n",
    "    - reconstruction_errors (np.ndarray): Array of reconstruction errors per spectrum and wavelength.\n",
    "    - wavelengths (array-like): Array of wavelength values corresponding to each error point.\n",
    "    - save_directory (str, optional): Directory path to save the generated plot image.\n",
    "    \"\"\"\n",
    "    if wavelengths is None or len(wavelengths) != reconstruction_errors.shape[1]:\n",
    "        raise ValueError(\"Wavelengths array must be provided and must match the number of wavelengths in reconstruction errors.\")\n",
    "    \n",
    "    mean_errors = np.mean(reconstruction_errors, axis=0)\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(wavelengths, mean_errors, label='Mean Reconstruction Error', color='#0571b0', linewidth=0.5)\n",
    "    plt.fill_between(wavelengths, mean_errors, color='#67a9cf', alpha=1)\n",
    "    plt.xlabel(\"Wavelength (Å)\")\n",
    "    plt.ylabel(\"Mean Reconstruction Error\")\n",
    "    plt.legend()\n",
    "\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    line_plot_path = create_save_path(save_directory, 'mean_reconstruction_error')\n",
    "    plt.savefig(line_plot_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Plot of mean reconstruction error saved to {line_plot_path}\")\n",
    "    \n",
    "def plot_redshift_distribution(zpix_cat, output_directory):\n",
    "    \"\"\"\n",
    "    Plots and saves a histogram of redshift values from the zpix_cat DataFrame \n",
    "    and prints key redshift statistics.\n",
    "\n",
    "    Parameters:\n",
    "    - zpix_cat (pd.DataFrame): DataFrame containing redshift values in a column named 'z'.\n",
    "    - output_directory (str): Path to the directory where the plot will be saved.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    redshifts = zpix_cat['z'].to_numpy()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(redshifts, bins=50, kde=True, color=\"blue\", alpha=0.7)\n",
    "    plt.xlabel(\"Redshift (z)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Distribution of Redshifts in the Dataset\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    line_plot_path = create_save_path(OUT_DIR, 'redshift_dist')\n",
    "    plt.savefig(line_plot_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Redshift distribution plot saved to {line_plot_path}\")\n",
    "\n",
    "    print(f\"Total Galaxies: {len(redshifts)}\")\n",
    "    print(f\"Redshift Range: {redshifts.min():.2f} - {redshifts.max():.2f}\")\n",
    "    print(f\"Mean Redshift: {redshifts.mean():.2f}\")\n",
    "    print(f\"Median Redshift: {np.median(redshifts):.2f}\")\n",
    "    print(f\"Number of Galaxies with Redshift <= 0.5: {np.sum(redshifts <= 0.5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3ac33f-a5bc-4145-9c93-b08cf08ad946",
   "metadata": {},
   "source": [
    "# Main Execution Flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20ecf6e5-8458-43ed-aced-4de612c90805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /Users/elicox/Desktop/Mac/Work/Yr4 Work/Project/CNN-auto/spectra_data>2.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing spectra in batches: 100%|██████████| 28/28 [09:32<00:00, 20.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No padding was necessary; all spectra are of equal length.\n",
      "No padding was necessary; all spectra are of equal length.\n",
      "No padding was necessary; all spectra are of equal length.\n"
     ]
    }
   ],
   "source": [
    "zpix_cat = load_or_query_data(CSV_PATH)\n",
    "if zpix_cat is not None:\n",
    "    all_fluxes, all_wavelengths, all_errors = process_spectra_data(zpix_cat)\n",
    "    \n",
    "    max_length = max(len(f) for f in all_fluxes)\n",
    "    \n",
    "    all_fluxes_padded = pad_spectra(all_fluxes, max_length)\n",
    "    all_wavelengths_padded = pad_spectra(all_wavelengths, max_length)\n",
    "    all_errors_padded = pad_spectra(all_errors, max_length)\n",
    "    \n",
    "    mask = create_padding_mask(all_fluxes, max_length)\n",
    "    \n",
    "\n",
    "    all_fluxes_tensor = torch.tensor(all_fluxes_padded, dtype=torch.float32).unsqueeze(1)\n",
    "    all_wavelengths_tensor = torch.tensor(all_wavelengths_padded, dtype=torch.float32).unsqueeze(1)\n",
    "    all_errors_tensor = torch.tensor(all_errors_padded, dtype=torch.float32).unsqueeze(1)\n",
    "    mask_tensor = torch.tensor(mask, dtype=torch.float32).unsqueeze(1)\n",
    "    \n",
    "    wavelengths = np.mean(all_wavelengths, axis=0) if len(set(map(len, all_wavelengths))) == 1 else all_wavelengths[0]\n",
    "else:\n",
    "    print(\"No data available for processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9d13d5f-27e5-42b2-8c65-7395f0cf875f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.02630807, Time: 5.08s\n",
      "Epoch [2/50], Loss: 0.01291959, Time: 4.13s\n",
      "Epoch [3/50], Loss: 0.00353922, Time: 3.95s\n",
      "Epoch [4/50], Loss: 0.00123957, Time: 3.79s\n",
      "Epoch [5/50], Loss: 0.00050564, Time: 3.95s\n",
      "Epoch [6/50], Loss: 0.00027383, Time: 5.95s\n",
      "Epoch [7/50], Loss: 0.00019097, Time: 4.60s\n",
      "Epoch [8/50], Loss: 0.00014494, Time: 4.44s\n",
      "Epoch [9/50], Loss: 0.00012458, Time: 4.93s\n",
      "Epoch [10/50], Loss: 0.00010752, Time: 5.11s\n",
      "Epoch [11/50], Loss: 0.00010162, Time: 4.16s\n",
      "Epoch [12/50], Loss: 0.00009732, Time: 4.09s\n",
      "Epoch [13/50], Loss: 0.00009356, Time: 5.12s\n",
      "Epoch [14/50], Loss: 0.00008985, Time: 4.31s\n",
      "Epoch [15/50], Loss: 0.00008416, Time: 4.14s\n",
      "Epoch [16/50], Loss: 0.00007706, Time: 4.32s\n",
      "Epoch [17/50], Loss: 0.00007088, Time: 3.85s\n",
      "Epoch [18/50], Loss: 0.00006284, Time: 4.30s\n",
      "Epoch [19/50], Loss: 0.00005951, Time: 5.10s\n",
      "Epoch [20/50], Loss: 0.00005538, Time: 4.40s\n",
      "Epoch [21/50], Loss: 0.00005192, Time: 4.14s\n",
      "Epoch [22/50], Loss: 0.00004899, Time: 4.18s\n",
      "Epoch [23/50], Loss: 0.00004983, Time: 4.84s\n",
      "Epoch [24/50], Loss: 0.00005400, Time: 4.28s\n",
      "Epoch [25/50], Loss: 0.00005659, Time: 4.15s\n",
      "Epoch [26/50], Loss: 0.00006006, Time: 4.00s\n",
      "Epoch [27/50], Loss: 0.00005246, Time: 4.01s\n",
      "Epoch [28/50], Loss: 0.00004529, Time: 3.76s\n",
      "Epoch [29/50], Loss: 0.00004904, Time: 3.89s\n",
      "Epoch [30/50], Loss: 0.00004036, Time: 3.87s\n",
      "Epoch [31/50], Loss: 0.00004480, Time: 3.91s\n",
      "Epoch [32/50], Loss: 0.00003878, Time: 3.76s\n",
      "Epoch [33/50], Loss: 0.00003843, Time: 4.38s\n",
      "Epoch [34/50], Loss: 0.00003390, Time: 4.40s\n",
      "Epoch [35/50], Loss: 0.00003739, Time: 4.34s\n",
      "Epoch [36/50], Loss: 0.00003265, Time: 4.09s\n",
      "Epoch [37/50], Loss: 0.00003252, Time: 4.06s\n",
      "Epoch [38/50], Loss: 0.00002635, Time: 4.11s\n",
      "Epoch [39/50], Loss: 0.00003069, Time: 4.26s\n",
      "Epoch [40/50], Loss: 0.00003111, Time: 4.05s\n",
      "Epoch [41/50], Loss: 0.00003706, Time: 3.72s\n",
      "Epoch [42/50], Loss: 0.00002398, Time: 3.96s\n",
      "Epoch [43/50], Loss: 0.00003398, Time: 4.19s\n",
      "Epoch [44/50], Loss: 0.00003601, Time: 4.10s\n",
      "Epoch [45/50], Loss: 0.00002394, Time: 4.27s\n",
      "Epoch [46/50], Loss: 0.00002649, Time: 3.72s\n",
      "Epoch [47/50], Loss: 0.00002639, Time: 4.16s\n",
      "Epoch [48/50], Loss: 0.00002961, Time: 4.00s\n",
      "Epoch [49/50], Loss: 0.00001901, Time: 3.98s\n",
      "Epoch [50/50], Loss: 0.00001882, Time: 4.58s\n",
      "Final Loss after 50 epochs: 0.00001882\n"
     ]
    }
   ],
   "source": [
    "filtered_wavelengths, filtered_fluxes, filtered_errors = filter_spectral_data(\n",
    "    wavelengths=all_wavelengths,\n",
    "    fluxes=all_fluxes_padded,\n",
    "    errors=all_errors_padded,\n",
    "    min_wavelength=4000\n",
    ")\n",
    "\n",
    "max_length = max(len(f) for f in filtered_fluxes)\n",
    "padded_fluxes = np.array([np.pad(f, (0, max_length - len(f)), constant_values=0) for f in filtered_fluxes])\n",
    "padded_errors = np.array([np.pad(e, (0, max_length - len(e)), constant_values=0) for e in filtered_errors])\n",
    "\n",
    "filtered_flux_tensor = torch.tensor(padded_fluxes, dtype=torch.float32).unsqueeze(1)  \n",
    "filtered_error_tensor = torch.tensor(padded_errors, dtype=torch.float32).unsqueeze(1)  \n",
    "filtered_mask_tensor = (filtered_flux_tensor > 0).float()\n",
    "\n",
    "autoencoder = CNNAutoencoderWithSkip()\n",
    "train_autoencoder(autoencoder, filtered_flux_tensor, filtered_mask_tensor, filtered_error_tensor)\n",
    "\n",
    "autoencoder.eval()\n",
    "reconstructed_fluxes = autoencoder(filtered_flux_tensor).detach().numpy().squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb60bd99-0a34-477e-84e6-64e297f44d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Genetic Algorithm Progress: 100%|██████████| 50/50 [1:05:34<00:00, 78.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Parameters: {'abs_residual_threshold': 1.6401159722358578, 'rel_residual_threshold': 1.4051370923060555, 'range_mismatch_factor': 2.275631548528253, 'overall_anomaly_threshold': 0.46746769381116254}\n",
      "Best Fitness: 0.0022660213473495094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"fluxes\": filtered_flux_tensor.numpy().squeeze(),\n",
    "    \"reconstructed_fluxes\": reconstructed_fluxes,\n",
    "}\n",
    "parameter_bounds = {\n",
    "    \"abs_residual_threshold\": (0.05, 2.0),      \n",
    "    \"rel_residual_threshold\": (0.05, 2.0),      \n",
    "    \"range_mismatch_factor\": (1.0, 3.0),        \n",
    "    \"overall_anomaly_threshold\": (0.0, 1.0)     \n",
    "}\n",
    "best_params, best_fitness = detection_parameters(\n",
    "    data=data,\n",
    "    param_bounds=parameter_bounds,\n",
    "    pop_size=20,\n",
    "    generations=50,\n",
    "    mutation_rate=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58a8e9b5-4d0f-4adb-a2cc-8e1ab1bdc9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redshift distribution plot saved to SpectralCNNAutoencoder_output/redshift_dist_14.png\n",
      "Total Galaxies: 543\n",
      "Redshift Range: -0.00 - 0.50\n",
      "Mean Redshift: 0.36\n",
      "Median Redshift: 0.37\n",
      "Number of Galaxies with Redshift <= 0.5: 543\n",
      "Random sampling seed: 1846\n",
      "Figure saved to SpectralCNNAutoencoder_output/spectra_reconstruction_33.png\n",
      "Number of anomalous spectra: 543\n",
      "Plot of mean reconstruction error saved to SpectralCNNAutoencoder_output/mean_reconstruction_error_73.png\n",
      "Original Fitness: 0.00045046416843572894\n",
      "Initial Fitness: 0.002213027552324865\n",
      "Optimized Fitness: 0.0022660213473495094\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Plot redshift distribution for the data\n",
    "plot_redshift_distribution(zpix_cat, OUT_DIR)\n",
    "\n",
    "(anomalous_regions, spectrum_anomalies, anomaly_metadata, \n",
    " abs_residual_threshold, rel_residual_threshold, range_mismatch_factor) = detect_anomalous_regions(\n",
    "    original_fluxes=filtered_flux_tensor.numpy().squeeze(),\n",
    "    reconstructed_fluxes=reconstructed_fluxes,\n",
    "    window_size=50,\n",
    "    abs_residual_threshold=best_params[\"abs_residual_threshold\"],\n",
    "    rel_residual_threshold=best_params[\"rel_residual_threshold\"],\n",
    "    range_mismatch_factor=best_params[\"range_mismatch_factor\"],\n",
    "    overall_anomaly_threshold=best_params[\"overall_anomaly_threshold\"]\n",
    ")\n",
    "\n",
    "anomalous_indices, plot_indices, seed = plot_spectra(\n",
    "    original_fluxes=filtered_flux_tensor.numpy().squeeze(),\n",
    "    reconstructed_fluxes=reconstructed_fluxes,\n",
    "    anomalous_regions=anomalous_regions,\n",
    "    spectrum_anomalies=spectrum_anomalies,\n",
    "    zpix_cat=zpix_cat,\n",
    "    wavelengths=np.mean(filtered_wavelengths, axis=0),  \n",
    "    save_directory='SpectralCNNAutoencoder_output',\n",
    "    max_samples = 10,\n",
    "    only_anomalous=False\n",
    ")\n",
    "reconstruction_errors = compute_reconstruction_errors(\n",
    "    original_fluxes=filtered_flux_tensor.numpy().squeeze(),\n",
    "    reconstructed_fluxes=reconstructed_fluxes\n",
    ")\n",
    "plot_reconstruction_error_distribution(\n",
    "    reconstruction_errors=reconstruction_errors,\n",
    "    wavelengths=np.mean(filtered_wavelengths, axis=0),  \n",
    "    save_directory=OUT_DIR\n",
    ")\n",
    "default_params = {\n",
    "    \"abs_residual_threshold\": (parameter_bounds[\"abs_residual_threshold\"][0] + parameter_bounds[\"abs_residual_threshold\"][1]) / 2,\n",
    "    \"rel_residual_threshold\": (parameter_bounds[\"rel_residual_threshold\"][0] + parameter_bounds[\"rel_residual_threshold\"][1]) / 2,\n",
    "    \"range_mismatch_factor\": (parameter_bounds[\"range_mismatch_factor\"][0] + parameter_bounds[\"range_mismatch_factor\"][1]) / 2,\n",
    "    \"overall_anomaly_threshold\": (parameter_bounds[\"overall_anomaly_threshold\"][0] + parameter_bounds[\"overall_anomaly_threshold\"][1]) / 2,\n",
    "}\n",
    "original_params = {\"abs_residual_threshold\":0.1, \"rel_residual_threshold\":0.1, \n",
    "                             \"range_mismatch_factor\":1.5, \"overall_anomaly_threshold\":0.3}\n",
    "original_fitness = fitness_function(original_params, data)\n",
    "print(f\"Original Fitness: {original_fitness[0]}\")\n",
    "\n",
    "baseline_fitness = fitness_function(default_params, data)\n",
    "print(f\"Initial Fitness: {baseline_fitness[0]}\")\n",
    "\n",
    "print(f\"Optimized Fitness: {best_fitness}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29baef7-f6a9-4183-9f97-69ca2f038784",
   "metadata": {},
   "source": [
    "# vvvvvvvvvvv DEBUG ZONE vvvvvvvvvvv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70c704ac-3874-490b-8527-de6c48c14a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
