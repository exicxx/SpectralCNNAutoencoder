{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5917234b-e981-47c5-a2fb-a2448a8bf23f",
   "metadata": {},
   "source": [
    "# Goals\n",
    "- Access and filter DESI EDR galaxy spectra data from a database using SPARCL.\n",
    "- Process and normalize the spectra data to prepare it for model training.\n",
    "- Develop a CNN autoencoder with skip connections to perform dimensionality reduction and reconstruction of the spectra.\n",
    "- Train the autoencoder model using a weighted mean squared error (MSE) loss function to emphasize critical spectral features.\n",
    "- Identify and visualize anomalies in the galaxy spectra based on high reconstruction errors.\n",
    "- Provide visual representations of detected anomalies and evaluate the model's performance through training loss metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7961d6d0-48c6-48bf-99f2-f5c303f5e9ad",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This project leverages the Dark Energy Spectroscopic Instrument (DESI) Early Data Release (EDR) dataset to train a Convolutional Neural Network (CNN) autoencoder for anomaly detection in galaxy spectra. The code retrieves galaxy spectra data from a database, processes and normalizes it, and then applies an autoencoder with skip connections to reconstruct the spectra. The reconstruction errors are used to identify anomalous spectra, which may indicate unusual features or observational issues in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b23f113-050b-4ca1-92a5-3ee98f4f33eb",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a1de084-baa5-4745-9969-19f963924355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "from sparcl.client import SparclClient\n",
    "from dl import queryClient as qc, authClient as ac\n",
    "from getpass import getpass\n",
    "import os\n",
    "import re\n",
    "import csv \n",
    "import torch.nn.functional as F\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "from torchviz import make_dot\n",
    "import time\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed \n",
    "import logging\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5db018df-184f-48a3-9d4c-96618fa95321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure file directories\n",
    "DATA_DIR = '/Users/elicox/Desktop/Mac/Work/Yr4 Work/Project/CNN-auto/'\n",
    "IMG_DIR = os.path.join(DATA_DIR, 'output_images')\n",
    "CSV_PATH = os.path.join(DATA_DIR, 'spectra_data.csv')\n",
    "os.makedirs(IMG_DIR, exist_ok=True)\n",
    "#os.environ[\"PATH\"] += os.pathsep + \"/opt/homebrew/bin\" # uncomment when CNN visualisation is needed\n",
    "\n",
    "# Initialize SPARCL client\n",
    "client = SparclClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c523b7-3358-483a-914d-5a19436c1d47",
   "metadata": {},
   "source": [
    "# Data Loading and Saving Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19115282-83b0-40f3-8aa5-c32ca3d1488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query the database\n",
    "def query_spectra_data():\n",
    "    \"\"\"\n",
    "    Queries the DESI EDR database to retrieve primary galaxy spectra.\n",
    "    Returns a DataFrame with the spectra data.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT zp.targetid, zp.survey, zp.program, zp.healpix,  \n",
    "           zp.z, zp.zwarn, zp.coadd_fiberstatus, zp.spectype, \n",
    "           zp.mean_fiber_ra, zp.mean_fiber_dec, zp.zcat_nspec, \n",
    "           zp.desi_target, zp.sv1_desi_target, zp.sv2_desi_target, zp.sv3_desi_target\n",
    "    FROM desi_edr.zpix AS zp\n",
    "    WHERE zp.zcat_primary = 't'\n",
    "      AND zp.zcat_nspec > 2\n",
    "      AND zp.spectype = 'GALAXY'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        zpix_cat = qc.query(sql=query, fmt='table')\n",
    "        df = zpix_cat.to_pandas()\n",
    "        print(f\"Retrieved {len(df)} records from the database.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a900283b-b35e-4558-9806-a7603966df14",
   "metadata": {},
   "source": [
    "# Data Preparation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75b06a4d-0da6-4bc9-bfcb-3aa1d9642bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_flux(targetid, inc, retries=5, delay=2):\n",
    "    \"\"\"\n",
    "    Helper function to retrieve and normalize flux for a single target ID.\n",
    "    Attempts multiple retries on failure.\n",
    "    \"\"\"\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            # Attempt to retrieve the flux data\n",
    "            res = client.retrieve_by_specid(specid_list=[targetid], include=inc, dataset_list=['DESI-EDR'])\n",
    "            for record in res.records:\n",
    "                if record['specprimary']:\n",
    "                    flux = record['flux']\n",
    "                    flux_min, flux_max = np.min(flux), np.max(flux)\n",
    "                    return (flux - flux_min) / (flux_max - flux_min)  # Return normalized flux if successful\n",
    "        except Exception:\n",
    "            if attempt < retries:\n",
    "                time.sleep(delay * (2 ** (attempt - 1)))  # Exponential backoff\n",
    "    return None  # Return None if all attempts fail\n",
    "\n",
    "\n",
    "def process_spectra_data(zpix_cat, batch_size=20, max_workers=10):\n",
    "    \"\"\"\n",
    "    Processes the spectra data in batches with parallel retrieval.\n",
    "    Assumes that every galaxy should have flux data, logs if missing.\n",
    "    \"\"\"\n",
    "    all_fluxes = []\n",
    "    total_records = len(zpix_cat)\n",
    "    inc = ['specid', 'redshift', 'flux', 'wavelength', 'spectype', \n",
    "           'specprimary', 'survey', 'program', 'targetid', 'coadd_fiberstatus']\n",
    "\n",
    "    for start_idx in tqdm(range(0, total_records, batch_size), desc=\"Processing spectra in large batches\"):\n",
    "        batch = zpix_cat.iloc[start_idx:start_idx+batch_size]\n",
    "        batch_fluxes = []\n",
    "\n",
    "        # Parallelize requests within the batch\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {executor.submit(retrieve_flux, int(row['targetid']), inc): row['targetid'] for _, row in batch.iterrows()}\n",
    "            for future in as_completed(futures):\n",
    "                flux = future.result()\n",
    "                if flux is not None:\n",
    "                    batch_fluxes.append(flux)\n",
    "                else:\n",
    "                    print(f\"Warning: Missing flux for target ID {futures[future]}\")  # Log missing data if any\n",
    "\n",
    "        all_fluxes.extend(batch_fluxes)\n",
    "\n",
    "    if not all_fluxes:\n",
    "        raise ValueError(\"Error: No flux data retrieved. Check data source or retrieval logic.\")\n",
    "\n",
    "    return np.array(all_fluxes)  # Return as numpy array for efficiency\n",
    "#################################################\n",
    "\n",
    " \n",
    "# Pad spectra data\n",
    "def pad_spectra(fluxes, target_length):\n",
    "    \"\"\"\n",
    "    Pads or truncates each flux array to match the target length.\n",
    "    Returns a numpy array of padded flux data.\n",
    "    \"\"\"\n",
    "    padded_fluxes = []\n",
    "    num_padded = 0  # Track only the padded spectra\n",
    "\n",
    "    for flux in fluxes:\n",
    "        if len(flux) < target_length:\n",
    "            padding = np.zeros(target_length - len(flux))\n",
    "            padded_flux = np.concatenate([flux, padding])\n",
    "            num_padded += 1\n",
    "        else:\n",
    "            padded_flux = flux[:target_length]\n",
    "        padded_fluxes.append(padded_flux)\n",
    "\n",
    "    if num_padded > 0:\n",
    "        print(f\"Padding applied to {num_padded} spectra to match the target length.\")\n",
    "    else:\n",
    "        print(\"No padding was necessary; all spectra are of equal length.\")\n",
    "\n",
    "    return np.array(padded_fluxes)\n",
    "\n",
    "\n",
    "# Create padding mask\n",
    "def create_padding_mask(fluxes, target_length):\n",
    "    \"\"\"\n",
    "    Generates a binary mask for padded values in the flux arrays.\n",
    "    Used in the loss calculation to ignore padded regions.\n",
    "    \"\"\"\n",
    "    masks = []\n",
    "    for flux in fluxes:\n",
    "        mask = np.ones_like(flux)\n",
    "        if len(flux) < target_length:\n",
    "            mask = np.concatenate([mask, np.zeros(target_length - len(flux))])\n",
    "        masks.append(mask[:target_length])\n",
    "    return np.array(masks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3170b744-e2c8-4b91-b002-987caaa6bd72",
   "metadata": {},
   "source": [
    "# Autoencoder Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86c6013e-b078-4616-8219-6b96646b2a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNAutoencoderWithSkip(nn.Module):\n",
    "    \"\"\"\n",
    "    A CNN-based autoencoder with skip connections to reconstruct spectra data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CNNAutoencoderWithSkip, self).__init__()\n",
    "        # Encoder layers with reduced channels\n",
    "        self.encoder1 = nn.Conv1d(1, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.encoder2 = nn.Conv1d(64, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.encoder3 = nn.Conv1d(32, 16, kernel_size=3, stride=2, padding=1)  # Adjusted from 128 max channels\n",
    "\n",
    "        # Decoder layers with reduced channels\n",
    "        self.decoder3 = nn.ConvTranspose1d(16, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.decoder2 = nn.ConvTranspose1d(32, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.decoder1 = nn.ConvTranspose1d(64, 1, kernel_size=3, stride=2, padding=1, output_padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding with skip connections\n",
    "        x1 = F.relu(self.encoder1(x))  # Save this for skip connection\n",
    "        x2 = F.relu(self.encoder2(x1))  # Save this for skip connection\n",
    "        x3 = F.relu(self.encoder3(x2))  # Last encoding layer\n",
    "\n",
    "        # Decoding with skip connections\n",
    "        x = F.relu(self.decoder3(x3))\n",
    "        \n",
    "        # Adjust sizes for skip connection with x2\n",
    "        if x2.size(2) > x.size(2):\n",
    "            x2 = x2[:, :, :x.size(2)]\n",
    "        elif x2.size(2) < x.size(2):\n",
    "            x = x[:, :, :x2.size(2)]\n",
    "        x = F.relu(self.decoder2(x + x2))\n",
    "        \n",
    "        # Adjust sizes for skip connection with x1\n",
    "        if x1.size(2) > x.size(2):\n",
    "            x1 = x1[:, :, :x.size(2)]\n",
    "        elif x1.size(2) < x.size(2):\n",
    "            x = x[:, :, :x1.size(2)]\n",
    "        x = self.decoder1(x + x1)\n",
    "\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cccd34d-19bb-44b9-84ec-96679dfcb785",
   "metadata": {},
   "source": [
    "# Custom Loss Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d062b54-70fa-4b8a-b69c-4e68aaf177ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom weighted MSE loss function\n",
    "def weighted_mse_loss(output, target, mask, weight_factor=10):\n",
    "    \"\"\"\n",
    "    Calculates MSE loss, amplifying high-residual areas with weight based on spectral gradient.\n",
    "    \"\"\"\n",
    "    mse_loss = (output - target) ** 2\n",
    "    gradient = torch.abs(target[:, :, 1:] - target[:, :, :-1])\n",
    "    weighted_loss = mse_loss[:, :, 1:] * (1 + weight_factor * gradient)\n",
    "    return (weighted_loss * mask[:, :, 1:]).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284f9dc0-7f9c-4766-8c71-acffcca5c7eb",
   "metadata": {},
   "source": [
    "# Training the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92113172-b222-4970-8721-25dd057499d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized train_autoencoder with gradient clipping\n",
    "def train_autoencoder(model, data, mask, epochs=50, batch_size=32, lr=0.001, grad_clip=1.0):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0\n",
    "        optimizer.zero_grad()  # Zero gradients at the start of each epoch\n",
    "\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            batch_data, mask_batch = data[i:i+batch_size], mask[i:i+batch_size]\n",
    "            loss = weighted_mse_loss(model(batch_data), batch_data, mask_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients to prevent explosions\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()  # Zero gradients after each batch\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_duration = time.time() - start_time\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(data):.4f}, Time: {epoch_duration:.2f}s\")\n",
    "    # Print final loss after all epochs\n",
    "    print(f\"Final Loss after {epochs} epochs: {final_loss:.8f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c570b2-cefd-4bb2-afc7-0437fe55a118",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf594d12-32e8-42b0-888d-e8c0e6bfdbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def detect_anomalous_regions(original_fluxes, reconstructed_fluxes, window_size=50, percentile_threshold=95, range_mismatch_factor=1.5, overall_anomaly_threshold=0.3):\n",
    "#     \"\"\"\n",
    "#     Identifies anomalous regions within spectra using residuals and a mismatch factor.\n",
    "#     \"\"\"\n",
    "#     num_spectra, spectrum_length = original_fluxes.shape\n",
    "#     absolute_residuals = np.abs(original_fluxes - reconstructed_fluxes)\n",
    "#     relative_residuals = np.abs((original_fluxes - reconstructed_fluxes) / (original_fluxes + 1e-5))\n",
    "\n",
    "#     abs_residual_threshold = np.percentile(\n",
    "#         [np.mean(absolute_residuals[:, i:i+window_size], axis=1)\n",
    "#         for i in range(0, spectrum_length - window_size + 1, window_size // 2)], percentile_threshold)\n",
    "#     rel_residual_threshold = np.percentile(\n",
    "#         [np.mean(relative_residuals[:, i:i+window_size], axis=1)\n",
    "#         for i in range(0, spectrum_length - window_size + 1, window_size // 2)], percentile_threshold)\n",
    "    \n",
    "#     anomalies, spectrum_anomalies = [], np.zeros(num_spectra, dtype=bool)\n",
    "#     for i in range(num_spectra):\n",
    "#         spectrum_anomalies_count = 0\n",
    "#         spectrum_anomaly_flags = np.zeros(spectrum_length, dtype=bool)\n",
    "        \n",
    "#         for start in range(0, spectrum_length - window_size + 1, window_size // 2):\n",
    "#             end = start + window_size\n",
    "#             window_abs_residual, window_rel_residual = np.mean(absolute_residuals[i, start:end]), np.mean(relative_residuals[i, start:end])\n",
    "#             original_range, reconstructed_range = np.ptp(original_fluxes[i, start:end]), np.ptp(reconstructed_fluxes[i, start:end])\n",
    "            \n",
    "#             if (window_abs_residual > abs_residual_threshold or\n",
    "#                 window_rel_residual > rel_residual_threshold or\n",
    "#                 reconstructed_range > original_range * range_mismatch_factor):\n",
    "#                 spectrum_anomaly_flags[start:end] = True\n",
    "#                 spectrum_anomalies_count += end - start\n",
    "\n",
    "#         spectrum_anomalies[i] = spectrum_anomalies_count / spectrum_length > overall_anomaly_threshold\n",
    "#         anomalies.append(spectrum_anomaly_flags)\n",
    "#     return anomalies, spectrum_anomalies\n",
    "############################################\n",
    "# Enhanced anomaly detection with justification for each anomaly\n",
    "def detect_anomalous_regions(original_fluxes, reconstructed_fluxes, window_size=50, percentile_threshold=95, \n",
    "                             range_mismatch_factor=1.5, overall_anomaly_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Identifies anomalous regions within spectra using residuals and mismatch factors,\n",
    "    with added logging to justify each detected anomaly.\n",
    "    \"\"\"\n",
    "    num_spectra, spectrum_length = original_fluxes.shape\n",
    "    absolute_residuals = np.abs(original_fluxes - reconstructed_fluxes)\n",
    "    relative_residuals = np.abs((original_fluxes - reconstructed_fluxes) / (original_fluxes + 1e-5))\n",
    "\n",
    "    abs_residual_threshold = np.percentile(\n",
    "        [np.mean(absolute_residuals[:, i:i+window_size], axis=1)\n",
    "         for i in range(0, spectrum_length - window_size + 1, window_size // 2)], percentile_threshold)\n",
    "    rel_residual_threshold = np.percentile(\n",
    "        [np.mean(relative_residuals[:, i:i+window_size], axis=1)\n",
    "         for i in range(0, spectrum_length - window_size + 1, window_size // 2)], percentile_threshold)\n",
    "\n",
    "    anomalies, spectrum_anomalies = [], np.zeros(num_spectra, dtype=bool)\n",
    "    anomaly_metadata = []\n",
    "\n",
    "    for i in range(num_spectra):\n",
    "        spectrum_anomalies_count = 0\n",
    "        spectrum_anomaly_flags = np.zeros(spectrum_length, dtype=bool)\n",
    "        anomaly_justification = []  # To log reasons for each anomaly\n",
    "\n",
    "        for start in range(0, spectrum_length - window_size + 1, window_size // 2):\n",
    "            end = start + window_size\n",
    "            window_abs_residual = np.mean(absolute_residuals[i, start:end])\n",
    "            window_rel_residual = np.mean(relative_residuals[i, start:end])\n",
    "            original_range = np.ptp(original_fluxes[i, start:end])\n",
    "            reconstructed_range = np.ptp(reconstructed_fluxes[i, start:end])\n",
    "\n",
    "            if (window_abs_residual > abs_residual_threshold or\n",
    "                window_rel_residual > rel_residual_threshold or\n",
    "                reconstructed_range > original_range * range_mismatch_factor):\n",
    "                \n",
    "                spectrum_anomaly_flags[start:end] = True\n",
    "                spectrum_anomalies_count += end - start\n",
    "\n",
    "                # Log specific reasons why this region was flagged as anomalous\n",
    "                reason = {\n",
    "                    \"window_start\": start,\n",
    "                    \"window_end\": end,\n",
    "                    \"abs_residual\": window_abs_residual,\n",
    "                    \"rel_residual\": window_rel_residual,\n",
    "                    \"range_mismatch\": reconstructed_range / (original_range + 1e-5)\n",
    "                }\n",
    "                anomaly_justification.append(reason)\n",
    "\n",
    "        spectrum_anomalies[i] = spectrum_anomalies_count / spectrum_length > overall_anomaly_threshold\n",
    "        anomalies.append(spectrum_anomaly_flags)\n",
    "        anomaly_metadata.append(anomaly_justification)  # Add metadata for each spectrum's anomalies\n",
    "\n",
    "    return anomalies, spectrum_anomalies, anomaly_metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d2f44-2ea6-4682-8e2f-b0f1f3848964",
   "metadata": {},
   "source": [
    "# Spectra Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b452bcb-c624-4ef6-96ce-1fac8069a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dynamic save path with numbering\n",
    "def create_save_path(save_directory, base_filename):\n",
    "    \"\"\"\n",
    "    Creates a new file path in the specified directory with a unique numeric suffix.\n",
    "    Ensures saved file names are sequentially numbered.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    existing_files = os.listdir(save_directory)\n",
    "    numbers = [\n",
    "        int(re.search(r'\\d+', f).group())\n",
    "        for f in existing_files if re.search(fr'{base_filename}_(\\d+)\\.png', f)\n",
    "    ]\n",
    "    next_number = max(numbers) + 1 if numbers else 1\n",
    "    return os.path.join(save_directory, f'{base_filename}_{next_number}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "794883d6-ae86-41a5-b8f6-43b26973b34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectra(original_fluxes, reconstructed_fluxes, anomalous_regions, spectrum_anomalies, zpix_cat, wavelengths=None, save_directory='output_images', max_samples=20):\n",
    "    num_spectra = len(original_fluxes)\n",
    "    indices = list(range(num_spectra))\n",
    "\n",
    "    # Sample only max_samples if dataset is too large\n",
    "    if num_spectra > max_samples:\n",
    "        print(f\"Too many spectra to plot ({num_spectra}). Sampling {max_samples} spectra.\")\n",
    "        indices = random.sample(indices, max_samples)\n",
    "    else:\n",
    "        print(f\"Plotting all {num_spectra} spectra.\")\n",
    "\n",
    "    fig, axes = plt.subplots(len(indices), 1, figsize=(10, 6 * len(indices)), sharex=True)\n",
    "    if len(indices) == 1:\n",
    "        axes = [axes]  # Ensure axes is iterable for a single plot\n",
    "\n",
    "    for idx, i in enumerate(indices):\n",
    "        ax = axes[idx]\n",
    "        x_axis = wavelengths if wavelengths is not None else range(len(original_fluxes[i]))\n",
    "        \n",
    "        # Plot original and reconstructed spectra\n",
    "        ax.plot(x_axis, original_fluxes[i], label=\"Original\", color='#2c7bb6', linewidth=0.5)\n",
    "        ax.plot(x_axis, reconstructed_fluxes[i], label=\"Reconstructed\", color='#d7191c', alpha=0.7, linewidth=0.5)\n",
    "\n",
    "        # Highlight anomalous regions\n",
    "        in_anomaly, anomaly_start = False, 0\n",
    "        for j in range(len(x_axis)):\n",
    "            if anomalous_regions[i][j] and not in_anomaly:\n",
    "                anomaly_start = j\n",
    "                in_anomaly = True\n",
    "            elif not anomalous_regions[i][j] and in_anomaly:\n",
    "                ax.axvspan(x_axis[anomaly_start], x_axis[j], color='#fdae61', alpha=0.7)\n",
    "                in_anomaly = False\n",
    "        if in_anomaly:\n",
    "            ax.axvspan(x_axis[anomaly_start], x_axis[-1], color='#fdae61', alpha=0.7)\n",
    "\n",
    "        # Set background color for anomalous spectra\n",
    "        if spectrum_anomalies[i]:\n",
    "            ax.set_facecolor('#fee090')  # Yellow background\n",
    "            ax.set_title(f\"Spectrum {i+1} (ID: {zpix_cat['targetid'].iloc[i]}) - Anomalous\", color='red')\n",
    "        else:\n",
    "            ax.set_title(f\"Spectrum {i+1} (ID: {zpix_cat['targetid'].iloc[i]})\", color='black')\n",
    "\n",
    "        # Add residuals plot\n",
    "        divider = make_axes_locatable(ax)\n",
    "        ax_residual = divider.append_axes(\"bottom\", size=\"25%\", pad=0, sharex=ax)\n",
    "        ax_residual.plot(x_axis, original_fluxes[i] - reconstructed_fluxes[i], color='#4dac26', linewidth=0.5)\n",
    "        ax_residual.set_ylabel(\"Residuals\")\n",
    "        ax.set_ylabel(\"Flux (normalized)\")\n",
    "        ax.set_xlabel(\"Wavelength (Å)\")\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = create_save_path(save_directory, 'spectra_reconstruction')\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close(fig)  # Close figure to free memory\n",
    "    print(f\"Figure saved to {save_path}\")\n",
    "\n",
    "\n",
    "def plot_anomalous_spectra(original_fluxes, reconstructed_fluxes, anomalous_regions, spectrum_anomalies, zpix_cat, wavelengths=None, save_directory='output_images', max_samples=20):\n",
    "    \"\"\"\n",
    "    Plots only the spectra that are flagged as anomalous.\n",
    "    \"\"\"\n",
    "    # Filter for only anomalous spectra\n",
    "    anomalous_indices = [i for i, is_anomalous in enumerate(spectrum_anomalies) if is_anomalous]\n",
    "    \n",
    "    if not anomalous_indices:\n",
    "        print(\"No anomalous spectra detected.\")\n",
    "        return\n",
    "    \n",
    "    # Limit the number of spectra to plot if needed\n",
    "    if len(anomalous_indices) > max_samples:\n",
    "        print(f\"Too many anomalous spectra to plot ({len(anomalous_indices)}). Sampling {max_samples} spectra.\")\n",
    "        anomalous_indices = random.sample(anomalous_indices, max_samples)\n",
    "    else:\n",
    "        print(f\"Plotting all {len(anomalous_indices)} anomalous spectra.\")\n",
    "\n",
    "    # Extract only the anomalous spectra data\n",
    "    filtered_original_fluxes = [original_fluxes[i] for i in anomalous_indices]\n",
    "    filtered_reconstructed_fluxes = [reconstructed_fluxes[i] for i in anomalous_indices]\n",
    "    filtered_anomalous_regions = [anomalous_regions[i] for i in anomalous_indices]\n",
    "    filtered_zpix_cat = zpix_cat.iloc[anomalous_indices]  # Select rows for anomalous spectra\n",
    "\n",
    "    # Call the original plotting function with filtered data\n",
    "    plot_spectra(\n",
    "        original_fluxes=filtered_original_fluxes,\n",
    "        reconstructed_fluxes=filtered_reconstructed_fluxes,\n",
    "        anomalous_regions=filtered_anomalous_regions,\n",
    "        spectrum_anomalies=[True] * len(filtered_original_fluxes),  # Mark all as anomalous for consistent background\n",
    "        zpix_cat=filtered_zpix_cat,\n",
    "        wavelengths=wavelengths,\n",
    "        save_directory=save_directory,\n",
    "        max_samples=max_samples\n",
    "    )\n",
    "\n",
    "#Save anomalies to CSV for analysis\n",
    "def save_anomaly_data(zpix_cat, spectrum_anomalies, file_path='anomaly_data.csv'):\n",
    "    \"\"\"\n",
    "    Saves information about detected anomalies to a CSV file for further analysis.\n",
    "    Each row includes the target ID and an anomaly indicator.\n",
    "    \"\"\"\n",
    "    with open(file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['targetid', 'is_anomalous'])  # Header row\n",
    "        for i in range(len(zpix_cat)):\n",
    "            writer.writerow([zpix_cat['targetid'].iloc[i], int(spectrum_anomalies[i])])\n",
    "    print(f\"Anomaly data saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b052ad9-acde-41d6-97a8-71c828627cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the full autoencoder\n",
    "def visualize_autoencoder(autoencoder, input_data):\n",
    "    \"\"\"\n",
    "    Visualizes the entire autoencoder model structure, saving it as an image.\n",
    "    Takes input data, passes it through the model, and generates a network graph.\n",
    "    \"\"\"\n",
    "    outputs = autoencoder(input_data)\n",
    "    model_viz = make_dot(outputs, params=dict(autoencoder.named_parameters()))\n",
    "    model_viz.format = \"png\"\n",
    "    save_path = create_save_path(IMG_DIR, 'autoencoder_visualization')\n",
    "    model_viz.render(save_path.replace(\".png\", \"\"))\n",
    "    print(f\"Full autoencoder visualization saved to {save_path}\")\n",
    "\n",
    "# Visualize only the encoder part\n",
    "def visualize_encoder(autoencoder, input_data):\n",
    "    \"\"\"\n",
    "    Visualizes the encoder portion of the autoencoder model, saving it as an image.\n",
    "    Passes input data through only the encoder layers and generates a graph.\n",
    "    \"\"\"\n",
    "    encoder_output = autoencoder.encoder3(autoencoder.encoder2(autoencoder.encoder1(input_data)))\n",
    "    encoder_viz = make_dot(encoder_output, params=dict(autoencoder.named_parameters()), \n",
    "                           show_attrs=True, show_saved=True)\n",
    "    encoder_viz.format = \"png\"\n",
    "    save_path = create_save_path(IMG_DIR, 'encoder_visualization')\n",
    "    encoder_viz.render(save_path.replace(\".png\", \"\"))\n",
    "    print(f\"Encoder visualization saved to {save_path}\")\n",
    "\n",
    "\n",
    "# Visualize only the decoder part, given an encoded input\n",
    "def visualize_decoder(autoencoder, encoded_input):\n",
    "    \"\"\"\n",
    "    Visualizes the decoder portion of the autoencoder model, saving it as an image.\n",
    "    Takes an encoded input and generates a graph of the decoder layers.\n",
    "    \"\"\"\n",
    "    decoder_output = autoencoder.decoder1(autoencoder.decoder2(autoencoder.decoder3(encoded_input)))\n",
    "    decoder_viz = make_dot(decoder_output, params=dict(autoencoder.named_parameters()), \n",
    "                           show_attrs=True, show_saved=True)\n",
    "    decoder_viz.format = \"png\"\n",
    "    save_path = create_save_path(IMG_DIR, 'decoder_visualization')\n",
    "    decoder_viz.render(save_path.replace(\".png\", \"\"))\n",
    "    print(f\"Decoder visualization saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564197ce-d2a7-4b9b-b1af-866c1b839956",
   "metadata": {},
   "source": [
    "# Autoencoder Memory Wipe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cbbc12c-d384-4dda-a3a4-cfefad311722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_model_weights(model):\n",
    "    \"\"\"\n",
    "    Wipes the memory of the model by reinitialising all weights.\n",
    "    This allows the model to be retrained from scratch.\n",
    "    \"\"\"\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, (nn.Conv1d, nn.ConvTranspose1d, nn.Linear)):\n",
    "            # Reinitialize weights and biases for layers that have them\n",
    "            layer.reset_parameters()\n",
    "    print(\"Model weights and biases have been reset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3ac33f-a5bc-4145-9c93-b08cf08ad946",
   "metadata": {},
   "source": [
    "# Main Execution Flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbad36dd-b45b-4b00-b8e2-a8dee5bb34df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 739 records from the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing spectra in large batches: 100%|██████████| 37/37 [10:59<00:00, 17.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No padding was necessary; all spectra are of equal length.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "zpix_cat = query_spectra_data()\n",
    "if zpix_cat is not None:\n",
    "    all_fluxes = process_spectra_data(zpix_cat)\n",
    "    max_length = max(len(f) for f in all_fluxes)\n",
    "    all_fluxes_padded = pad_spectra(all_fluxes, max_length)\n",
    "    mask = create_padding_mask(all_fluxes, max_length)\n",
    "\n",
    "    # Prepare tensors for model training\n",
    "    all_fluxes_tensor = torch.tensor(all_fluxes_padded, dtype=torch.float32).unsqueeze(1)\n",
    "    mask_tensor = torch.tensor(mask, dtype=torch.float32).unsqueeze(1)\n",
    "else:\n",
    "    print(\"No data available for processing.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea0ed778-d20f-437b-b22a-fad663a399fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights and biases have been reset.\n",
      "Epoch [1/50], Loss: 0.0026, Time: 4.89s\n",
      "Epoch [2/50], Loss: 0.0007, Time: 4.43s\n",
      "Epoch [3/50], Loss: 0.0001, Time: 4.16s\n",
      "Epoch [4/50], Loss: 0.0001, Time: 4.27s\n",
      "Epoch [5/50], Loss: 0.0001, Time: 4.19s\n",
      "Epoch [6/50], Loss: 0.0000, Time: 4.12s\n",
      "Epoch [7/50], Loss: 0.0000, Time: 3.90s\n",
      "Epoch [8/50], Loss: 0.0000, Time: 3.84s\n",
      "Epoch [9/50], Loss: 0.0000, Time: 4.10s\n",
      "Epoch [10/50], Loss: 0.0000, Time: 3.83s\n",
      "Epoch [11/50], Loss: 0.0000, Time: 3.95s\n",
      "Epoch [12/50], Loss: 0.0000, Time: 3.81s\n",
      "Epoch [13/50], Loss: 0.0000, Time: 3.95s\n",
      "Epoch [14/50], Loss: 0.0000, Time: 3.80s\n",
      "Epoch [15/50], Loss: 0.0000, Time: 3.87s\n",
      "Epoch [16/50], Loss: 0.0000, Time: 4.21s\n",
      "Epoch [17/50], Loss: 0.0000, Time: 4.06s\n",
      "Epoch [18/50], Loss: 0.0000, Time: 3.91s\n",
      "Epoch [19/50], Loss: 0.0000, Time: 3.84s\n",
      "Epoch [20/50], Loss: 0.0000, Time: 3.92s\n",
      "Epoch [21/50], Loss: 0.0000, Time: 3.99s\n",
      "Epoch [22/50], Loss: 0.0000, Time: 4.04s\n",
      "Epoch [23/50], Loss: 0.0000, Time: 3.92s\n",
      "Epoch [24/50], Loss: 0.0000, Time: 4.12s\n",
      "Epoch [25/50], Loss: 0.0000, Time: 4.79s\n",
      "Epoch [26/50], Loss: 0.0000, Time: 4.54s\n",
      "Epoch [27/50], Loss: 0.0000, Time: 4.00s\n",
      "Epoch [28/50], Loss: 0.0000, Time: 3.90s\n",
      "Epoch [29/50], Loss: 0.0000, Time: 4.03s\n",
      "Epoch [30/50], Loss: 0.0000, Time: 3.88s\n",
      "Epoch [31/50], Loss: 0.0000, Time: 4.40s\n",
      "Epoch [32/50], Loss: 0.0000, Time: 4.77s\n",
      "Epoch [33/50], Loss: 0.0000, Time: 4.29s\n",
      "Epoch [34/50], Loss: 0.0000, Time: 4.23s\n",
      "Epoch [35/50], Loss: 0.0000, Time: 4.37s\n",
      "Epoch [36/50], Loss: 0.0000, Time: 4.36s\n",
      "Epoch [37/50], Loss: 0.0000, Time: 4.12s\n",
      "Epoch [38/50], Loss: 0.0000, Time: 4.11s\n",
      "Epoch [39/50], Loss: 0.0000, Time: 3.89s\n",
      "Epoch [40/50], Loss: 0.0000, Time: 4.04s\n",
      "Epoch [41/50], Loss: 0.0000, Time: 4.15s\n",
      "Epoch [42/50], Loss: 0.0000, Time: 4.04s\n",
      "Epoch [43/50], Loss: 0.0000, Time: 3.93s\n",
      "Epoch [44/50], Loss: 0.0000, Time: 3.98s\n",
      "Epoch [45/50], Loss: 0.0000, Time: 3.91s\n",
      "Epoch [46/50], Loss: 0.0000, Time: 3.93s\n",
      "Epoch [47/50], Loss: 0.0000, Time: 3.95s\n",
      "Epoch [48/50], Loss: 0.0000, Time: 4.11s\n",
      "Epoch [49/50], Loss: 0.0000, Time: 4.06s\n",
      "Epoch [50/50], Loss: 0.0000, Time: 3.96s\n",
      "Anomaly data saved to anomalous_spectra_data.csv\n",
      "Too many anomalous spectra to plot (93). Sampling 20 spectra.\n",
      "Plotting all 20 spectra.\n",
      "Figure saved to /Users/elicox/Desktop/Mac/Work/Yr4 Work/Project/CNN-auto/output_images/spectra_reconstruction_56.png\n",
      "Too many spectra to plot (739). Sampling 20 spectra.\n",
      "Figure saved to /Users/elicox/Desktop/Mac/Work/Yr4 Work/Project/CNN-auto/output_images/spectra_reconstruction_57.png\n"
     ]
    }
   ],
   "source": [
    "# Model setup and initialization\n",
    "autoencoder = CNNAutoencoderWithSkip()\n",
    "reset_model_weights(autoencoder)\n",
    "\n",
    "# Train the model\n",
    "train_autoencoder(autoencoder, all_fluxes_tensor, mask_tensor)\n",
    "\n",
    "# Evaluate the model and perform anomaly detection\n",
    "autoencoder.eval()\n",
    "reconstructed_fluxes = autoencoder(all_fluxes_tensor).detach().numpy().squeeze()\n",
    "# Detect anomalies\n",
    "anomalous_regions, spectrum_anomalies = detect_anomalous_regions(\n",
    "    original_fluxes=all_fluxes_tensor.numpy().squeeze(),\n",
    "    reconstructed_fluxes=reconstructed_fluxes,\n",
    "    window_size=50,\n",
    "    percentile_threshold=95,\n",
    "    range_mismatch_factor=1.5,\n",
    "    overall_anomaly_threshold=0.3\n",
    ")\n",
    "\n",
    "# Save anomaly information\n",
    "save_anomaly_data(zpix_cat, spectrum_anomalies, file_path='anomalous_spectra_data.csv')\n",
    "\n",
    "# Plot only the anomalous spectra\n",
    "plot_anomalous_spectra(\n",
    "    original_fluxes=all_fluxes_tensor.numpy().squeeze(),\n",
    "    reconstructed_fluxes=reconstructed_fluxes,\n",
    "    anomalous_regions=anomalous_regions,\n",
    "    spectrum_anomalies=spectrum_anomalies,\n",
    "    zpix_cat=zpix_cat,\n",
    "    wavelengths=None,  # or provide the wavelength range if available\n",
    "    save_directory=IMG_DIR\n",
    ")\n",
    "\n",
    "# Plot the results, using dynamic naming\n",
    "plot_spectra(\n",
    "    original_fluxes=all_fluxes_tensor.numpy().squeeze(),\n",
    "    reconstructed_fluxes=reconstructed_fluxes,\n",
    "    anomalous_regions=anomalous_regions,\n",
    "    spectrum_anomalies=spectrum_anomalies,\n",
    "    zpix_cat=zpix_cat,\n",
    "    save_directory=IMG_DIR\n",
    ")\n",
    "\n",
    "# # Optional: Visualize the autoencoder and its components if needed\n",
    "#real_input = all_fluxes_tensor[0].unsqueeze(0)  # Select a real input sample\n",
    "\n",
    "####### Uncomment when visualizations are required #########\n",
    "# visualize_autoencoder(autoencoder, real_input)\n",
    "# visualize_encoder(autoencoder, real_input)\n",
    "# encoded_input = autoencoder.encoder3(autoencoder.encoder2(autoencoder.encoder1(real_input)))\n",
    "# visualize_decoder(autoencoder, encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29baef7-f6a9-4183-9f97-69ca2f038784",
   "metadata": {},
   "source": [
    "# vvvvvvvvvvv DEBUG ZONE vvvvvvvvvvv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
